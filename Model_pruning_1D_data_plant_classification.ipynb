{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mEyU01VKZJi"
      },
      "source": [
        "# Latest experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HleQjExKiT7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0eYdAbxKiT9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3s-ewmMKiT9",
        "outputId": "d90d7558-c34a-470c-c504-acdcad90c835"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAfg57FWKiT-",
        "outputId": "3221bba8-6667-458b-ab4f-7eafe7825f9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBc0_Q1kiMh3"
      },
      "source": [
        "## Housing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4oIwf3BKiT-"
      },
      "outputs": [],
      "source": [
        "class PlantsDataset(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, csv_file, root_dir, transform=None, train = True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.housing_data = pd.read_csv(csv_file)\n",
        "        self.housing_data = self.housing_data.sample(frac=1)\n",
        "        self.x = self.housing_data.drop('Class', axis = 1)\n",
        "        self.y = self.housing_data[['Class']]\n",
        "        self.y = self.y - 1\n",
        "        \n",
        "        length = len(self.y)\n",
        "        print(length)\n",
        "        train_len = int(length*0.8)\n",
        "        \n",
        "        if train:\n",
        "          self.x = self.x.iloc[:train_len, :]\n",
        "          self.y = self.y.iloc[:train_len, :]\n",
        "\n",
        "        else:\n",
        "          self.x = self.x.iloc[train_len:, :]\n",
        "          self.y = self.y.iloc[train_len:, :]  \n",
        "\n",
        "        self.x = torch.Tensor(self.x.values).to(device)\n",
        "        self.y = torch.Tensor(self.y.values).to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        features = self.x[idx]\n",
        "        price = self.y[idx]\n",
        "        return {'features': features, 'price': price}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHE3Dd6lKiT_",
        "outputId": "de3211d9-eacf-4016-c5cf-509cdda4c5ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1599\n",
            "1599\n"
          ]
        }
      ],
      "source": [
        "train_dataset = PlantsDataset(csv_file='plants.csv',\n",
        "                                    root_dir='/', train = True)\n",
        "\n",
        "test_dataset = PlantsDataset(csv_file='plants.csv',\n",
        "                                    root_dir='/', train = False)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32,\n",
        "                        shuffle=True, num_workers=0)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32,\n",
        "                        shuffle=True, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGrKyJ4KUSiw",
        "outputId": "8e2a40fe-92b5-4b35-d3af-9dee010de66f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "320"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg9SZre4KiUE"
      },
      "outputs": [],
      "source": [
        "def test(net, test_loader, loss):\n",
        "  running_loss = 0.0\n",
        "  equal = 0\n",
        "  total = 0\n",
        "  for i, data in enumerate(test_loader, 0):\n",
        "      inputs, labels = data['features'].to(device), data['price'].to(device)\n",
        "      labels = labels.squeeze()\n",
        "      labels = labels.long()\n",
        "      outputs = net(inputs)\n",
        "      preds = outputs.argmax(1)\n",
        "      loss = criterion(outputs, labels)\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      equal += torch.eq(labels, preds).sum()\n",
        "      total += len(labels)\n",
        "\n",
        "  running_loss/=320\n",
        "  accuracy = equal * 100 / total\n",
        "  accuracy = accuracy.cpu().numpy()\n",
        "  print(\"Avg Loss during Testing - \", running_loss)\n",
        "  print(\"Test Accuracy - \", accuracy)\n",
        "  return running_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3lJMthqKiUF"
      },
      "outputs": [],
      "source": [
        "def train(net, train_loader, num_epoch, optimizer, criterion, running_loss_batch = 200):\n",
        "  for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(train_loader, 0):\n",
        "          # get the inputs; data is a list of [inputs, labels]\n",
        "          inputs, labels = data['features'].to(device), data['price'].to(device)\n",
        "          labels = labels.squeeze()\n",
        "          labels = labels.long()\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "          # print(inputs)\n",
        "          # forward + backward + optimize\n",
        "          outputs = net(inputs)\n",
        "          # print(outputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          # print statistics\n",
        "          running_loss += loss.item()\n",
        "          if i % running_loss_batch == (running_loss_batch-1):    # print every 150 mini-batches\n",
        "              print('[%d, %5d] loss: %.6f' %\n",
        "                    (epoch + 1, i + 1, running_loss / running_loss_batch))\n",
        "              running_loss = 0.0\n",
        "\n",
        "  print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(64, 32)\n",
        "        self.fc2 = nn.Linear(32, 16)\n",
        "        self.fc3 = nn.Linear(16, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = Net().to(device)"
      ],
      "metadata": {
        "id": "5JFUWuKM16xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V8pOkZv1vr8"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c429abd-38e9-4c1b-ab17-b22c04e859d4",
        "id": "Zk1QPyAG1vsC"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg Loss during Testing -  0.14447331428527832\n",
            "Test Accuracy -  1.25\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.14447331428527832"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# Testing before training\n",
        "test(net, test_loader, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(5):\n",
        "  train(net, train_loader, 100, optimizer, criterion, 50)\n",
        "  test(net, test_loader, criterion)"
      ],
      "metadata": {
        "id": "ln4TGzoZ7q-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6c2dad7-dc8a-4d8b-857a-c67c0887e3e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Training\n",
            "Avg Loss during Testing -  0.04480822645127773\n",
            "Test Accuracy -  tensor(61.2500, device='cuda:0')\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.028824968822300436\n",
            "Test Accuracy -  tensor(76.2500, device='cuda:0')\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.021220549941062927\n",
            "Test Accuracy -  tensor(83.7500, device='cuda:0')\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.016541658621281385\n",
            "Test Accuracy -  tensor(87.8125, device='cuda:0')\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.013987440336495637\n",
            "Test Accuracy -  tensor(91.8750, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing before training\n",
        "test(net, test_loader, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7qZ8dqz7zcR",
        "outputId": "869c12cc-b691-4bc6-c626-e6287b447b2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg Loss during Testing -  0.013987441221252084\n",
            "Test Accuracy -  tensor(91.8750, device='cuda:0')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.013987441221252084"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_dcfdypiUlA"
      },
      "source": [
        "### 3 layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACO-sU_9N6Wy"
      },
      "outputs": [],
      "source": [
        "def print_sparsity(net):\n",
        "  print(\n",
        "      \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc1.weight == 0))\n",
        "          / float(net.fc1.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Sparsity in fc2.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc2.weight == 0))\n",
        "          / float(net.fc2.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Sparsity in fc3.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc3.weight == 0))\n",
        "          / float(net.fc3.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Global sparsity: {:.2f}%\".format(\n",
        "          100. * float(\n",
        "              + torch.sum(net.fc1.weight == 0)\n",
        "              + torch.sum(net.fc2.weight == 0)\n",
        "              + torch.sum(net.fc3.weight == 0)\n",
        "          )\n",
        "          / float(\n",
        "              + net.fc1.weight.nelement()\n",
        "              + net.fc2.weight.nelement()\n",
        "              + net.fc3.weight.nelement()\n",
        "          )\n",
        "      )\n",
        "  )\n",
        "\n",
        "# def print_sparsity(net):\n",
        "#   for name, param in net.named_parameters():\n",
        "#     if 'weight' in name:\n",
        "#       print(\n",
        "#               \"Sparsity in {}: {:.2f}%\".format(name, \n",
        "#                   100. * float(torch.sum(param.data == 0))\n",
        "#                   / float(param.data.nelement())\n",
        "#               )\n",
        "#           )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfMqGD0nKiT_"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(64, 32)\n",
        "        self.fc2 = nn.Linear(32, 16)\n",
        "        self.fc3 = nn.Linear(16, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = Net().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6r1P5wPKiUE"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmmQhR_kLTnD",
        "outputId": "220aacf3-2e1c-4b11-d28c-5bf1b1209444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg Loss during Testing -  0.14447331428527832\n",
            "Test Accuracy -  tensor(1.2500, device='cuda:0')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.14447331428527832"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Testing before training\n",
        "test(net, test_loader, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_7FLch8MyLz",
        "outputId": "49b8aa41-f927-49ae-829a-57f4946d4623"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sparsity in fc1.weight: 0.00%\n",
            "Sparsity in fc2.weight: 0.00%\n",
            "Sparsity in fc3.weight: 0.00%\n",
            "Global sparsity: 0.00%\n",
            "[1,    25] loss: 4.617849\n",
            "[2,    25] loss: 4.604599\n",
            "[3,    25] loss: 4.593285\n",
            "[4,    25] loss: 4.572779\n",
            "[5,    25] loss: 4.517997\n",
            "[6,    25] loss: 4.396888\n",
            "[7,    25] loss: 4.245118\n",
            "[8,    25] loss: 4.123367\n",
            "[9,    25] loss: 3.981923\n",
            "[10,    25] loss: 3.906324\n",
            "[11,    25] loss: 3.806250\n",
            "[12,    25] loss: 3.734241\n",
            "[13,    25] loss: 3.665391\n",
            "[14,    25] loss: 3.606824\n",
            "[15,    25] loss: 3.540056\n",
            "[16,    25] loss: 3.475074\n",
            "[17,    25] loss: 3.407463\n",
            "[18,    25] loss: 3.319760\n",
            "[19,    25] loss: 3.285664\n",
            "[20,    25] loss: 3.227842\n",
            "[21,    25] loss: 3.195293\n",
            "[22,    25] loss: 3.118127\n",
            "[23,    25] loss: 3.045318\n",
            "[24,    25] loss: 2.990921\n",
            "[25,    25] loss: 2.939512\n",
            "[26,    25] loss: 2.910761\n",
            "[27,    25] loss: 2.865515\n",
            "[28,    25] loss: 2.801127\n",
            "[29,    25] loss: 2.752358\n",
            "[30,    25] loss: 2.696606\n",
            "[31,    25] loss: 2.699037\n",
            "[32,    25] loss: 2.675367\n",
            "[33,    25] loss: 2.589303\n",
            "[34,    25] loss: 2.599364\n",
            "[35,    25] loss: 2.540843\n",
            "[36,    25] loss: 2.523185\n",
            "[37,    25] loss: 2.504368\n",
            "[38,    25] loss: 2.440095\n",
            "[39,    25] loss: 2.430557\n",
            "[40,    25] loss: 2.397515\n",
            "[41,    25] loss: 2.363635\n",
            "[42,    25] loss: 2.326118\n",
            "[43,    25] loss: 2.275738\n",
            "[44,    25] loss: 2.249841\n",
            "[45,    25] loss: 2.265666\n",
            "[46,    25] loss: 2.252664\n",
            "[47,    25] loss: 2.206277\n",
            "[48,    25] loss: 2.212633\n",
            "[49,    25] loss: 2.219114\n",
            "[50,    25] loss: 2.194912\n",
            "[51,    25] loss: 2.149397\n",
            "[52,    25] loss: 2.138731\n",
            "[53,    25] loss: 2.111883\n",
            "[54,    25] loss: 2.080668\n",
            "[55,    25] loss: 2.049294\n",
            "[56,    25] loss: 2.051598\n",
            "[57,    25] loss: 2.017187\n",
            "[58,    25] loss: 2.020492\n",
            "[59,    25] loss: 1.972526\n",
            "[60,    25] loss: 2.006708\n",
            "[61,    25] loss: 1.932252\n",
            "[62,    25] loss: 1.962032\n",
            "[63,    25] loss: 1.907523\n",
            "[64,    25] loss: 1.919040\n",
            "[65,    25] loss: 1.922428\n",
            "[66,    25] loss: 1.898050\n",
            "[67,    25] loss: 1.851002\n",
            "[68,    25] loss: 1.876968\n",
            "[69,    25] loss: 1.855163\n",
            "[70,    25] loss: 1.849605\n",
            "[71,    25] loss: 1.841468\n",
            "[72,    25] loss: 1.805861\n",
            "[73,    25] loss: 1.814340\n",
            "[74,    25] loss: 1.736043\n",
            "[75,    25] loss: 1.786019\n",
            "[76,    25] loss: 1.769771\n",
            "[77,    25] loss: 1.738382\n",
            "[78,    25] loss: 1.733688\n",
            "[79,    25] loss: 1.740096\n",
            "[80,    25] loss: 1.734751\n",
            "[81,    25] loss: 1.638499\n",
            "[82,    25] loss: 1.707909\n",
            "[83,    25] loss: 1.660504\n",
            "[84,    25] loss: 1.678172\n",
            "[85,    25] loss: 1.616514\n",
            "[86,    25] loss: 1.637240\n",
            "[87,    25] loss: 1.619979\n",
            "[88,    25] loss: 1.632273\n",
            "[89,    25] loss: 1.632629\n",
            "[90,    25] loss: 1.617152\n",
            "[91,    25] loss: 1.574785\n",
            "[92,    25] loss: 1.599085\n",
            "[93,    25] loss: 1.552480\n",
            "[94,    25] loss: 1.530777\n",
            "[95,    25] loss: 1.539145\n",
            "[96,    25] loss: 1.552148\n",
            "[97,    25] loss: 1.527164\n",
            "[98,    25] loss: 1.545513\n",
            "[99,    25] loss: 1.554995\n",
            "[100,    25] loss: 1.499922\n",
            "[101,    25] loss: 1.487737\n",
            "[102,    25] loss: 1.507045\n",
            "[103,    25] loss: 1.444042\n",
            "[104,    25] loss: 1.478270\n",
            "[105,    25] loss: 1.471730\n",
            "[106,    25] loss: 1.465218\n",
            "[107,    25] loss: 1.423548\n",
            "[108,    25] loss: 1.435612\n",
            "[109,    25] loss: 1.449040\n",
            "[110,    25] loss: 1.459884\n",
            "[111,    25] loss: 1.416927\n",
            "[112,    25] loss: 1.449435\n",
            "[113,    25] loss: 1.398577\n",
            "[114,    25] loss: 1.382894\n",
            "[115,    25] loss: 1.377625\n",
            "[116,    25] loss: 1.359458\n",
            "[117,    25] loss: 1.353223\n",
            "[118,    25] loss: 1.399746\n",
            "[119,    25] loss: 1.383463\n",
            "[120,    25] loss: 1.359150\n",
            "[121,    25] loss: 1.336185\n",
            "[122,    25] loss: 1.341907\n",
            "[123,    25] loss: 1.303123\n",
            "[124,    25] loss: 1.304028\n",
            "[125,    25] loss: 1.290645\n",
            "[126,    25] loss: 1.292089\n",
            "[127,    25] loss: 1.276840\n",
            "[128,    25] loss: 1.286288\n",
            "[129,    25] loss: 1.318960\n",
            "[130,    25] loss: 1.265856\n",
            "[131,    25] loss: 1.275608\n",
            "[132,    25] loss: 1.254368\n",
            "[133,    25] loss: 1.255343\n",
            "[134,    25] loss: 1.258404\n",
            "[135,    25] loss: 1.218821\n",
            "[136,    25] loss: 1.287209\n",
            "[137,    25] loss: 1.229408\n",
            "[138,    25] loss: 1.260733\n",
            "[139,    25] loss: 1.226150\n",
            "[140,    25] loss: 1.220120\n",
            "[141,    25] loss: 1.207386\n",
            "[142,    25] loss: 1.189420\n",
            "[143,    25] loss: 1.218866\n",
            "[144,    25] loss: 1.200128\n",
            "[145,    25] loss: 1.201284\n",
            "[146,    25] loss: 1.179190\n",
            "[147,    25] loss: 1.178879\n",
            "[148,    25] loss: 1.166692\n",
            "[149,    25] loss: 1.155741\n",
            "[150,    25] loss: 1.128670\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.0368280079215765\n",
            "Test Accuracy -  70.9375\n",
            "Sparsity in fc1.weight: 23.39%\n",
            "Sparsity in fc2.weight: 18.55%\n",
            "Sparsity in fc3.weight: 16.12%\n",
            "Global sparsity: 20.00%\n",
            "Avg Loss during Testing -  0.03824604116380215\n",
            "Test Accuracy -  67.5\n",
            "[1,    25] loss: 2.559530\n",
            "[2,    25] loss: 1.741226\n",
            "[3,    25] loss: 1.640587\n",
            "[4,    25] loss: 1.580931\n",
            "[5,    25] loss: 1.527383\n",
            "[6,    25] loss: 1.596651\n",
            "[7,    25] loss: 1.357494\n",
            "[8,    25] loss: 1.577944\n",
            "[9,    25] loss: 1.490370\n",
            "[10,    25] loss: 1.446379\n",
            "[11,    25] loss: 1.315381\n",
            "[12,    25] loss: 1.195171\n",
            "[13,    25] loss: 1.269530\n",
            "[14,    25] loss: 1.186041\n",
            "[15,    25] loss: 1.319757\n",
            "[16,    25] loss: 1.591065\n",
            "[17,    25] loss: 1.450567\n",
            "[18,    25] loss: 1.284228\n",
            "[19,    25] loss: 1.295377\n",
            "[20,    25] loss: 1.283117\n",
            "[21,    25] loss: 1.057233\n",
            "[22,    25] loss: 1.119905\n",
            "[23,    25] loss: 1.135655\n",
            "[24,    25] loss: 1.296433\n",
            "[25,    25] loss: 1.402441\n",
            "[26,    25] loss: 1.316081\n",
            "[27,    25] loss: 1.275914\n",
            "[28,    25] loss: 1.399639\n",
            "[29,    25] loss: 1.279017\n",
            "[30,    25] loss: 1.067436\n",
            "[31,    25] loss: 1.075413\n",
            "[32,    25] loss: 1.193927\n",
            "[33,    25] loss: 1.364197\n",
            "[34,    25] loss: 1.299171\n",
            "[35,    25] loss: 1.063513\n",
            "[36,    25] loss: 1.352689\n",
            "[37,    25] loss: 1.086886\n",
            "[38,    25] loss: 1.158043\n",
            "[39,    25] loss: 1.300796\n",
            "[40,    25] loss: 1.184278\n",
            "[41,    25] loss: 1.383221\n",
            "[42,    25] loss: 1.035456\n",
            "[43,    25] loss: 1.071650\n",
            "[44,    25] loss: 0.914560\n",
            "[45,    25] loss: 1.047992\n",
            "[46,    25] loss: 1.206335\n",
            "[47,    25] loss: 1.083013\n",
            "[48,    25] loss: 1.033923\n",
            "[49,    25] loss: 1.160786\n",
            "[50,    25] loss: 0.978383\n",
            "[51,    25] loss: 0.969486\n",
            "[52,    25] loss: 0.908363\n",
            "[53,    25] loss: 1.009089\n",
            "[54,    25] loss: 0.916079\n",
            "[55,    25] loss: 0.962196\n",
            "[56,    25] loss: 1.126012\n",
            "[57,    25] loss: 1.111848\n",
            "[58,    25] loss: 1.241508\n",
            "[59,    25] loss: 1.097767\n",
            "[60,    25] loss: 0.933971\n",
            "[61,    25] loss: 1.389747\n",
            "[62,    25] loss: 1.042520\n",
            "[63,    25] loss: 1.269263\n",
            "[64,    25] loss: 1.147262\n",
            "[65,    25] loss: 1.035904\n",
            "[66,    25] loss: 1.015594\n",
            "[67,    25] loss: 1.001431\n",
            "[68,    25] loss: 0.887715\n",
            "[69,    25] loss: 1.007332\n",
            "[70,    25] loss: 1.035065\n",
            "[71,    25] loss: 1.002788\n",
            "[72,    25] loss: 1.025401\n",
            "[73,    25] loss: 1.084541\n",
            "[74,    25] loss: 1.181599\n",
            "[75,    25] loss: 1.233358\n",
            "[76,    25] loss: 0.756244\n",
            "[77,    25] loss: 0.943924\n",
            "[78,    25] loss: 1.043313\n",
            "[79,    25] loss: 0.892428\n",
            "[80,    25] loss: 0.904285\n",
            "[81,    25] loss: 1.054318\n",
            "[82,    25] loss: 0.911500\n",
            "[83,    25] loss: 0.961376\n",
            "[84,    25] loss: 0.815170\n",
            "[85,    25] loss: 0.999048\n",
            "[86,    25] loss: 0.927859\n",
            "[87,    25] loss: 0.919827\n",
            "[88,    25] loss: 1.314743\n",
            "[89,    25] loss: 0.985615\n",
            "[90,    25] loss: 1.085512\n",
            "[91,    25] loss: 1.040807\n",
            "[92,    25] loss: 0.824901\n",
            "[93,    25] loss: 0.941757\n",
            "[94,    25] loss: 1.129235\n",
            "[95,    25] loss: 1.129970\n",
            "[96,    25] loss: 0.910359\n",
            "[97,    25] loss: 0.883914\n",
            "[98,    25] loss: 0.725321\n",
            "[99,    25] loss: 0.681045\n",
            "[100,    25] loss: 0.854171\n",
            "[101,    25] loss: 0.854060\n",
            "[102,    25] loss: 0.782672\n",
            "[103,    25] loss: 0.918651\n",
            "[104,    25] loss: 0.946939\n",
            "[105,    25] loss: 1.025383\n",
            "[106,    25] loss: 0.922346\n",
            "[107,    25] loss: 1.017998\n",
            "[108,    25] loss: 1.131201\n",
            "[109,    25] loss: 0.886851\n",
            "[110,    25] loss: 0.963337\n",
            "[111,    25] loss: 0.913233\n",
            "[112,    25] loss: 1.279762\n",
            "[113,    25] loss: 1.010840\n",
            "[114,    25] loss: 0.886517\n",
            "[115,    25] loss: 0.857761\n",
            "[116,    25] loss: 0.868939\n",
            "[117,    25] loss: 0.976684\n",
            "[118,    25] loss: 0.833815\n",
            "[119,    25] loss: 1.321664\n",
            "[120,    25] loss: 1.158763\n",
            "[121,    25] loss: 0.928322\n",
            "[122,    25] loss: 0.852134\n",
            "[123,    25] loss: 1.011182\n",
            "[124,    25] loss: 0.980917\n",
            "[125,    25] loss: 0.746733\n",
            "[126,    25] loss: 0.694985\n",
            "[127,    25] loss: 0.842214\n",
            "[128,    25] loss: 0.838878\n",
            "[129,    25] loss: 0.879925\n",
            "[130,    25] loss: 0.769314\n",
            "[131,    25] loss: 0.711461\n",
            "[132,    25] loss: 0.708755\n",
            "[133,    25] loss: 0.888022\n",
            "[134,    25] loss: 1.103115\n",
            "[135,    25] loss: 0.918061\n",
            "[136,    25] loss: 0.915850\n",
            "[137,    25] loss: 0.937194\n",
            "[138,    25] loss: 1.033319\n",
            "[139,    25] loss: 0.877827\n",
            "[140,    25] loss: 0.650849\n",
            "[141,    25] loss: 0.895156\n",
            "[142,    25] loss: 0.729188\n",
            "[143,    25] loss: 0.618622\n",
            "[144,    25] loss: 0.748569\n",
            "[145,    25] loss: 0.823944\n",
            "[146,    25] loss: 0.993301\n",
            "[147,    25] loss: 1.090798\n",
            "[148,    25] loss: 1.310206\n",
            "[149,    25] loss: 1.007352\n",
            "[150,    25] loss: 1.151999\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.03439737726002932\n",
            "Test Accuracy -  77.1875\n",
            "L1 values -  [0.0368280079215765, 0.03824604116380215]\n",
            "L1 values -  [0.0368280079215765, 0.03439737726002932]\n",
            "Sparsity in fc1.weight: 20.31%\n",
            "Sparsity in fc2.weight: 21.29%\n",
            "Sparsity in fc3.weight: 19.19%\n",
            "Global sparsity: 20.00%\n",
            "Avg Loss during Testing -  0.06372473575174809\n",
            "Test Accuracy -  39.6875\n",
            "[1,    25] loss: 2.778466\n",
            "[2,    25] loss: 2.122773\n",
            "[3,    25] loss: 1.743979\n",
            "[4,    25] loss: 1.675602\n",
            "[5,    25] loss: 1.569693\n",
            "[6,    25] loss: 1.774918\n",
            "[7,    25] loss: 1.518583\n",
            "[8,    25] loss: 1.505349\n",
            "[9,    25] loss: 1.529784\n",
            "[10,    25] loss: 1.451511\n",
            "[11,    25] loss: 1.408883\n",
            "[12,    25] loss: 1.587939\n",
            "[13,    25] loss: 1.796065\n",
            "[14,    25] loss: 1.485776\n",
            "[15,    25] loss: 1.285693\n",
            "[16,    25] loss: 1.222068\n",
            "[17,    25] loss: 1.423055\n",
            "[18,    25] loss: 1.452438\n",
            "[19,    25] loss: 1.339932\n",
            "[20,    25] loss: 1.393789\n",
            "[21,    25] loss: 1.142182\n",
            "[22,    25] loss: 1.460957\n",
            "[23,    25] loss: 1.579958\n",
            "[24,    25] loss: 1.327974\n",
            "[25,    25] loss: 1.239109\n",
            "[26,    25] loss: 1.069941\n",
            "[27,    25] loss: 1.484543\n",
            "[28,    25] loss: 1.332285\n",
            "[29,    25] loss: 1.253508\n",
            "[30,    25] loss: 1.315745\n",
            "[31,    25] loss: 1.487379\n",
            "[32,    25] loss: 1.138493\n",
            "[33,    25] loss: 1.140444\n",
            "[34,    25] loss: 1.140574\n",
            "[35,    25] loss: 1.038614\n",
            "[36,    25] loss: 1.339072\n",
            "[37,    25] loss: 1.537284\n",
            "[38,    25] loss: 1.263064\n",
            "[39,    25] loss: 1.148841\n",
            "[40,    25] loss: 1.182832\n",
            "[41,    25] loss: 1.229203\n",
            "[42,    25] loss: 1.242044\n",
            "[43,    25] loss: 1.165445\n",
            "[44,    25] loss: 1.313135\n",
            "[45,    25] loss: 0.939371\n",
            "[46,    25] loss: 0.930052\n",
            "[47,    25] loss: 0.947320\n",
            "[48,    25] loss: 1.314673\n",
            "[49,    25] loss: 1.128141\n",
            "[50,    25] loss: 1.128350\n",
            "[51,    25] loss: 1.000952\n",
            "[52,    25] loss: 1.035682\n",
            "[53,    25] loss: 1.021796\n",
            "[54,    25] loss: 0.949603\n",
            "[55,    25] loss: 0.864318\n",
            "[56,    25] loss: 1.039391\n",
            "[57,    25] loss: 1.000538\n",
            "[58,    25] loss: 1.059043\n",
            "[59,    25] loss: 1.137043\n",
            "[60,    25] loss: 1.010143\n",
            "[61,    25] loss: 1.049447\n",
            "[62,    25] loss: 1.006852\n",
            "[63,    25] loss: 1.167539\n",
            "[64,    25] loss: 0.973091\n",
            "[65,    25] loss: 0.816486\n",
            "[66,    25] loss: 1.033022\n",
            "[67,    25] loss: 1.088090\n",
            "[68,    25] loss: 1.074795\n",
            "[69,    25] loss: 1.116773\n",
            "[70,    25] loss: 0.895491\n",
            "[71,    25] loss: 1.105587\n",
            "[72,    25] loss: 0.912856\n",
            "[73,    25] loss: 0.935923\n",
            "[74,    25] loss: 1.180157\n",
            "[75,    25] loss: 0.962398\n",
            "[76,    25] loss: 1.054590\n",
            "[77,    25] loss: 1.445154\n",
            "[78,    25] loss: 1.141487\n",
            "[79,    25] loss: 1.126659\n",
            "[80,    25] loss: 1.162318\n",
            "[81,    25] loss: 1.071018\n",
            "[82,    25] loss: 0.996305\n",
            "[83,    25] loss: 0.967054\n",
            "[84,    25] loss: 1.136471\n",
            "[85,    25] loss: 1.188844\n",
            "[86,    25] loss: 1.259385\n",
            "[87,    25] loss: 1.166817\n",
            "[88,    25] loss: 1.048869\n",
            "[89,    25] loss: 1.072709\n",
            "[90,    25] loss: 1.045971\n",
            "[91,    25] loss: 1.164768\n",
            "[92,    25] loss: 0.978109\n",
            "[93,    25] loss: 0.975463\n",
            "[94,    25] loss: 0.917385\n",
            "[95,    25] loss: 1.165688\n",
            "[96,    25] loss: 1.447895\n",
            "[97,    25] loss: 1.292007\n",
            "[98,    25] loss: 1.348971\n",
            "[99,    25] loss: 1.113404\n",
            "[100,    25] loss: 0.874633\n",
            "[101,    25] loss: 0.987167\n",
            "[102,    25] loss: 1.104442\n",
            "[103,    25] loss: 1.023265\n",
            "[104,    25] loss: 1.076720\n",
            "[105,    25] loss: 1.156158\n",
            "[106,    25] loss: 1.143521\n",
            "[107,    25] loss: 1.198003\n",
            "[108,    25] loss: 0.925144\n",
            "[109,    25] loss: 0.798307\n",
            "[110,    25] loss: 0.836676\n",
            "[111,    25] loss: 0.873657\n",
            "[112,    25] loss: 0.816826\n",
            "[113,    25] loss: 0.931745\n",
            "[114,    25] loss: 0.949311\n",
            "[115,    25] loss: 0.848267\n",
            "[116,    25] loss: 0.796718\n",
            "[117,    25] loss: 0.853381\n",
            "[118,    25] loss: 0.816980\n",
            "[119,    25] loss: 0.985373\n",
            "[120,    25] loss: 1.097509\n",
            "[121,    25] loss: 1.291525\n",
            "[122,    25] loss: 1.326620\n",
            "[123,    25] loss: 1.504873\n",
            "[124,    25] loss: 1.555209\n",
            "[125,    25] loss: 1.036595\n",
            "[126,    25] loss: 0.915707\n",
            "[127,    25] loss: 1.175183\n",
            "[128,    25] loss: 1.154280\n",
            "[129,    25] loss: 1.127740\n",
            "[130,    25] loss: 1.160098\n",
            "[131,    25] loss: 0.908465\n",
            "[132,    25] loss: 0.902165\n",
            "[133,    25] loss: 0.823776\n",
            "[134,    25] loss: 0.875961\n",
            "[135,    25] loss: 1.063118\n",
            "[136,    25] loss: 0.869071\n",
            "[137,    25] loss: 0.963327\n",
            "[138,    25] loss: 0.808956\n",
            "[139,    25] loss: 0.857527\n",
            "[140,    25] loss: 0.731524\n",
            "[141,    25] loss: 1.171853\n",
            "[142,    25] loss: 0.949496\n",
            "[143,    25] loss: 1.028513\n",
            "[144,    25] loss: 1.151977\n",
            "[145,    25] loss: 1.344632\n",
            "[146,    25] loss: 0.966645\n",
            "[147,    25] loss: 0.698502\n",
            "[148,    25] loss: 0.840818\n",
            "[149,    25] loss: 1.162723\n",
            "[150,    25] loss: 0.976252\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.04004273284226656\n",
            "Test Accuracy -  74.0625\n",
            "L2 values -  [0.0368280079215765, 0.06372473575174809]\n",
            "L2 values -  [0.0368280079215765, 0.04004273284226656]\n",
            "Sparsity in fc1.weight: 42.87%\n",
            "Sparsity in fc2.weight: 37.89%\n",
            "Sparsity in fc3.weight: 37.00%\n",
            "Global sparsity: 40.00%\n",
            "Avg Loss during Testing -  0.046889663115143775\n",
            "Test Accuracy -  58.4375\n",
            "[1,    25] loss: 2.291923\n",
            "[2,    25] loss: 1.632650\n",
            "[3,    25] loss: 1.457681\n",
            "[4,    25] loss: 1.556441\n",
            "[5,    25] loss: 1.476278\n",
            "[6,    25] loss: 1.428153\n",
            "[7,    25] loss: 1.427891\n",
            "[8,    25] loss: 1.439257\n",
            "[9,    25] loss: 1.374341\n",
            "[10,    25] loss: 1.389016\n",
            "[11,    25] loss: 1.463662\n",
            "[12,    25] loss: 1.366161\n",
            "[13,    25] loss: 1.173400\n",
            "[14,    25] loss: 1.265966\n",
            "[15,    25] loss: 1.234079\n",
            "[16,    25] loss: 1.276346\n",
            "[17,    25] loss: 1.200536\n",
            "[18,    25] loss: 1.150293\n",
            "[19,    25] loss: 1.180118\n",
            "[20,    25] loss: 1.175095\n",
            "[21,    25] loss: 1.304420\n",
            "[22,    25] loss: 1.099954\n",
            "[23,    25] loss: 1.076613\n",
            "[24,    25] loss: 1.204851\n",
            "[25,    25] loss: 1.108319\n",
            "[26,    25] loss: 1.095313\n",
            "[27,    25] loss: 1.086516\n",
            "[28,    25] loss: 1.019913\n",
            "[29,    25] loss: 1.176471\n",
            "[30,    25] loss: 1.135009\n",
            "[31,    25] loss: 1.255710\n",
            "[32,    25] loss: 1.083135\n",
            "[33,    25] loss: 1.099873\n",
            "[34,    25] loss: 0.992628\n",
            "[35,    25] loss: 1.127661\n",
            "[36,    25] loss: 1.004314\n",
            "[37,    25] loss: 1.037836\n",
            "[38,    25] loss: 1.056320\n",
            "[39,    25] loss: 1.205959\n",
            "[40,    25] loss: 1.105636\n",
            "[41,    25] loss: 1.031664\n",
            "[42,    25] loss: 0.963811\n",
            "[43,    25] loss: 0.948847\n",
            "[44,    25] loss: 0.926000\n",
            "[45,    25] loss: 0.956642\n",
            "[46,    25] loss: 1.106934\n",
            "[47,    25] loss: 1.161760\n",
            "[48,    25] loss: 1.207609\n",
            "[49,    25] loss: 1.188268\n",
            "[50,    25] loss: 1.024699\n",
            "[51,    25] loss: 1.332316\n",
            "[52,    25] loss: 1.101897\n",
            "[53,    25] loss: 0.924971\n",
            "[54,    25] loss: 0.934467\n",
            "[55,    25] loss: 0.906446\n",
            "[56,    25] loss: 0.819144\n",
            "[57,    25] loss: 1.056273\n",
            "[58,    25] loss: 1.068624\n",
            "[59,    25] loss: 1.016976\n",
            "[60,    25] loss: 0.950858\n",
            "[61,    25] loss: 0.811567\n",
            "[62,    25] loss: 0.920838\n",
            "[63,    25] loss: 0.894349\n",
            "[64,    25] loss: 0.939714\n",
            "[65,    25] loss: 1.025419\n",
            "[66,    25] loss: 0.974579\n",
            "[67,    25] loss: 1.100018\n",
            "[68,    25] loss: 1.108218\n",
            "[69,    25] loss: 1.067049\n",
            "[70,    25] loss: 0.862300\n",
            "[71,    25] loss: 0.751643\n",
            "[72,    25] loss: 0.982888\n",
            "[73,    25] loss: 0.958618\n",
            "[74,    25] loss: 0.891004\n",
            "[75,    25] loss: 0.949328\n",
            "[76,    25] loss: 0.917409\n",
            "[77,    25] loss: 0.988176\n",
            "[78,    25] loss: 0.994173\n",
            "[79,    25] loss: 0.833500\n",
            "[80,    25] loss: 0.891801\n",
            "[81,    25] loss: 0.861819\n",
            "[82,    25] loss: 1.151761\n",
            "[83,    25] loss: 1.087394\n",
            "[84,    25] loss: 0.910705\n",
            "[85,    25] loss: 1.095306\n",
            "[86,    25] loss: 0.985231\n",
            "[87,    25] loss: 0.788369\n",
            "[88,    25] loss: 0.911632\n",
            "[89,    25] loss: 0.912351\n",
            "[90,    25] loss: 0.812125\n",
            "[91,    25] loss: 0.837485\n",
            "[92,    25] loss: 0.765319\n",
            "[93,    25] loss: 0.758002\n",
            "[94,    25] loss: 0.757076\n",
            "[95,    25] loss: 1.197051\n",
            "[96,    25] loss: 1.074654\n",
            "[97,    25] loss: 0.843047\n",
            "[98,    25] loss: 0.930610\n",
            "[99,    25] loss: 1.007136\n",
            "[100,    25] loss: 0.945081\n",
            "[101,    25] loss: 0.900570\n",
            "[102,    25] loss: 0.931820\n",
            "[103,    25] loss: 0.660779\n",
            "[104,    25] loss: 0.804637\n",
            "[105,    25] loss: 0.801909\n",
            "[106,    25] loss: 0.788222\n",
            "[107,    25] loss: 0.731121\n",
            "[108,    25] loss: 1.076375\n",
            "[109,    25] loss: 1.088224\n",
            "[110,    25] loss: 1.099803\n",
            "[111,    25] loss: 0.965117\n",
            "[112,    25] loss: 0.894708\n",
            "[113,    25] loss: 0.829761\n",
            "[114,    25] loss: 0.714441\n",
            "[115,    25] loss: 0.850790\n",
            "[116,    25] loss: 0.922198\n",
            "[117,    25] loss: 0.763960\n",
            "[118,    25] loss: 0.696080\n",
            "[119,    25] loss: 0.880257\n",
            "[120,    25] loss: 0.855638\n",
            "[121,    25] loss: 0.742902\n",
            "[122,    25] loss: 0.832678\n",
            "[123,    25] loss: 0.922611\n",
            "[124,    25] loss: 0.939579\n",
            "[125,    25] loss: 0.805238\n",
            "[126,    25] loss: 0.800313\n",
            "[127,    25] loss: 0.721434\n",
            "[128,    25] loss: 0.864204\n",
            "[129,    25] loss: 0.916572\n",
            "[130,    25] loss: 1.203205\n",
            "[131,    25] loss: 0.806495\n",
            "[132,    25] loss: 0.810971\n",
            "[133,    25] loss: 1.127907\n",
            "[134,    25] loss: 1.155869\n",
            "[135,    25] loss: 1.084488\n",
            "[136,    25] loss: 0.921517\n",
            "[137,    25] loss: 1.031689\n",
            "[138,    25] loss: 0.819895\n",
            "[139,    25] loss: 0.847313\n",
            "[140,    25] loss: 1.361115\n",
            "[141,    25] loss: 1.058736\n",
            "[142,    25] loss: 0.873206\n",
            "[143,    25] loss: 1.071706\n",
            "[144,    25] loss: 1.058465\n",
            "[145,    25] loss: 0.802070\n",
            "[146,    25] loss: 0.997204\n",
            "[147,    25] loss: 1.074780\n",
            "[148,    25] loss: 0.857247\n",
            "[149,    25] loss: 0.911656\n",
            "[150,    25] loss: 0.806398\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.029042783565819262\n",
            "Test Accuracy -  76.5625\n",
            "L1 values -  [0.0368280079215765, 0.03824604116380215, 0.046889663115143775]\n",
            "L1 values -  [0.0368280079215765, 0.03439737726002932, 0.029042783565819262]\n",
            "Sparsity in fc1.weight: 41.31%\n",
            "Sparsity in fc2.weight: 41.41%\n",
            "Sparsity in fc3.weight: 37.88%\n",
            "Global sparsity: 40.00%\n",
            "Avg Loss during Testing -  0.09863003119826316\n",
            "Test Accuracy -  18.75\n",
            "[1,    25] loss: 3.161831\n",
            "[2,    25] loss: 2.126504\n",
            "[3,    25] loss: 1.860199\n",
            "[4,    25] loss: 1.664044\n",
            "[5,    25] loss: 1.558529\n",
            "[6,    25] loss: 1.717269\n",
            "[7,    25] loss: 1.585391\n",
            "[8,    25] loss: 1.338768\n",
            "[9,    25] loss: 1.395463\n",
            "[10,    25] loss: 1.454372\n",
            "[11,    25] loss: 1.502575\n",
            "[12,    25] loss: 1.333230\n",
            "[13,    25] loss: 1.171448\n",
            "[14,    25] loss: 1.207848\n",
            "[15,    25] loss: 1.194916\n",
            "[16,    25] loss: 1.282239\n",
            "[17,    25] loss: 1.305228\n",
            "[18,    25] loss: 1.540974\n",
            "[19,    25] loss: 1.244055\n",
            "[20,    25] loss: 1.075000\n",
            "[21,    25] loss: 1.174068\n",
            "[22,    25] loss: 1.141814\n",
            "[23,    25] loss: 1.112919\n",
            "[24,    25] loss: 1.134570\n",
            "[25,    25] loss: 1.263551\n",
            "[26,    25] loss: 1.288683\n",
            "[27,    25] loss: 1.113755\n",
            "[28,    25] loss: 1.067905\n",
            "[29,    25] loss: 1.103425\n",
            "[30,    25] loss: 1.116059\n",
            "[31,    25] loss: 1.106111\n",
            "[32,    25] loss: 1.080600\n",
            "[33,    25] loss: 1.036474\n",
            "[34,    25] loss: 1.174899\n",
            "[35,    25] loss: 0.972101\n",
            "[36,    25] loss: 1.249766\n",
            "[37,    25] loss: 1.032829\n",
            "[38,    25] loss: 1.138954\n",
            "[39,    25] loss: 1.040828\n",
            "[40,    25] loss: 0.989158\n",
            "[41,    25] loss: 1.013574\n",
            "[42,    25] loss: 1.015333\n",
            "[43,    25] loss: 1.100003\n",
            "[44,    25] loss: 1.075715\n",
            "[45,    25] loss: 1.447934\n",
            "[46,    25] loss: 1.363887\n",
            "[47,    25] loss: 1.011804\n",
            "[48,    25] loss: 0.996031\n",
            "[49,    25] loss: 1.276810\n",
            "[50,    25] loss: 1.400831\n",
            "[51,    25] loss: 1.199740\n",
            "[52,    25] loss: 1.001247\n",
            "[53,    25] loss: 1.037936\n",
            "[54,    25] loss: 0.998053\n",
            "[55,    25] loss: 1.128316\n",
            "[56,    25] loss: 1.047837\n",
            "[57,    25] loss: 0.886563\n",
            "[58,    25] loss: 0.947222\n",
            "[59,    25] loss: 0.976877\n",
            "[60,    25] loss: 0.822040\n",
            "[61,    25] loss: 0.870848\n",
            "[62,    25] loss: 0.962865\n",
            "[63,    25] loss: 1.001298\n",
            "[64,    25] loss: 0.959668\n",
            "[65,    25] loss: 1.204225\n",
            "[66,    25] loss: 0.957914\n",
            "[67,    25] loss: 0.951555\n",
            "[68,    25] loss: 1.225546\n",
            "[69,    25] loss: 1.208399\n",
            "[70,    25] loss: 0.957776\n",
            "[71,    25] loss: 0.910392\n",
            "[72,    25] loss: 1.073873\n",
            "[73,    25] loss: 1.051968\n",
            "[74,    25] loss: 0.964304\n",
            "[75,    25] loss: 0.969847\n",
            "[76,    25] loss: 0.827283\n",
            "[77,    25] loss: 1.076443\n",
            "[78,    25] loss: 1.016906\n",
            "[79,    25] loss: 0.896975\n",
            "[80,    25] loss: 1.077742\n",
            "[81,    25] loss: 0.944833\n",
            "[82,    25] loss: 0.935935\n",
            "[83,    25] loss: 0.962898\n",
            "[84,    25] loss: 1.011723\n",
            "[85,    25] loss: 0.844964\n",
            "[86,    25] loss: 0.891284\n",
            "[87,    25] loss: 0.846316\n",
            "[88,    25] loss: 0.986806\n",
            "[89,    25] loss: 1.089595\n",
            "[90,    25] loss: 0.811584\n",
            "[91,    25] loss: 0.689730\n",
            "[92,    25] loss: 0.859806\n",
            "[93,    25] loss: 0.945713\n",
            "[94,    25] loss: 0.993303\n",
            "[95,    25] loss: 1.022194\n",
            "[96,    25] loss: 1.037645\n",
            "[97,    25] loss: 1.036251\n",
            "[98,    25] loss: 0.950646\n",
            "[99,    25] loss: 0.718681\n",
            "[100,    25] loss: 0.809807\n",
            "[101,    25] loss: 0.906542\n",
            "[102,    25] loss: 0.747701\n",
            "[103,    25] loss: 0.757478\n",
            "[104,    25] loss: 0.853546\n",
            "[105,    25] loss: 0.785965\n",
            "[106,    25] loss: 0.863906\n",
            "[107,    25] loss: 0.760739\n",
            "[108,    25] loss: 1.050071\n",
            "[109,    25] loss: 0.854242\n",
            "[110,    25] loss: 1.038602\n",
            "[111,    25] loss: 0.869622\n",
            "[112,    25] loss: 0.914231\n",
            "[113,    25] loss: 1.140195\n",
            "[114,    25] loss: 1.140178\n",
            "[115,    25] loss: 0.937778\n",
            "[116,    25] loss: 1.022093\n",
            "[117,    25] loss: 1.009928\n",
            "[118,    25] loss: 0.955645\n",
            "[119,    25] loss: 0.929825\n",
            "[120,    25] loss: 1.089965\n",
            "[121,    25] loss: 0.989136\n",
            "[122,    25] loss: 0.835554\n",
            "[123,    25] loss: 0.690061\n",
            "[124,    25] loss: 0.831434\n",
            "[125,    25] loss: 0.776172\n",
            "[126,    25] loss: 0.995505\n",
            "[127,    25] loss: 0.920474\n",
            "[128,    25] loss: 0.884793\n",
            "[129,    25] loss: 0.806708\n",
            "[130,    25] loss: 0.761861\n",
            "[131,    25] loss: 0.991317\n",
            "[132,    25] loss: 0.931148\n",
            "[133,    25] loss: 0.847979\n",
            "[134,    25] loss: 0.922334\n",
            "[135,    25] loss: 0.852000\n",
            "[136,    25] loss: 1.064006\n",
            "[137,    25] loss: 0.986364\n",
            "[138,    25] loss: 0.921519\n",
            "[139,    25] loss: 0.796021\n",
            "[140,    25] loss: 1.396168\n",
            "[141,    25] loss: 1.099570\n",
            "[142,    25] loss: 0.781651\n",
            "[143,    25] loss: 0.942630\n",
            "[144,    25] loss: 0.904306\n",
            "[145,    25] loss: 1.011068\n",
            "[146,    25] loss: 0.805994\n",
            "[147,    25] loss: 0.791499\n",
            "[148,    25] loss: 0.923174\n",
            "[149,    25] loss: 0.906576\n",
            "[150,    25] loss: 0.753045\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.05285604279488325\n",
            "Test Accuracy -  67.5\n",
            "L2 values -  [0.0368280079215765, 0.06372473575174809, 0.09863003119826316]\n",
            "L2 values -  [0.0368280079215765, 0.04004273284226656, 0.05285604279488325]\n",
            "Sparsity in fc1.weight: 60.89%\n",
            "Sparsity in fc2.weight: 52.15%\n",
            "Sparsity in fc3.weight: 61.38%\n",
            "Global sparsity: 60.00%\n",
            "Avg Loss during Testing -  0.07239627912640571\n",
            "Test Accuracy -  35.0\n",
            "[1,    25] loss: 2.497426\n",
            "[2,    25] loss: 1.771492\n",
            "[3,    25] loss: 1.558702\n",
            "[4,    25] loss: 1.505288\n",
            "[5,    25] loss: 1.459216\n",
            "[6,    25] loss: 1.526225\n",
            "[7,    25] loss: 1.459595\n",
            "[8,    25] loss: 1.425718\n",
            "[9,    25] loss: 1.400801\n",
            "[10,    25] loss: 1.283758\n",
            "[11,    25] loss: 1.378804\n",
            "[12,    25] loss: 1.483952\n",
            "[13,    25] loss: 1.261881\n",
            "[14,    25] loss: 1.202475\n",
            "[15,    25] loss: 1.573776\n",
            "[16,    25] loss: 1.232548\n",
            "[17,    25] loss: 1.261019\n",
            "[18,    25] loss: 1.281230\n",
            "[19,    25] loss: 1.425170\n",
            "[20,    25] loss: 1.170364\n",
            "[21,    25] loss: 1.258497\n",
            "[22,    25] loss: 1.244368\n",
            "[23,    25] loss: 1.246520\n",
            "[24,    25] loss: 1.167262\n",
            "[25,    25] loss: 1.233415\n",
            "[26,    25] loss: 1.245910\n",
            "[27,    25] loss: 1.007512\n",
            "[28,    25] loss: 1.261640\n",
            "[29,    25] loss: 1.157582\n",
            "[30,    25] loss: 1.197882\n",
            "[31,    25] loss: 1.216858\n",
            "[32,    25] loss: 1.107590\n",
            "[33,    25] loss: 1.138962\n",
            "[34,    25] loss: 1.249415\n",
            "[35,    25] loss: 1.137773\n",
            "[36,    25] loss: 1.123191\n",
            "[37,    25] loss: 1.072231\n",
            "[38,    25] loss: 1.072419\n",
            "[39,    25] loss: 1.187504\n",
            "[40,    25] loss: 1.258473\n",
            "[41,    25] loss: 1.236580\n",
            "[42,    25] loss: 1.221935\n",
            "[43,    25] loss: 1.097810\n",
            "[44,    25] loss: 1.155413\n",
            "[45,    25] loss: 1.114797\n",
            "[46,    25] loss: 1.249670\n",
            "[47,    25] loss: 1.142716\n",
            "[48,    25] loss: 1.211934\n",
            "[49,    25] loss: 1.198762\n",
            "[50,    25] loss: 1.152624\n",
            "[51,    25] loss: 1.112029\n",
            "[52,    25] loss: 1.149591\n",
            "[53,    25] loss: 0.982381\n",
            "[54,    25] loss: 1.056789\n",
            "[55,    25] loss: 1.206758\n",
            "[56,    25] loss: 1.367965\n",
            "[57,    25] loss: 1.187152\n",
            "[58,    25] loss: 1.116114\n",
            "[59,    25] loss: 1.159018\n",
            "[60,    25] loss: 1.099857\n",
            "[61,    25] loss: 1.206827\n",
            "[62,    25] loss: 1.291745\n",
            "[63,    25] loss: 1.120152\n",
            "[64,    25] loss: 1.131290\n",
            "[65,    25] loss: 1.120742\n",
            "[66,    25] loss: 1.004975\n",
            "[67,    25] loss: 0.993790\n",
            "[68,    25] loss: 0.990373\n",
            "[69,    25] loss: 0.963515\n",
            "[70,    25] loss: 1.165128\n",
            "[71,    25] loss: 1.135600\n",
            "[72,    25] loss: 0.937412\n",
            "[73,    25] loss: 0.992168\n",
            "[74,    25] loss: 1.040949\n",
            "[75,    25] loss: 1.154231\n",
            "[76,    25] loss: 1.084051\n",
            "[77,    25] loss: 1.133190\n",
            "[78,    25] loss: 0.965929\n",
            "[79,    25] loss: 1.091854\n",
            "[80,    25] loss: 1.110222\n",
            "[81,    25] loss: 1.103168\n",
            "[82,    25] loss: 1.022359\n",
            "[83,    25] loss: 0.868076\n",
            "[84,    25] loss: 0.930876\n",
            "[85,    25] loss: 1.120269\n",
            "[86,    25] loss: 1.046144\n",
            "[87,    25] loss: 1.150538\n",
            "[88,    25] loss: 0.998265\n",
            "[89,    25] loss: 1.115617\n",
            "[90,    25] loss: 1.106702\n",
            "[91,    25] loss: 1.065646\n",
            "[92,    25] loss: 1.029538\n",
            "[93,    25] loss: 1.036593\n",
            "[94,    25] loss: 0.840777\n",
            "[95,    25] loss: 0.990923\n",
            "[96,    25] loss: 0.892816\n",
            "[97,    25] loss: 0.925115\n",
            "[98,    25] loss: 1.130895\n",
            "[99,    25] loss: 0.982812\n",
            "[100,    25] loss: 0.905134\n",
            "[101,    25] loss: 0.867151\n",
            "[102,    25] loss: 0.955690\n",
            "[103,    25] loss: 0.957764\n",
            "[104,    25] loss: 1.223183\n",
            "[105,    25] loss: 1.140006\n",
            "[106,    25] loss: 1.136625\n",
            "[107,    25] loss: 1.035118\n",
            "[108,    25] loss: 0.929712\n",
            "[109,    25] loss: 1.186337\n",
            "[110,    25] loss: 1.057224\n",
            "[111,    25] loss: 0.962431\n",
            "[112,    25] loss: 1.028638\n",
            "[113,    25] loss: 1.053912\n",
            "[114,    25] loss: 1.089453\n",
            "[115,    25] loss: 0.977902\n",
            "[116,    25] loss: 0.899064\n",
            "[117,    25] loss: 0.951321\n",
            "[118,    25] loss: 1.004039\n",
            "[119,    25] loss: 1.046852\n",
            "[120,    25] loss: 1.101723\n",
            "[121,    25] loss: 1.094199\n",
            "[122,    25] loss: 0.958394\n",
            "[123,    25] loss: 1.139690\n",
            "[124,    25] loss: 1.181429\n",
            "[125,    25] loss: 0.964504\n",
            "[126,    25] loss: 1.005770\n",
            "[127,    25] loss: 0.871458\n",
            "[128,    25] loss: 0.932754\n",
            "[129,    25] loss: 0.976756\n",
            "[130,    25] loss: 1.601800\n",
            "[131,    25] loss: 1.434720\n",
            "[132,    25] loss: 1.254394\n",
            "[133,    25] loss: 1.152217\n",
            "[134,    25] loss: 0.992032\n",
            "[135,    25] loss: 1.055375\n",
            "[136,    25] loss: 1.084668\n",
            "[137,    25] loss: 0.937749\n",
            "[138,    25] loss: 1.049729\n",
            "[139,    25] loss: 1.013375\n",
            "[140,    25] loss: 1.103103\n",
            "[141,    25] loss: 1.078923\n",
            "[142,    25] loss: 1.091314\n",
            "[143,    25] loss: 1.052724\n",
            "[144,    25] loss: 0.963330\n",
            "[145,    25] loss: 0.924948\n",
            "[146,    25] loss: 0.884407\n",
            "[147,    25] loss: 0.961113\n",
            "[148,    25] loss: 1.119874\n",
            "[149,    25] loss: 1.060200\n",
            "[150,    25] loss: 1.042033\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.03849522266536951\n",
            "Test Accuracy -  65.625\n",
            "L1 values -  [0.0368280079215765, 0.03824604116380215, 0.046889663115143775, 0.07239627912640571]\n",
            "L1 values -  [0.0368280079215765, 0.03439737726002932, 0.029042783565819262, 0.03849522266536951]\n",
            "Sparsity in fc1.weight: 58.84%\n",
            "Sparsity in fc2.weight: 58.59%\n",
            "Sparsity in fc3.weight: 61.94%\n",
            "Global sparsity: 60.00%\n",
            "Avg Loss during Testing -  0.12740116715431213\n",
            "Test Accuracy -  5.3125\n",
            "[1,    25] loss: 3.704641\n",
            "[2,    25] loss: 2.774058\n",
            "[3,    25] loss: 2.386679\n",
            "[4,    25] loss: 2.205877\n",
            "[5,    25] loss: 2.315830\n",
            "[6,    25] loss: 2.010472\n",
            "[7,    25] loss: 1.845232\n",
            "[8,    25] loss: 1.842884\n",
            "[9,    25] loss: 1.876837\n",
            "[10,    25] loss: 1.851240\n",
            "[11,    25] loss: 1.677329\n",
            "[12,    25] loss: 1.687826\n",
            "[13,    25] loss: 1.552410\n",
            "[14,    25] loss: 1.599944\n",
            "[15,    25] loss: 1.566848\n",
            "[16,    25] loss: 1.503966\n",
            "[17,    25] loss: 1.369116\n",
            "[18,    25] loss: 1.419761\n",
            "[19,    25] loss: 1.392889\n",
            "[20,    25] loss: 1.465548\n",
            "[21,    25] loss: 1.400374\n",
            "[22,    25] loss: 1.437662\n",
            "[23,    25] loss: 1.399393\n",
            "[24,    25] loss: 1.269687\n",
            "[25,    25] loss: 1.283305\n",
            "[26,    25] loss: 1.277575\n",
            "[27,    25] loss: 1.302407\n",
            "[28,    25] loss: 1.302668\n",
            "[29,    25] loss: 1.381061\n",
            "[30,    25] loss: 1.345266\n",
            "[31,    25] loss: 1.261463\n",
            "[32,    25] loss: 1.245598\n",
            "[33,    25] loss: 1.147503\n",
            "[34,    25] loss: 1.248532\n",
            "[35,    25] loss: 1.358503\n",
            "[36,    25] loss: 1.422103\n",
            "[37,    25] loss: 1.332094\n",
            "[38,    25] loss: 1.447519\n",
            "[39,    25] loss: 1.294338\n",
            "[40,    25] loss: 1.239606\n",
            "[41,    25] loss: 1.308287\n",
            "[42,    25] loss: 1.311278\n",
            "[43,    25] loss: 1.287142\n",
            "[44,    25] loss: 1.187494\n",
            "[45,    25] loss: 1.257197\n",
            "[46,    25] loss: 1.222057\n",
            "[47,    25] loss: 1.137660\n",
            "[48,    25] loss: 1.070093\n",
            "[49,    25] loss: 1.083222\n",
            "[50,    25] loss: 1.111658\n",
            "[51,    25] loss: 1.315468\n",
            "[52,    25] loss: 1.271113\n",
            "[53,    25] loss: 1.192936\n",
            "[54,    25] loss: 1.193797\n",
            "[55,    25] loss: 1.239810\n",
            "[56,    25] loss: 1.084906\n",
            "[57,    25] loss: 1.002964\n",
            "[58,    25] loss: 1.065755\n",
            "[59,    25] loss: 1.093963\n",
            "[60,    25] loss: 1.172478\n",
            "[61,    25] loss: 1.236660\n",
            "[62,    25] loss: 1.149638\n",
            "[63,    25] loss: 1.326897\n",
            "[64,    25] loss: 1.123573\n",
            "[65,    25] loss: 1.048193\n",
            "[66,    25] loss: 1.154428\n",
            "[67,    25] loss: 1.071540\n",
            "[68,    25] loss: 1.002135\n",
            "[69,    25] loss: 1.082483\n",
            "[70,    25] loss: 1.027271\n",
            "[71,    25] loss: 0.917289\n",
            "[72,    25] loss: 0.899983\n",
            "[73,    25] loss: 0.972105\n",
            "[74,    25] loss: 1.102055\n",
            "[75,    25] loss: 1.082648\n",
            "[76,    25] loss: 1.046173\n",
            "[77,    25] loss: 1.010166\n",
            "[78,    25] loss: 1.186356\n",
            "[79,    25] loss: 1.304749\n",
            "[80,    25] loss: 1.111693\n",
            "[81,    25] loss: 1.038332\n",
            "[82,    25] loss: 1.007528\n",
            "[83,    25] loss: 1.007598\n",
            "[84,    25] loss: 1.154584\n",
            "[85,    25] loss: 1.158347\n",
            "[86,    25] loss: 1.005531\n",
            "[87,    25] loss: 1.068087\n",
            "[88,    25] loss: 1.066189\n",
            "[89,    25] loss: 0.998256\n",
            "[90,    25] loss: 1.175099\n",
            "[91,    25] loss: 0.961819\n",
            "[92,    25] loss: 1.220362\n",
            "[93,    25] loss: 1.308988\n",
            "[94,    25] loss: 1.141726\n",
            "[95,    25] loss: 1.164111\n",
            "[96,    25] loss: 1.079756\n",
            "[97,    25] loss: 0.997626\n",
            "[98,    25] loss: 0.937268\n",
            "[99,    25] loss: 1.022657\n",
            "[100,    25] loss: 0.950569\n",
            "[101,    25] loss: 0.991789\n",
            "[102,    25] loss: 0.916132\n",
            "[103,    25] loss: 1.082183\n",
            "[104,    25] loss: 1.122210\n",
            "[105,    25] loss: 1.085660\n",
            "[106,    25] loss: 1.106840\n",
            "[107,    25] loss: 0.980507\n",
            "[108,    25] loss: 0.935048\n",
            "[109,    25] loss: 0.881181\n",
            "[110,    25] loss: 1.078562\n",
            "[111,    25] loss: 0.938211\n",
            "[112,    25] loss: 0.850171\n",
            "[113,    25] loss: 0.918043\n",
            "[114,    25] loss: 1.198624\n",
            "[115,    25] loss: 1.251587\n",
            "[116,    25] loss: 1.077504\n",
            "[117,    25] loss: 0.937590\n",
            "[118,    25] loss: 0.977465\n",
            "[119,    25] loss: 0.840038\n",
            "[120,    25] loss: 0.913492\n",
            "[121,    25] loss: 1.195332\n",
            "[122,    25] loss: 0.954860\n",
            "[123,    25] loss: 1.001546\n",
            "[124,    25] loss: 1.107820\n",
            "[125,    25] loss: 0.914087\n",
            "[126,    25] loss: 1.164014\n",
            "[127,    25] loss: 1.088093\n",
            "[128,    25] loss: 0.924779\n",
            "[129,    25] loss: 0.779665\n",
            "[130,    25] loss: 0.768836\n",
            "[131,    25] loss: 0.968191\n",
            "[132,    25] loss: 0.900124\n",
            "[133,    25] loss: 1.015751\n",
            "[134,    25] loss: 1.063295\n",
            "[135,    25] loss: 1.073196\n",
            "[136,    25] loss: 1.248472\n",
            "[137,    25] loss: 1.166046\n",
            "[138,    25] loss: 1.147610\n",
            "[139,    25] loss: 1.179210\n",
            "[140,    25] loss: 1.005410\n",
            "[141,    25] loss: 0.946443\n",
            "[142,    25] loss: 0.996189\n",
            "[143,    25] loss: 0.910297\n",
            "[144,    25] loss: 0.979791\n",
            "[145,    25] loss: 1.026249\n",
            "[146,    25] loss: 0.845910\n",
            "[147,    25] loss: 0.857769\n",
            "[148,    25] loss: 0.912521\n",
            "[149,    25] loss: 0.935766\n",
            "[150,    25] loss: 0.923171\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.0464575856924057\n",
            "Test Accuracy -  65.625\n",
            "L2 values -  [0.0368280079215765, 0.06372473575174809, 0.09863003119826316, 0.12740116715431213]\n",
            "L2 values -  [0.0368280079215765, 0.04004273284226656, 0.05285604279488325, 0.0464575856924057]\n",
            "Sparsity in fc1.weight: 78.91%\n",
            "Sparsity in fc2.weight: 70.70%\n",
            "Sparsity in fc3.weight: 84.38%\n",
            "Global sparsity: 80.00%\n",
            "Avg Loss during Testing -  0.11953216195106506\n",
            "Test Accuracy -  11.5625\n",
            "[1,    25] loss: 3.456096\n",
            "[2,    25] loss: 2.771805\n",
            "[3,    25] loss: 2.632350\n",
            "[4,    25] loss: 2.592366\n",
            "[5,    25] loss: 2.531033\n",
            "[6,    25] loss: 2.432632\n",
            "[7,    25] loss: 2.365410\n",
            "[8,    25] loss: 2.296153\n",
            "[9,    25] loss: 2.284529\n",
            "[10,    25] loss: 2.279459\n",
            "[11,    25] loss: 2.201053\n",
            "[12,    25] loss: 2.203732\n",
            "[13,    25] loss: 2.179999\n",
            "[14,    25] loss: 2.177035\n",
            "[15,    25] loss: 2.133884\n",
            "[16,    25] loss: 2.187893\n",
            "[17,    25] loss: 2.155526\n",
            "[18,    25] loss: 2.015266\n",
            "[19,    25] loss: 2.050663\n",
            "[20,    25] loss: 2.066277\n",
            "[21,    25] loss: 2.067242\n",
            "[22,    25] loss: 2.211657\n",
            "[23,    25] loss: 2.007951\n",
            "[24,    25] loss: 2.013537\n",
            "[25,    25] loss: 2.061892\n",
            "[26,    25] loss: 1.942010\n",
            "[27,    25] loss: 2.030732\n",
            "[28,    25] loss: 1.998248\n",
            "[29,    25] loss: 2.105577\n",
            "[30,    25] loss: 2.152392\n",
            "[31,    25] loss: 2.091792\n",
            "[32,    25] loss: 2.060053\n",
            "[33,    25] loss: 1.876240\n",
            "[34,    25] loss: 2.081715\n",
            "[35,    25] loss: 1.900539\n",
            "[36,    25] loss: 2.049771\n",
            "[37,    25] loss: 1.979781\n",
            "[38,    25] loss: 2.107718\n",
            "[39,    25] loss: 1.981441\n",
            "[40,    25] loss: 2.021052\n",
            "[41,    25] loss: 1.985192\n",
            "[42,    25] loss: 1.930065\n",
            "[43,    25] loss: 1.953937\n",
            "[44,    25] loss: 1.920330\n",
            "[45,    25] loss: 2.068753\n",
            "[46,    25] loss: 1.926280\n",
            "[47,    25] loss: 1.933708\n",
            "[48,    25] loss: 1.871541\n",
            "[49,    25] loss: 1.863968\n",
            "[50,    25] loss: 2.022205\n",
            "[51,    25] loss: 1.900383\n",
            "[52,    25] loss: 1.893474\n",
            "[53,    25] loss: 1.993957\n",
            "[54,    25] loss: 1.838587\n",
            "[55,    25] loss: 1.822299\n",
            "[56,    25] loss: 1.902858\n",
            "[57,    25] loss: 2.031021\n",
            "[58,    25] loss: 1.968968\n",
            "[59,    25] loss: 1.875517\n",
            "[60,    25] loss: 1.897077\n",
            "[61,    25] loss: 1.781665\n",
            "[62,    25] loss: 1.912096\n",
            "[63,    25] loss: 2.067573\n",
            "[64,    25] loss: 1.942584\n",
            "[65,    25] loss: 1.864540\n",
            "[66,    25] loss: 1.977063\n",
            "[67,    25] loss: 1.962792\n",
            "[68,    25] loss: 1.901342\n",
            "[69,    25] loss: 1.868658\n",
            "[70,    25] loss: 1.849174\n",
            "[71,    25] loss: 1.813640\n",
            "[72,    25] loss: 1.743495\n",
            "[73,    25] loss: 1.741496\n",
            "[74,    25] loss: 1.796126\n",
            "[75,    25] loss: 1.870584\n",
            "[76,    25] loss: 2.015726\n",
            "[77,    25] loss: 1.972708\n",
            "[78,    25] loss: 1.880521\n",
            "[79,    25] loss: 1.802007\n",
            "[80,    25] loss: 1.896370\n",
            "[81,    25] loss: 1.909846\n",
            "[82,    25] loss: 1.886493\n",
            "[83,    25] loss: 1.884594\n",
            "[84,    25] loss: 1.875492\n",
            "[85,    25] loss: 1.931818\n",
            "[86,    25] loss: 1.847737\n",
            "[87,    25] loss: 1.841886\n",
            "[88,    25] loss: 1.733956\n",
            "[89,    25] loss: 1.884682\n",
            "[90,    25] loss: 1.712704\n",
            "[91,    25] loss: 1.775879\n",
            "[92,    25] loss: 1.726488\n",
            "[93,    25] loss: 1.937218\n",
            "[94,    25] loss: 1.824439\n",
            "[95,    25] loss: 2.195590\n",
            "[96,    25] loss: 1.931741\n",
            "[97,    25] loss: 1.774506\n",
            "[98,    25] loss: 1.763305\n",
            "[99,    25] loss: 1.917108\n",
            "[100,    25] loss: 2.003574\n",
            "[101,    25] loss: 1.786932\n",
            "[102,    25] loss: 1.805907\n",
            "[103,    25] loss: 1.702505\n",
            "[104,    25] loss: 1.718373\n",
            "[105,    25] loss: 1.861288\n",
            "[106,    25] loss: 1.865700\n",
            "[107,    25] loss: 1.759117\n",
            "[108,    25] loss: 1.837506\n",
            "[109,    25] loss: 1.825407\n",
            "[110,    25] loss: 1.965395\n",
            "[111,    25] loss: 1.889466\n",
            "[112,    25] loss: 1.753770\n",
            "[113,    25] loss: 1.754428\n",
            "[114,    25] loss: 1.725804\n",
            "[115,    25] loss: 1.895842\n",
            "[116,    25] loss: 1.961089\n",
            "[117,    25] loss: 1.727709\n",
            "[118,    25] loss: 1.896996\n",
            "[119,    25] loss: 1.705255\n",
            "[120,    25] loss: 1.944727\n",
            "[121,    25] loss: 1.801430\n",
            "[122,    25] loss: 1.818163\n",
            "[123,    25] loss: 1.740464\n",
            "[124,    25] loss: 2.019386\n",
            "[125,    25] loss: 1.741483\n",
            "[126,    25] loss: 1.796045\n",
            "[127,    25] loss: 1.795653\n",
            "[128,    25] loss: 1.791113\n",
            "[129,    25] loss: 1.719788\n",
            "[130,    25] loss: 1.703247\n",
            "[131,    25] loss: 1.826693\n",
            "[132,    25] loss: 1.759172\n",
            "[133,    25] loss: 1.800366\n",
            "[134,    25] loss: 1.774939\n",
            "[135,    25] loss: 1.803847\n",
            "[136,    25] loss: 1.726416\n",
            "[137,    25] loss: 1.740929\n",
            "[138,    25] loss: 1.750263\n",
            "[139,    25] loss: 1.748479\n",
            "[140,    25] loss: 1.751885\n",
            "[141,    25] loss: 1.887041\n",
            "[142,    25] loss: 1.824213\n",
            "[143,    25] loss: 1.936464\n",
            "[144,    25] loss: 1.840019\n",
            "[145,    25] loss: 1.791040\n",
            "[146,    25] loss: 1.796393\n",
            "[147,    25] loss: 1.774513\n",
            "[148,    25] loss: 1.879074\n",
            "[149,    25] loss: 1.821955\n",
            "[150,    25] loss: 1.903867\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.0538621798157692\n",
            "Test Accuracy -  52.8125\n",
            "L1 values -  [0.0368280079215765, 0.03824604116380215, 0.046889663115143775, 0.07239627912640571, 0.11953216195106506]\n",
            "L1 values -  [0.0368280079215765, 0.03439737726002932, 0.029042783565819262, 0.03849522266536951, 0.0538621798157692]\n",
            "Sparsity in fc1.weight: 81.40%\n",
            "Sparsity in fc2.weight: 76.95%\n",
            "Sparsity in fc3.weight: 79.19%\n",
            "Global sparsity: 80.00%\n",
            "Avg Loss during Testing -  0.13979460597038268\n",
            "Test Accuracy -  3.4375\n",
            "[1,    25] loss: 4.312885\n",
            "[2,    25] loss: 3.543370\n",
            "[3,    25] loss: 3.128840\n",
            "[4,    25] loss: 2.907699\n",
            "[5,    25] loss: 2.734717\n",
            "[6,    25] loss: 2.584031\n",
            "[7,    25] loss: 2.493266\n",
            "[8,    25] loss: 2.521250\n",
            "[9,    25] loss: 2.410265\n",
            "[10,    25] loss: 2.419100\n",
            "[11,    25] loss: 2.368431\n",
            "[12,    25] loss: 2.261643\n",
            "[13,    25] loss: 2.162695\n",
            "[14,    25] loss: 2.193190\n",
            "[15,    25] loss: 2.219030\n",
            "[16,    25] loss: 2.292495\n",
            "[17,    25] loss: 2.148036\n",
            "[18,    25] loss: 2.100572\n",
            "[19,    25] loss: 1.986465\n",
            "[20,    25] loss: 2.052480\n",
            "[21,    25] loss: 2.131133\n",
            "[22,    25] loss: 2.069440\n",
            "[23,    25] loss: 1.988445\n",
            "[24,    25] loss: 2.068747\n",
            "[25,    25] loss: 1.971869\n",
            "[26,    25] loss: 1.995514\n",
            "[27,    25] loss: 2.002254\n",
            "[28,    25] loss: 1.976455\n",
            "[29,    25] loss: 2.010593\n",
            "[30,    25] loss: 1.980357\n",
            "[31,    25] loss: 1.898897\n",
            "[32,    25] loss: 1.848978\n",
            "[33,    25] loss: 1.908345\n",
            "[34,    25] loss: 2.054639\n",
            "[35,    25] loss: 2.069218\n",
            "[36,    25] loss: 1.963748\n",
            "[37,    25] loss: 1.882400\n",
            "[38,    25] loss: 1.846751\n",
            "[39,    25] loss: 1.887030\n",
            "[40,    25] loss: 1.820015\n",
            "[41,    25] loss: 1.850965\n",
            "[42,    25] loss: 1.812264\n",
            "[43,    25] loss: 1.845960\n",
            "[44,    25] loss: 1.823649\n",
            "[45,    25] loss: 1.914014\n",
            "[46,    25] loss: 1.810612\n",
            "[47,    25] loss: 1.766822\n",
            "[48,    25] loss: 1.862029\n",
            "[49,    25] loss: 1.804579\n",
            "[50,    25] loss: 1.819911\n",
            "[51,    25] loss: 1.791397\n",
            "[52,    25] loss: 1.707939\n",
            "[53,    25] loss: 1.809005\n",
            "[54,    25] loss: 1.765719\n",
            "[55,    25] loss: 1.762399\n",
            "[56,    25] loss: 1.728617\n",
            "[57,    25] loss: 1.803938\n",
            "[58,    25] loss: 1.752204\n",
            "[59,    25] loss: 1.693363\n",
            "[60,    25] loss: 1.743911\n",
            "[61,    25] loss: 1.798331\n",
            "[62,    25] loss: 1.774200\n",
            "[63,    25] loss: 1.690055\n",
            "[64,    25] loss: 1.653241\n",
            "[65,    25] loss: 1.734889\n",
            "[66,    25] loss: 1.630296\n",
            "[67,    25] loss: 1.620427\n",
            "[68,    25] loss: 1.694525\n",
            "[69,    25] loss: 1.673950\n",
            "[70,    25] loss: 1.609592\n",
            "[71,    25] loss: 1.592030\n",
            "[72,    25] loss: 1.788611\n",
            "[73,    25] loss: 1.638433\n",
            "[74,    25] loss: 1.753594\n",
            "[75,    25] loss: 1.678237\n",
            "[76,    25] loss: 1.598767\n",
            "[77,    25] loss: 1.613085\n",
            "[78,    25] loss: 1.587081\n",
            "[79,    25] loss: 1.587371\n",
            "[80,    25] loss: 1.687314\n",
            "[81,    25] loss: 1.653557\n",
            "[82,    25] loss: 1.700861\n",
            "[83,    25] loss: 1.709839\n",
            "[84,    25] loss: 1.646005\n",
            "[85,    25] loss: 1.713062\n",
            "[86,    25] loss: 1.609477\n",
            "[87,    25] loss: 1.672835\n",
            "[88,    25] loss: 1.709675\n",
            "[89,    25] loss: 1.723149\n",
            "[90,    25] loss: 1.767390\n",
            "[91,    25] loss: 1.659499\n",
            "[92,    25] loss: 1.614229\n",
            "[93,    25] loss: 1.615196\n",
            "[94,    25] loss: 1.605710\n",
            "[95,    25] loss: 1.794667\n",
            "[96,    25] loss: 1.612704\n",
            "[97,    25] loss: 1.636168\n",
            "[98,    25] loss: 1.627866\n",
            "[99,    25] loss: 1.564323\n",
            "[100,    25] loss: 1.640209\n",
            "[101,    25] loss: 1.739779\n",
            "[102,    25] loss: 1.645815\n",
            "[103,    25] loss: 1.514421\n",
            "[104,    25] loss: 1.499929\n",
            "[105,    25] loss: 1.624382\n",
            "[106,    25] loss: 1.602934\n",
            "[107,    25] loss: 1.526930\n",
            "[108,    25] loss: 1.581653\n",
            "[109,    25] loss: 1.527714\n",
            "[110,    25] loss: 1.715069\n",
            "[111,    25] loss: 1.625780\n",
            "[112,    25] loss: 1.659604\n",
            "[113,    25] loss: 1.649783\n",
            "[114,    25] loss: 1.489106\n",
            "[115,    25] loss: 1.578087\n",
            "[116,    25] loss: 1.555206\n",
            "[117,    25] loss: 1.711000\n",
            "[118,    25] loss: 1.597992\n",
            "[119,    25] loss: 1.587851\n",
            "[120,    25] loss: 1.572252\n",
            "[121,    25] loss: 1.686528\n",
            "[122,    25] loss: 1.528703\n",
            "[123,    25] loss: 1.734879\n",
            "[124,    25] loss: 1.517950\n",
            "[125,    25] loss: 1.504676\n",
            "[126,    25] loss: 1.597641\n",
            "[127,    25] loss: 1.484090\n",
            "[128,    25] loss: 1.431909\n",
            "[129,    25] loss: 1.526994\n",
            "[130,    25] loss: 1.495125\n",
            "[131,    25] loss: 1.575520\n",
            "[132,    25] loss: 1.572799\n",
            "[133,    25] loss: 1.462353\n",
            "[134,    25] loss: 1.543336\n",
            "[135,    25] loss: 1.501524\n",
            "[136,    25] loss: 1.609142\n",
            "[137,    25] loss: 1.541151\n",
            "[138,    25] loss: 1.600352\n",
            "[139,    25] loss: 1.654823\n",
            "[140,    25] loss: 1.568197\n",
            "[141,    25] loss: 1.483740\n",
            "[142,    25] loss: 1.590594\n",
            "[143,    25] loss: 1.639233\n",
            "[144,    25] loss: 1.458648\n",
            "[145,    25] loss: 1.637518\n",
            "[146,    25] loss: 1.618025\n",
            "[147,    25] loss: 1.540503\n",
            "[148,    25] loss: 1.549971\n",
            "[149,    25] loss: 1.493824\n",
            "[150,    25] loss: 1.486491\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.044045723602175714\n",
            "Test Accuracy -  63.4375\n",
            "L2 values -  [0.0368280079215765, 0.06372473575174809, 0.09863003119826316, 0.12740116715431213, 0.13979460597038268]\n",
            "L2 values -  [0.0368280079215765, 0.04004273284226656, 0.05285604279488325, 0.0464575856924057, 0.044045723602175714]\n",
            "Sparsity in fc1.weight: 88.67%\n",
            "Sparsity in fc2.weight: 82.03%\n",
            "Sparsity in fc3.weight: 94.25%\n",
            "Global sparsity: 90.00%\n",
            "Avg Loss during Testing -  0.14299183040857316\n",
            "Test Accuracy -  1.875\n",
            "[1,    25] loss: 4.414650\n",
            "[2,    25] loss: 4.003400\n",
            "[3,    25] loss: 3.917237\n",
            "[4,    25] loss: 3.682186\n",
            "[5,    25] loss: 3.709441\n",
            "[6,    25] loss: 3.643785\n",
            "[7,    25] loss: 3.537989\n",
            "[8,    25] loss: 3.537750\n",
            "[9,    25] loss: 3.561512\n",
            "[10,    25] loss: 3.543448\n",
            "[11,    25] loss: 3.509951\n",
            "[12,    25] loss: 3.564041\n",
            "[13,    25] loss: 3.481305\n",
            "[14,    25] loss: 3.453705\n",
            "[15,    25] loss: 3.503383\n",
            "[16,    25] loss: 3.453720\n",
            "[17,    25] loss: 3.522169\n",
            "[18,    25] loss: 3.443572\n",
            "[19,    25] loss: 3.390183\n",
            "[20,    25] loss: 3.402640\n",
            "[21,    25] loss: 3.459481\n",
            "[22,    25] loss: 3.357888\n",
            "[23,    25] loss: 3.320057\n",
            "[24,    25] loss: 3.462624\n",
            "[25,    25] loss: 3.457560\n",
            "[26,    25] loss: 3.337502\n",
            "[27,    25] loss: 3.353032\n",
            "[28,    25] loss: 3.275091\n",
            "[29,    25] loss: 3.448233\n",
            "[30,    25] loss: 3.392199\n",
            "[31,    25] loss: 3.404761\n",
            "[32,    25] loss: 3.351345\n",
            "[33,    25] loss: 3.317914\n",
            "[34,    25] loss: 3.262858\n",
            "[35,    25] loss: 3.333521\n",
            "[36,    25] loss: 3.290234\n",
            "[37,    25] loss: 3.265235\n",
            "[38,    25] loss: 3.337529\n",
            "[39,    25] loss: 3.256338\n",
            "[40,    25] loss: 3.298384\n",
            "[41,    25] loss: 3.336904\n",
            "[42,    25] loss: 3.277990\n",
            "[43,    25] loss: 3.244101\n",
            "[44,    25] loss: 3.257913\n",
            "[45,    25] loss: 3.266850\n",
            "[46,    25] loss: 3.225578\n",
            "[47,    25] loss: 3.279030\n",
            "[48,    25] loss: 3.281095\n",
            "[49,    25] loss: 3.389641\n",
            "[50,    25] loss: 3.292905\n",
            "[51,    25] loss: 3.295210\n",
            "[52,    25] loss: 3.283932\n",
            "[53,    25] loss: 3.305332\n",
            "[54,    25] loss: 3.287886\n",
            "[55,    25] loss: 3.304907\n",
            "[56,    25] loss: 3.277264\n",
            "[57,    25] loss: 3.263357\n",
            "[58,    25] loss: 3.154216\n",
            "[59,    25] loss: 3.301146\n",
            "[60,    25] loss: 3.213899\n",
            "[61,    25] loss: 3.248347\n",
            "[62,    25] loss: 3.227037\n",
            "[63,    25] loss: 3.267091\n",
            "[64,    25] loss: 3.218673\n",
            "[65,    25] loss: 3.187438\n",
            "[66,    25] loss: 3.257570\n",
            "[67,    25] loss: 3.090337\n",
            "[68,    25] loss: 3.275819\n",
            "[69,    25] loss: 3.222701\n",
            "[70,    25] loss: 3.253872\n",
            "[71,    25] loss: 3.150782\n",
            "[72,    25] loss: 3.197448\n",
            "[73,    25] loss: 3.262259\n",
            "[74,    25] loss: 3.256935\n",
            "[75,    25] loss: 3.273646\n",
            "[76,    25] loss: 3.181257\n",
            "[77,    25] loss: 3.159153\n",
            "[78,    25] loss: 3.248967\n",
            "[79,    25] loss: 3.278252\n",
            "[80,    25] loss: 3.204075\n",
            "[81,    25] loss: 3.199146\n",
            "[82,    25] loss: 3.127477\n",
            "[83,    25] loss: 3.195619\n",
            "[84,    25] loss: 3.176194\n",
            "[85,    25] loss: 3.225550\n",
            "[86,    25] loss: 3.201236\n",
            "[87,    25] loss: 3.370636\n",
            "[88,    25] loss: 3.184138\n",
            "[89,    25] loss: 3.285472\n",
            "[90,    25] loss: 3.337778\n",
            "[91,    25] loss: 3.191493\n",
            "[92,    25] loss: 3.276067\n",
            "[93,    25] loss: 3.267944\n",
            "[94,    25] loss: 3.247791\n",
            "[95,    25] loss: 3.243482\n",
            "[96,    25] loss: 3.220841\n",
            "[97,    25] loss: 3.059584\n",
            "[98,    25] loss: 3.098454\n",
            "[99,    25] loss: 3.168786\n",
            "[100,    25] loss: 3.240029\n",
            "[101,    25] loss: 3.140638\n",
            "[102,    25] loss: 3.124321\n",
            "[103,    25] loss: 3.216072\n",
            "[104,    25] loss: 3.282592\n",
            "[105,    25] loss: 3.179822\n",
            "[106,    25] loss: 3.099284\n",
            "[107,    25] loss: 3.245002\n",
            "[108,    25] loss: 3.286807\n",
            "[109,    25] loss: 3.154136\n",
            "[110,    25] loss: 3.191792\n",
            "[111,    25] loss: 3.204278\n",
            "[112,    25] loss: 3.212347\n",
            "[113,    25] loss: 3.257934\n",
            "[114,    25] loss: 3.205000\n",
            "[115,    25] loss: 3.188890\n",
            "[116,    25] loss: 3.189305\n",
            "[117,    25] loss: 3.176377\n",
            "[118,    25] loss: 3.131421\n",
            "[119,    25] loss: 3.124600\n",
            "[120,    25] loss: 3.166041\n",
            "[121,    25] loss: 3.177966\n",
            "[122,    25] loss: 3.044299\n",
            "[123,    25] loss: 3.145971\n",
            "[124,    25] loss: 3.188050\n",
            "[125,    25] loss: 3.148307\n",
            "[126,    25] loss: 3.043023\n",
            "[127,    25] loss: 3.171948\n",
            "[128,    25] loss: 3.179937\n",
            "[129,    25] loss: 3.147733\n",
            "[130,    25] loss: 3.219153\n",
            "[131,    25] loss: 3.156253\n",
            "[132,    25] loss: 3.173093\n",
            "[133,    25] loss: 3.082418\n",
            "[134,    25] loss: 3.109098\n",
            "[135,    25] loss: 3.171803\n",
            "[136,    25] loss: 3.206794\n",
            "[137,    25] loss: 3.159945\n",
            "[138,    25] loss: 3.104558\n",
            "[139,    25] loss: 3.147600\n",
            "[140,    25] loss: 3.047638\n",
            "[141,    25] loss: 3.123093\n",
            "[142,    25] loss: 3.305928\n",
            "[143,    25] loss: 3.140018\n",
            "[144,    25] loss: 3.020409\n",
            "[145,    25] loss: 3.110395\n",
            "[146,    25] loss: 3.123073\n",
            "[147,    25] loss: 3.150383\n",
            "[148,    25] loss: 3.148830\n",
            "[149,    25] loss: 3.160941\n",
            "[150,    25] loss: 3.177916\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.09886588528752327\n",
            "Test Accuracy -  27.5\n",
            "L1 values -  [0.0368280079215765, 0.03824604116380215, 0.046889663115143775, 0.07239627912640571, 0.11953216195106506, 0.14299183040857316]\n",
            "L1 values -  [0.0368280079215765, 0.03439737726002932, 0.029042783565819262, 0.03849522266536951, 0.0538621798157692, 0.09886588528752327]\n",
            "Sparsity in fc1.weight: 90.23%\n",
            "Sparsity in fc2.weight: 90.43%\n",
            "Sparsity in fc3.weight: 89.56%\n",
            "Global sparsity: 90.00%\n",
            "Avg Loss during Testing -  0.14633402675390245\n",
            "Test Accuracy -  0.9375\n",
            "[1,    25] loss: 4.694818\n",
            "[2,    25] loss: 4.381441\n",
            "[3,    25] loss: 4.046503\n",
            "[4,    25] loss: 3.917707\n",
            "[5,    25] loss: 3.800551\n",
            "[6,    25] loss: 3.745781\n",
            "[7,    25] loss: 3.642976\n",
            "[8,    25] loss: 3.688250\n",
            "[9,    25] loss: 3.609786\n",
            "[10,    25] loss: 3.573078\n",
            "[11,    25] loss: 3.517830\n",
            "[12,    25] loss: 3.515610\n",
            "[13,    25] loss: 3.507857\n",
            "[14,    25] loss: 3.497046\n",
            "[15,    25] loss: 3.504056\n",
            "[16,    25] loss: 3.485412\n",
            "[17,    25] loss: 3.434217\n",
            "[18,    25] loss: 3.453093\n",
            "[19,    25] loss: 3.451229\n",
            "[20,    25] loss: 3.413129\n",
            "[21,    25] loss: 3.389723\n",
            "[22,    25] loss: 3.396763\n",
            "[23,    25] loss: 3.369210\n",
            "[24,    25] loss: 3.361634\n",
            "[25,    25] loss: 3.332547\n",
            "[26,    25] loss: 3.348437\n",
            "[27,    25] loss: 3.346052\n",
            "[28,    25] loss: 3.400062\n",
            "[29,    25] loss: 3.379917\n",
            "[30,    25] loss: 3.308837\n",
            "[31,    25] loss: 3.326669\n",
            "[32,    25] loss: 3.284967\n",
            "[33,    25] loss: 3.358565\n",
            "[34,    25] loss: 3.337427\n",
            "[35,    25] loss: 3.221845\n",
            "[36,    25] loss: 3.261644\n",
            "[37,    25] loss: 3.256721\n",
            "[38,    25] loss: 3.272572\n",
            "[39,    25] loss: 3.227758\n",
            "[40,    25] loss: 3.272134\n",
            "[41,    25] loss: 3.208148\n",
            "[42,    25] loss: 3.282075\n",
            "[43,    25] loss: 3.249175\n",
            "[44,    25] loss: 3.231110\n",
            "[45,    25] loss: 3.238306\n",
            "[46,    25] loss: 3.294103\n",
            "[47,    25] loss: 3.226266\n",
            "[48,    25] loss: 3.153169\n",
            "[49,    25] loss: 3.251970\n",
            "[50,    25] loss: 3.228720\n",
            "[51,    25] loss: 3.258396\n",
            "[52,    25] loss: 3.246789\n",
            "[53,    25] loss: 3.155682\n",
            "[54,    25] loss: 3.245922\n",
            "[55,    25] loss: 3.150711\n",
            "[56,    25] loss: 3.214823\n",
            "[57,    25] loss: 3.209506\n",
            "[58,    25] loss: 3.148361\n",
            "[59,    25] loss: 3.159291\n",
            "[60,    25] loss: 3.199607\n",
            "[61,    25] loss: 3.165755\n",
            "[62,    25] loss: 3.169973\n",
            "[63,    25] loss: 3.172208\n",
            "[64,    25] loss: 3.123294\n",
            "[65,    25] loss: 3.214175\n",
            "[66,    25] loss: 3.083291\n",
            "[67,    25] loss: 3.093624\n",
            "[68,    25] loss: 3.067331\n",
            "[69,    25] loss: 3.128978\n",
            "[70,    25] loss: 3.146497\n",
            "[71,    25] loss: 3.161414\n",
            "[72,    25] loss: 3.097887\n",
            "[73,    25] loss: 3.089784\n",
            "[74,    25] loss: 3.114941\n",
            "[75,    25] loss: 3.209195\n",
            "[76,    25] loss: 3.212832\n",
            "[77,    25] loss: 3.119401\n",
            "[78,    25] loss: 3.142422\n",
            "[79,    25] loss: 3.105797\n",
            "[80,    25] loss: 3.017181\n",
            "[81,    25] loss: 3.092165\n",
            "[82,    25] loss: 3.185017\n",
            "[83,    25] loss: 3.073254\n",
            "[84,    25] loss: 3.181106\n",
            "[85,    25] loss: 3.127926\n",
            "[86,    25] loss: 3.089930\n",
            "[87,    25] loss: 3.145745\n",
            "[88,    25] loss: 3.113148\n",
            "[89,    25] loss: 3.132304\n",
            "[90,    25] loss: 3.159965\n",
            "[91,    25] loss: 3.171181\n",
            "[92,    25] loss: 3.105602\n",
            "[93,    25] loss: 3.084778\n",
            "[94,    25] loss: 3.250781\n",
            "[95,    25] loss: 3.216451\n",
            "[96,    25] loss: 3.091448\n",
            "[97,    25] loss: 3.089584\n",
            "[98,    25] loss: 3.105194\n",
            "[99,    25] loss: 3.127771\n",
            "[100,    25] loss: 3.122201\n",
            "[101,    25] loss: 3.092768\n",
            "[102,    25] loss: 3.048474\n",
            "[103,    25] loss: 3.029426\n",
            "[104,    25] loss: 3.084279\n",
            "[105,    25] loss: 3.129309\n",
            "[106,    25] loss: 3.113867\n",
            "[107,    25] loss: 3.094505\n",
            "[108,    25] loss: 3.127550\n",
            "[109,    25] loss: 3.021359\n",
            "[110,    25] loss: 3.153063\n",
            "[111,    25] loss: 3.117481\n",
            "[112,    25] loss: 3.085351\n",
            "[113,    25] loss: 3.064998\n",
            "[114,    25] loss: 3.184761\n",
            "[115,    25] loss: 3.179915\n",
            "[116,    25] loss: 3.170738\n",
            "[117,    25] loss: 3.030990\n",
            "[118,    25] loss: 3.117925\n",
            "[119,    25] loss: 3.063179\n",
            "[120,    25] loss: 3.029050\n",
            "[121,    25] loss: 3.102197\n",
            "[122,    25] loss: 3.057331\n",
            "[123,    25] loss: 3.139505\n",
            "[124,    25] loss: 3.234736\n",
            "[125,    25] loss: 3.033353\n",
            "[126,    25] loss: 3.147814\n",
            "[127,    25] loss: 3.047464\n",
            "[128,    25] loss: 3.104305\n",
            "[129,    25] loss: 3.136925\n",
            "[130,    25] loss: 3.115712\n",
            "[131,    25] loss: 3.105192\n",
            "[132,    25] loss: 3.077345\n",
            "[133,    25] loss: 3.070728\n",
            "[134,    25] loss: 3.054500\n",
            "[135,    25] loss: 3.079289\n",
            "[136,    25] loss: 3.056171\n",
            "[137,    25] loss: 3.130479\n",
            "[138,    25] loss: 3.166095\n",
            "[139,    25] loss: 3.155315\n",
            "[140,    25] loss: 3.117315\n",
            "[141,    25] loss: 3.132038\n",
            "[142,    25] loss: 3.043236\n",
            "[143,    25] loss: 3.040932\n",
            "[144,    25] loss: 3.038398\n",
            "[145,    25] loss: 3.048379\n",
            "[146,    25] loss: 2.967306\n",
            "[147,    25] loss: 2.974050\n",
            "[148,    25] loss: 3.051088\n",
            "[149,    25] loss: 3.084979\n",
            "[150,    25] loss: 3.099169\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.09139208942651748\n",
            "Test Accuracy -  28.125\n",
            "L2 values -  [0.0368280079215765, 0.06372473575174809, 0.09863003119826316, 0.12740116715431213, 0.13979460597038268, 0.14633402675390245]\n",
            "L2 values -  [0.0368280079215765, 0.04004273284226656, 0.05285604279488325, 0.0464575856924057, 0.044045723602175714, 0.09139208942651748]\n"
          ]
        }
      ],
      "source": [
        "# Full cycle\n",
        "\n",
        "parameters_to_prune = (\n",
        "    (net.fc1, 'weight'),\n",
        "    (net.fc2, 'weight'),\n",
        "    (net.fc3, 'weight'),\n",
        ")\n",
        "\n",
        "batch_loss = 25\n",
        "epoch = 150\n",
        "\n",
        "results_base_l1 = []\n",
        "results_finetune_l1 = []\n",
        "\n",
        "results_base_l1_acc = []\n",
        "results_finetune_l1_acc = []\n",
        "\n",
        "results_base_l2 = []\n",
        "results_finetune_l2 = []\n",
        "\n",
        "results_base_l2_acc = []\n",
        "results_finetune_l2_acc = []\n",
        "\n",
        "\n",
        "# Network\n",
        "net = Net().to(device)\n",
        "print_sparsity(net)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "train(net, train_loader, epoch, optimizer, criterion, batch_loss)\n",
        "loss_mse, acc = test(net, test_loader, criterion)\n",
        "\n",
        "results_base_l1.append(loss_mse)\n",
        "results_finetune_l1.append(loss_mse)\n",
        "results_base_l2.append(loss_mse)\n",
        "results_finetune_l2.append(loss_mse)\n",
        "\n",
        "results_base_l1_acc.append(acc)\n",
        "results_finetune_l1_acc.append(acc)\n",
        "results_base_l2_acc.append(acc)\n",
        "results_finetune_l2_acc.append(acc)\n",
        "\n",
        "PATH = './cifar_net.pth'\n",
        "torch.save(net.state_dict(), PATH)\n",
        "\n",
        "prune_values = [0.2, 0.4, 0.6, 0.8, 0.9]\n",
        "pruning_methods = [0,1]\n",
        "\n",
        "\n",
        "for prune_value in prune_values:\n",
        "  for pruning_method in pruning_methods:\n",
        "  # Each prune value experiment is independent\n",
        "    net = Net().to(device)\n",
        "    net.load_state_dict(torch.load(PATH))\n",
        "    \n",
        "    parameters_to_prune = (\n",
        "      (net.fc1, 'weight'),\n",
        "      (net.fc2, 'weight'),\n",
        "      (net.fc3, 'weight'),\n",
        "    )\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.1)\n",
        "    \n",
        "    if pruning_method == 0:\n",
        "      prune.global_unstructured(\n",
        "          parameters_to_prune,\n",
        "          pruning_method=prune.L1Unstructured,\n",
        "          amount=prune_value,\n",
        "      )\n",
        "    else:\n",
        "      prune.global_unstructured(\n",
        "          parameters_to_prune,\n",
        "          pruning_method=prune.RandomUnstructured,\n",
        "          amount=prune_value,\n",
        "      )\n",
        "\n",
        "    print_sparsity(net)\n",
        "    \n",
        "    base_loss, base_acc = test(net, test_loader, criterion)\n",
        "\n",
        "    train(net, train_loader, epoch, optimizer, criterion, batch_loss)\n",
        "    finetune_loss, fine_acc = test(net, test_loader, criterion)\n",
        "    \n",
        "    if pruning_method == 0:\n",
        "      results_base_l1.append(base_loss)\n",
        "      results_finetune_l1.append(finetune_loss)\n",
        "\n",
        "      results_base_l1_acc.append(base_acc)\n",
        "      results_finetune_l1_acc.append(fine_acc)\n",
        "\n",
        "      print('L1 values - ', results_base_l1)\n",
        "      print('L1 values - ', results_finetune_l1)\n",
        "    else:\n",
        "      results_base_l2.append(base_loss)\n",
        "      results_finetune_l2.append(finetune_loss)\n",
        "\n",
        "      results_base_l2_acc.append(base_acc)\n",
        "      results_finetune_l2_acc.append(fine_acc)\n",
        "      print('L2 values - ', results_base_l2)\n",
        "      print('L2 values - ', results_finetune_l2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFnPtoKpUea6",
        "outputId": "9e290c73-8b92-4b60-e837-03def9bf7ed1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0368280079215765,\n",
              " 0.03824604116380215,\n",
              " 0.046889663115143775,\n",
              " 0.07239627912640571,\n",
              " 0.11953216195106506,\n",
              " 0.14299183040857316]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "results_base_l1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI_HhRKGUhz7",
        "outputId": "43061742-a028-4eb7-f697-49964fb11b95"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0368280079215765,\n",
              " 0.03439737726002932,\n",
              " 0.029042783565819262,\n",
              " 0.03849522266536951,\n",
              " 0.0538621798157692,\n",
              " 0.09886588528752327]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "results_finetune_l1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3cUedcYlVkn",
        "outputId": "3822d68f-4e66-4f09-a3fa-a21bf5fbdaf4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0368280079215765,\n",
              " 0.06372473575174809,\n",
              " 0.09863003119826316,\n",
              " 0.12740116715431213,\n",
              " 0.13979460597038268,\n",
              " 0.14633402675390245]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "results_base_l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ANhmuuulWTZ",
        "outputId": "5bd80bbd-6911-4cf2-980e-edc536762b42"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0368280079215765,\n",
              " 0.04004273284226656,\n",
              " 0.05285604279488325,\n",
              " 0.0464575856924057,\n",
              " 0.044045723602175714,\n",
              " 0.09139208942651748]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "results_finetune_l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAE75a6IYIpt"
      },
      "outputs": [],
      "source": [
        "x_coord = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "mLvJ6Ue-Uk31",
        "outputId": "120f57b1-ac24-4b8f-b1e4-a2d0649f4eca"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('Housing Dataset with 3 layer NN and different pruning techniques')\n",
        "plt.plot(x_coord, results_base_l1, label = 'L1 without finetuning')\n",
        "plt.plot(x_coord, results_finetune_l1, label = 'L1 finetuned')\n",
        "plt.plot(x_coord, results_base_l2, label = 'Random without finetuning')\n",
        "plt.plot(x_coord, results_finetune_l2, label = 'Random finetuned')\n",
        "plt.ylabel('Cross Entropy loss')\n",
        "plt.xlabel('Sparsity')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68gqkcHomsJE"
      },
      "source": [
        "### 5 layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pj6NeLkdmsJF"
      },
      "outputs": [],
      "source": [
        "def print_sparsity(net):\n",
        "  print(\n",
        "      \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc1.weight == 0))\n",
        "          / float(net.fc1.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Sparsity in fc2.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc2.weight == 0))\n",
        "          / float(net.fc2.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Sparsity in fc3.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc3.weight == 0))\n",
        "          / float(net.fc3.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Sparsity in fc4.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc4.weight == 0))\n",
        "          / float(net.fc4.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Sparsity in fc5.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc5.weight == 0))\n",
        "          / float(net.fc5.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Global sparsity: {:.2f}%\".format(\n",
        "          100. * float(\n",
        "              + torch.sum(net.fc1.weight == 0)\n",
        "              + torch.sum(net.fc2.weight == 0)\n",
        "              + torch.sum(net.fc3.weight == 0)\n",
        "              + torch.sum(net.fc4.weight == 0)\n",
        "              + torch.sum(net.fc5.weight == 0)\n",
        "          )\n",
        "          / float(\n",
        "              + net.fc1.weight.nelement()\n",
        "              + net.fc2.weight.nelement()\n",
        "              + net.fc3.weight.nelement()\n",
        "              + net.fc4.weight.nelement()\n",
        "              + net.fc5.weight.nelement()\n",
        "          )\n",
        "      )\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W83uStXimsJF"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(64, 32)\n",
        "        self.fc2 = nn.Linear(32, 16)\n",
        "        self.fc3 = nn.Linear(16, 16)\n",
        "        self.fc4 = nn.Linear(16, 8)\n",
        "        self.fc5 = nn.Linear(8, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x)) \n",
        "        x = torch.tanh(self.fc3(x))\n",
        "        x = torch.tanh(self.fc4(x))\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "net = Net().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Tum6muImsJG"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0Cu78o5msJG",
        "outputId": "448f4377-67b9-40ad-ad74-0305d0a50ec8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg Loss during Testing -  0.14157524704933167\n",
            "Test Accuracy -  1.875\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.14157524704933167, array(1.875, dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# Testing before training\n",
        "test(net, test_loader, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
        "train(net, train_loader, 250, optimizer, criterion, 25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcQkE1p5xQB6",
        "outputId": "e1c4f22a-8c5c-4350-8ccb-24e5a405f50a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,    25] loss: 4.619119\n",
            "[2,    25] loss: 4.280630\n",
            "[3,    25] loss: 3.985687\n",
            "[4,    25] loss: 3.722831\n",
            "[5,    25] loss: 3.492791\n",
            "[6,    25] loss: 3.416917\n",
            "[7,    25] loss: 3.291355\n",
            "[8,    25] loss: 3.244715\n",
            "[9,    25] loss: 3.140762\n",
            "[10,    25] loss: 3.084871\n",
            "[11,    25] loss: 2.974913\n",
            "[12,    25] loss: 2.882841\n",
            "[13,    25] loss: 2.840765\n",
            "[14,    25] loss: 2.769365\n",
            "[15,    25] loss: 2.721797\n",
            "[16,    25] loss: 2.627972\n",
            "[17,    25] loss: 2.599522\n",
            "[18,    25] loss: 2.562167\n",
            "[19,    25] loss: 2.532221\n",
            "[20,    25] loss: 2.540459\n",
            "[21,    25] loss: 2.461726\n",
            "[22,    25] loss: 2.462004\n",
            "[23,    25] loss: 2.389047\n",
            "[24,    25] loss: 2.360477\n",
            "[25,    25] loss: 2.333812\n",
            "[26,    25] loss: 2.285867\n",
            "[27,    25] loss: 2.267117\n",
            "[28,    25] loss: 2.228044\n",
            "[29,    25] loss: 2.221278\n",
            "[30,    25] loss: 2.108635\n",
            "[31,    25] loss: 2.133812\n",
            "[32,    25] loss: 2.060961\n",
            "[33,    25] loss: 2.039736\n",
            "[34,    25] loss: 2.034802\n",
            "[35,    25] loss: 2.024740\n",
            "[36,    25] loss: 1.926041\n",
            "[37,    25] loss: 1.931806\n",
            "[38,    25] loss: 1.876178\n",
            "[39,    25] loss: 1.884618\n",
            "[40,    25] loss: 1.828824\n",
            "[41,    25] loss: 1.802511\n",
            "[42,    25] loss: 1.771222\n",
            "[43,    25] loss: 1.770372\n",
            "[44,    25] loss: 1.715459\n",
            "[45,    25] loss: 1.724246\n",
            "[46,    25] loss: 1.654293\n",
            "[47,    25] loss: 1.728929\n",
            "[48,    25] loss: 1.640380\n",
            "[49,    25] loss: 1.686191\n",
            "[50,    25] loss: 1.600651\n",
            "[51,    25] loss: 1.525272\n",
            "[52,    25] loss: 1.540961\n",
            "[53,    25] loss: 1.489077\n",
            "[54,    25] loss: 1.525511\n",
            "[55,    25] loss: 1.458340\n",
            "[56,    25] loss: 1.393446\n",
            "[57,    25] loss: 1.457326\n",
            "[58,    25] loss: 1.447829\n",
            "[59,    25] loss: 1.443573\n",
            "[60,    25] loss: 1.307289\n",
            "[61,    25] loss: 1.331999\n",
            "[62,    25] loss: 1.329833\n",
            "[63,    25] loss: 1.354356\n",
            "[64,    25] loss: 1.274966\n",
            "[65,    25] loss: 1.284269\n",
            "[66,    25] loss: 1.311384\n",
            "[67,    25] loss: 1.287940\n",
            "[68,    25] loss: 1.255650\n",
            "[69,    25] loss: 1.257908\n",
            "[70,    25] loss: 1.183495\n",
            "[71,    25] loss: 1.152594\n",
            "[72,    25] loss: 1.180955\n",
            "[73,    25] loss: 1.128499\n",
            "[74,    25] loss: 1.137537\n",
            "[75,    25] loss: 1.104551\n",
            "[76,    25] loss: 1.080512\n",
            "[77,    25] loss: 1.112918\n",
            "[78,    25] loss: 1.113123\n",
            "[79,    25] loss: 1.082048\n",
            "[80,    25] loss: 1.115821\n",
            "[81,    25] loss: 1.045543\n",
            "[82,    25] loss: 1.035564\n",
            "[83,    25] loss: 1.042287\n",
            "[84,    25] loss: 1.019773\n",
            "[85,    25] loss: 1.059426\n",
            "[86,    25] loss: 1.040393\n",
            "[87,    25] loss: 1.066836\n",
            "[88,    25] loss: 1.030567\n",
            "[89,    25] loss: 0.946870\n",
            "[90,    25] loss: 0.977276\n",
            "[91,    25] loss: 0.975382\n",
            "[92,    25] loss: 0.999534\n",
            "[93,    25] loss: 0.958007\n",
            "[94,    25] loss: 0.941801\n",
            "[95,    25] loss: 0.933813\n",
            "[96,    25] loss: 0.911732\n",
            "[97,    25] loss: 0.936686\n",
            "[98,    25] loss: 0.875193\n",
            "[99,    25] loss: 0.943090\n",
            "[100,    25] loss: 0.944066\n",
            "[101,    25] loss: 0.918527\n",
            "[102,    25] loss: 0.899928\n",
            "[103,    25] loss: 0.872649\n",
            "[104,    25] loss: 0.899830\n",
            "[105,    25] loss: 0.876070\n",
            "[106,    25] loss: 0.898335\n",
            "[107,    25] loss: 0.849854\n",
            "[108,    25] loss: 0.874904\n",
            "[109,    25] loss: 0.880230\n",
            "[110,    25] loss: 0.818218\n",
            "[111,    25] loss: 0.906787\n",
            "[112,    25] loss: 0.853541\n",
            "[113,    25] loss: 0.894255\n",
            "[114,    25] loss: 0.876673\n",
            "[115,    25] loss: 0.841962\n",
            "[116,    25] loss: 0.858661\n",
            "[117,    25] loss: 0.829143\n",
            "[118,    25] loss: 0.884368\n",
            "[119,    25] loss: 0.812790\n",
            "[120,    25] loss: 0.813740\n",
            "[121,    25] loss: 0.756243\n",
            "[122,    25] loss: 0.772564\n",
            "[123,    25] loss: 0.759692\n",
            "[124,    25] loss: 0.773129\n",
            "[125,    25] loss: 0.767169\n",
            "[126,    25] loss: 0.757588\n",
            "[127,    25] loss: 0.817387\n",
            "[128,    25] loss: 0.810732\n",
            "[129,    25] loss: 0.736718\n",
            "[130,    25] loss: 0.730474\n",
            "[131,    25] loss: 0.689622\n",
            "[132,    25] loss: 0.814080\n",
            "[133,    25] loss: 0.731541\n",
            "[134,    25] loss: 0.704377\n",
            "[135,    25] loss: 0.694430\n",
            "[136,    25] loss: 0.662481\n",
            "[137,    25] loss: 0.671299\n",
            "[138,    25] loss: 0.745914\n",
            "[139,    25] loss: 0.826344\n",
            "[140,    25] loss: 0.780118\n",
            "[141,    25] loss: 0.696706\n",
            "[142,    25] loss: 0.753162\n",
            "[143,    25] loss: 0.741197\n",
            "[144,    25] loss: 0.719025\n",
            "[145,    25] loss: 0.791799\n",
            "[146,    25] loss: 0.685114\n",
            "[147,    25] loss: 0.719828\n",
            "[148,    25] loss: 0.688286\n",
            "[149,    25] loss: 0.625105\n",
            "[150,    25] loss: 0.675731\n",
            "[151,    25] loss: 0.781406\n",
            "[152,    25] loss: 0.751831\n",
            "[153,    25] loss: 0.741986\n",
            "[154,    25] loss: 0.752299\n",
            "[155,    25] loss: 0.729337\n",
            "[156,    25] loss: 0.701343\n",
            "[157,    25] loss: 0.637894\n",
            "[158,    25] loss: 0.644008\n",
            "[159,    25] loss: 0.613822\n",
            "[160,    25] loss: 0.617105\n",
            "[161,    25] loss: 0.591927\n",
            "[162,    25] loss: 0.601573\n",
            "[163,    25] loss: 0.645863\n",
            "[164,    25] loss: 0.551837\n",
            "[165,    25] loss: 0.598195\n",
            "[166,    25] loss: 0.619100\n",
            "[167,    25] loss: 0.741195\n",
            "[168,    25] loss: 0.699177\n",
            "[169,    25] loss: 0.635983\n",
            "[170,    25] loss: 0.544460\n",
            "[171,    25] loss: 0.625200\n",
            "[172,    25] loss: 0.608176\n",
            "[173,    25] loss: 0.587230\n",
            "[174,    25] loss: 0.556718\n",
            "[175,    25] loss: 0.584036\n",
            "[176,    25] loss: 0.547354\n",
            "[177,    25] loss: 0.578554\n",
            "[178,    25] loss: 0.584214\n",
            "[179,    25] loss: 0.562291\n",
            "[180,    25] loss: 0.529544\n",
            "[181,    25] loss: 0.566255\n",
            "[182,    25] loss: 0.581355\n",
            "[183,    25] loss: 0.602711\n",
            "[184,    25] loss: 0.617366\n",
            "[185,    25] loss: 0.583373\n",
            "[186,    25] loss: 0.555662\n",
            "[187,    25] loss: 0.569602\n",
            "[188,    25] loss: 0.516307\n",
            "[189,    25] loss: 0.588517\n",
            "[190,    25] loss: 0.553995\n",
            "[191,    25] loss: 0.569011\n",
            "[192,    25] loss: 0.670135\n",
            "[193,    25] loss: 0.609998\n",
            "[194,    25] loss: 0.608560\n",
            "[195,    25] loss: 0.520618\n",
            "[196,    25] loss: 0.555547\n",
            "[197,    25] loss: 0.570138\n",
            "[198,    25] loss: 0.820832\n",
            "[199,    25] loss: 0.699529\n",
            "[200,    25] loss: 0.676615\n",
            "[201,    25] loss: 0.558287\n",
            "[202,    25] loss: 0.563797\n",
            "[203,    25] loss: 0.494859\n",
            "[204,    25] loss: 0.516235\n",
            "[205,    25] loss: 0.487359\n",
            "[206,    25] loss: 0.517561\n",
            "[207,    25] loss: 0.525584\n",
            "[208,    25] loss: 0.544052\n",
            "[209,    25] loss: 0.505276\n",
            "[210,    25] loss: 0.467700\n",
            "[211,    25] loss: 0.450937\n",
            "[212,    25] loss: 0.566675\n",
            "[213,    25] loss: 0.512301\n",
            "[214,    25] loss: 0.468416\n",
            "[215,    25] loss: 0.445419\n",
            "[216,    25] loss: 0.492891\n",
            "[217,    25] loss: 0.487011\n",
            "[218,    25] loss: 0.516421\n",
            "[219,    25] loss: 0.549918\n",
            "[220,    25] loss: 0.450888\n",
            "[221,    25] loss: 0.478858\n",
            "[222,    25] loss: 0.461699\n",
            "[223,    25] loss: 0.541036\n",
            "[224,    25] loss: 0.462807\n",
            "[225,    25] loss: 0.418862\n",
            "[226,    25] loss: 0.413421\n",
            "[227,    25] loss: 0.431967\n",
            "[228,    25] loss: 0.504704\n",
            "[229,    25] loss: 0.503674\n",
            "[230,    25] loss: 0.546529\n",
            "[231,    25] loss: 0.521941\n",
            "[232,    25] loss: 0.527295\n",
            "[233,    25] loss: 0.578480\n",
            "[234,    25] loss: 0.449911\n",
            "[235,    25] loss: 0.413509\n",
            "[236,    25] loss: 0.465999\n",
            "[237,    25] loss: 0.514647\n",
            "[238,    25] loss: 0.493749\n",
            "[239,    25] loss: 0.471862\n",
            "[240,    25] loss: 0.532411\n",
            "[241,    25] loss: 0.438358\n",
            "[242,    25] loss: 0.475411\n",
            "[243,    25] loss: 0.463149\n",
            "[244,    25] loss: 0.440604\n",
            "[245,    25] loss: 0.396665\n",
            "[246,    25] loss: 0.538930\n",
            "[247,    25] loss: 0.600551\n",
            "[248,    25] loss: 0.535242\n",
            "[249,    25] loss: 0.566065\n",
            "[250,    25] loss: 0.538015\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing before training\n",
        "test(net, test_loader, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ez9KoLL7xoSJ",
        "outputId": "bb20c236-409d-41ec-ae22-86f8ee4a54b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg Loss during Testing -  0.030147206410765647\n",
            "Test Accuracy -  78.125\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.030147206410765647, array(78.125, dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7BA2K2XmsJH",
        "outputId": "594aa98c-d31d-4480-f026-25746d822a0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sparsity in fc1.weight: 0.00%\n",
            "Sparsity in fc2.weight: 0.00%\n",
            "Sparsity in fc3.weight: 0.00%\n",
            "Sparsity in fc4.weight: 0.00%\n",
            "Sparsity in fc5.weight: 0.00%\n",
            "Global sparsity: 0.00%\n",
            "[1,    25] loss: 4.632544\n",
            "[2,    25] loss: 4.258997\n",
            "[3,    25] loss: 3.775475\n",
            "[4,    25] loss: 3.408725\n",
            "[5,    25] loss: 3.269175\n",
            "[6,    25] loss: 3.105145\n",
            "[7,    25] loss: 3.048202\n",
            "[8,    25] loss: 2.969313\n",
            "[9,    25] loss: 2.920715\n",
            "[10,    25] loss: 2.835890\n",
            "[11,    25] loss: 2.800755\n",
            "[12,    25] loss: 2.774574\n",
            "[13,    25] loss: 2.772318\n",
            "[14,    25] loss: 2.706975\n",
            "[15,    25] loss: 2.681251\n",
            "[16,    25] loss: 2.637559\n",
            "[17,    25] loss: 2.666759\n",
            "[18,    25] loss: 2.624788\n",
            "[19,    25] loss: 2.585574\n",
            "[20,    25] loss: 2.578463\n",
            "[21,    25] loss: 2.565245\n",
            "[22,    25] loss: 2.488984\n",
            "[23,    25] loss: 2.550325\n",
            "[24,    25] loss: 2.466408\n",
            "[25,    25] loss: 2.488472\n",
            "[26,    25] loss: 2.463687\n",
            "[27,    25] loss: 2.397088\n",
            "[28,    25] loss: 2.394201\n",
            "[29,    25] loss: 2.369960\n",
            "[30,    25] loss: 2.340850\n",
            "[31,    25] loss: 2.270438\n",
            "[32,    25] loss: 2.272766\n",
            "[33,    25] loss: 2.308440\n",
            "[34,    25] loss: 2.236883\n",
            "[35,    25] loss: 2.229266\n",
            "[36,    25] loss: 2.229957\n",
            "[37,    25] loss: 2.226495\n",
            "[38,    25] loss: 2.161316\n",
            "[39,    25] loss: 2.160893\n",
            "[40,    25] loss: 2.154194\n",
            "[41,    25] loss: 2.119449\n",
            "[42,    25] loss: 2.118104\n",
            "[43,    25] loss: 2.092885\n",
            "[44,    25] loss: 2.128185\n",
            "[45,    25] loss: 2.070833\n",
            "[46,    25] loss: 2.020111\n",
            "[47,    25] loss: 1.986764\n",
            "[48,    25] loss: 2.035499\n",
            "[49,    25] loss: 2.028610\n",
            "[50,    25] loss: 1.992542\n",
            "[51,    25] loss: 2.014721\n",
            "[52,    25] loss: 1.936876\n",
            "[53,    25] loss: 1.906036\n",
            "[54,    25] loss: 1.930524\n",
            "[55,    25] loss: 1.894392\n",
            "[56,    25] loss: 1.920074\n",
            "[57,    25] loss: 1.878040\n",
            "[58,    25] loss: 1.921344\n",
            "[59,    25] loss: 1.820205\n",
            "[60,    25] loss: 1.846444\n",
            "[61,    25] loss: 1.813832\n",
            "[62,    25] loss: 1.827776\n",
            "[63,    25] loss: 1.771270\n",
            "[64,    25] loss: 1.789105\n",
            "[65,    25] loss: 1.728585\n",
            "[66,    25] loss: 1.706241\n",
            "[67,    25] loss: 1.697619\n",
            "[68,    25] loss: 1.659979\n",
            "[69,    25] loss: 1.645831\n",
            "[70,    25] loss: 1.644457\n",
            "[71,    25] loss: 1.621714\n",
            "[72,    25] loss: 1.652513\n",
            "[73,    25] loss: 1.610153\n",
            "[74,    25] loss: 1.566185\n",
            "[75,    25] loss: 1.528735\n",
            "[76,    25] loss: 1.549903\n",
            "[77,    25] loss: 1.559931\n",
            "[78,    25] loss: 1.534819\n",
            "[79,    25] loss: 1.530498\n",
            "[80,    25] loss: 1.562763\n",
            "[81,    25] loss: 1.469768\n",
            "[82,    25] loss: 1.523602\n",
            "[83,    25] loss: 1.437709\n",
            "[84,    25] loss: 1.411027\n",
            "[85,    25] loss: 1.462137\n",
            "[86,    25] loss: 1.473183\n",
            "[87,    25] loss: 1.473777\n",
            "[88,    25] loss: 1.422818\n",
            "[89,    25] loss: 1.410199\n",
            "[90,    25] loss: 1.398014\n",
            "[91,    25] loss: 1.470793\n",
            "[92,    25] loss: 1.434452\n",
            "[93,    25] loss: 1.481645\n",
            "[94,    25] loss: 1.373053\n",
            "[95,    25] loss: 1.363926\n",
            "[96,    25] loss: 1.323388\n",
            "[97,    25] loss: 1.350461\n",
            "[98,    25] loss: 1.309071\n",
            "[99,    25] loss: 1.334724\n",
            "[100,    25] loss: 1.318779\n",
            "[101,    25] loss: 1.272952\n",
            "[102,    25] loss: 1.286255\n",
            "[103,    25] loss: 1.304797\n",
            "[104,    25] loss: 1.256810\n",
            "[105,    25] loss: 1.251721\n",
            "[106,    25] loss: 1.255371\n",
            "[107,    25] loss: 1.174415\n",
            "[108,    25] loss: 1.222897\n",
            "[109,    25] loss: 1.211509\n",
            "[110,    25] loss: 1.204884\n",
            "[111,    25] loss: 1.175250\n",
            "[112,    25] loss: 1.201657\n",
            "[113,    25] loss: 1.213677\n",
            "[114,    25] loss: 1.115319\n",
            "[115,    25] loss: 1.211611\n",
            "[116,    25] loss: 1.165732\n",
            "[117,    25] loss: 1.175285\n",
            "[118,    25] loss: 1.139375\n",
            "[119,    25] loss: 1.221137\n",
            "[120,    25] loss: 1.150368\n",
            "[121,    25] loss: 1.075431\n",
            "[122,    25] loss: 1.113838\n",
            "[123,    25] loss: 1.083093\n",
            "[124,    25] loss: 1.088716\n",
            "[125,    25] loss: 1.087799\n",
            "[126,    25] loss: 1.147865\n",
            "[127,    25] loss: 1.124687\n",
            "[128,    25] loss: 1.105051\n",
            "[129,    25] loss: 1.028119\n",
            "[130,    25] loss: 1.058812\n",
            "[131,    25] loss: 1.056452\n",
            "[132,    25] loss: 1.082345\n",
            "[133,    25] loss: 1.107801\n",
            "[134,    25] loss: 1.148743\n",
            "[135,    25] loss: 0.993873\n",
            "[136,    25] loss: 1.005768\n",
            "[137,    25] loss: 1.000539\n",
            "[138,    25] loss: 0.996805\n",
            "[139,    25] loss: 0.980304\n",
            "[140,    25] loss: 0.983948\n",
            "[141,    25] loss: 0.966521\n",
            "[142,    25] loss: 1.002760\n",
            "[143,    25] loss: 1.003520\n",
            "[144,    25] loss: 1.017824\n",
            "[145,    25] loss: 0.967381\n",
            "[146,    25] loss: 0.967023\n",
            "[147,    25] loss: 0.987489\n",
            "[148,    25] loss: 0.902536\n",
            "[149,    25] loss: 0.858049\n",
            "[150,    25] loss: 0.906811\n",
            "[151,    25] loss: 0.890607\n",
            "[152,    25] loss: 1.003559\n",
            "[153,    25] loss: 1.040675\n",
            "[154,    25] loss: 0.942686\n",
            "[155,    25] loss: 0.948886\n",
            "[156,    25] loss: 0.932302\n",
            "[157,    25] loss: 0.903865\n",
            "[158,    25] loss: 0.958492\n",
            "[159,    25] loss: 1.058629\n",
            "[160,    25] loss: 0.892049\n",
            "[161,    25] loss: 0.889742\n",
            "[162,    25] loss: 0.878000\n",
            "[163,    25] loss: 0.833778\n",
            "[164,    25] loss: 0.818790\n",
            "[165,    25] loss: 0.899671\n",
            "[166,    25] loss: 0.963254\n",
            "[167,    25] loss: 0.930168\n",
            "[168,    25] loss: 0.842683\n",
            "[169,    25] loss: 0.787444\n",
            "[170,    25] loss: 0.771954\n",
            "[171,    25] loss: 0.891364\n",
            "[172,    25] loss: 0.788044\n",
            "[173,    25] loss: 0.782459\n",
            "[174,    25] loss: 0.856001\n",
            "[175,    25] loss: 0.828848\n",
            "[176,    25] loss: 0.716948\n",
            "[177,    25] loss: 0.744253\n",
            "[178,    25] loss: 0.743772\n",
            "[179,    25] loss: 0.792164\n",
            "[180,    25] loss: 0.718221\n",
            "[181,    25] loss: 0.811604\n",
            "[182,    25] loss: 0.877960\n",
            "[183,    25] loss: 0.757780\n",
            "[184,    25] loss: 0.812414\n",
            "[185,    25] loss: 0.783606\n",
            "[186,    25] loss: 0.862324\n",
            "[187,    25] loss: 0.778237\n",
            "[188,    25] loss: 0.838876\n",
            "[189,    25] loss: 0.742813\n",
            "[190,    25] loss: 0.730088\n",
            "[191,    25] loss: 0.775323\n",
            "[192,    25] loss: 0.774344\n",
            "[193,    25] loss: 0.772748\n",
            "[194,    25] loss: 0.733055\n",
            "[195,    25] loss: 0.682350\n",
            "[196,    25] loss: 0.678299\n",
            "[197,    25] loss: 0.742522\n",
            "[198,    25] loss: 0.753869\n",
            "[199,    25] loss: 0.803890\n",
            "[200,    25] loss: 0.837260\n",
            "Finished Training\n",
            "[1,    25] loss: 0.855703\n",
            "[2,    25] loss: 0.712774\n",
            "[3,    25] loss: 0.712218\n",
            "[4,    25] loss: 0.724295\n",
            "[5,    25] loss: 0.688284\n",
            "[6,    25] loss: 0.705468\n",
            "[7,    25] loss: 0.723372\n",
            "[8,    25] loss: 0.698856\n",
            "[9,    25] loss: 0.654529\n",
            "[10,    25] loss: 0.625604\n",
            "[11,    25] loss: 0.624898\n",
            "[12,    25] loss: 0.665813\n",
            "[13,    25] loss: 0.652833\n",
            "[14,    25] loss: 0.637917\n",
            "[15,    25] loss: 0.600779\n",
            "[16,    25] loss: 0.627579\n",
            "[17,    25] loss: 0.673842\n",
            "[18,    25] loss: 0.763250\n",
            "[19,    25] loss: 0.819890\n",
            "[20,    25] loss: 0.946200\n",
            "[21,    25] loss: 0.721987\n",
            "[22,    25] loss: 0.627148\n",
            "[23,    25] loss: 0.633382\n",
            "[24,    25] loss: 0.576761\n",
            "[25,    25] loss: 0.603003\n",
            "[26,    25] loss: 0.647347\n",
            "[27,    25] loss: 0.629406\n",
            "[28,    25] loss: 0.656187\n",
            "[29,    25] loss: 0.766228\n",
            "[30,    25] loss: 0.876000\n",
            "[31,    25] loss: 0.702852\n",
            "[32,    25] loss: 0.800062\n",
            "[33,    25] loss: 0.764731\n",
            "[34,    25] loss: 0.666956\n",
            "[35,    25] loss: 0.635950\n",
            "[36,    25] loss: 0.604630\n",
            "[37,    25] loss: 0.613636\n",
            "[38,    25] loss: 0.619826\n",
            "[39,    25] loss: 0.656918\n",
            "[40,    25] loss: 0.611393\n",
            "[41,    25] loss: 0.615490\n",
            "[42,    25] loss: 0.590476\n",
            "[43,    25] loss: 0.545002\n",
            "[44,    25] loss: 0.566992\n",
            "[45,    25] loss: 0.580242\n",
            "[46,    25] loss: 0.542887\n",
            "[47,    25] loss: 0.536649\n",
            "[48,    25] loss: 0.528336\n",
            "[49,    25] loss: 0.580046\n",
            "[50,    25] loss: 0.585395\n",
            "[51,    25] loss: 0.576208\n",
            "[52,    25] loss: 0.515707\n",
            "[53,    25] loss: 0.554259\n",
            "[54,    25] loss: 0.666079\n",
            "[55,    25] loss: 0.781187\n",
            "[56,    25] loss: 0.633850\n",
            "[57,    25] loss: 0.611457\n",
            "[58,    25] loss: 0.618116\n",
            "[59,    25] loss: 0.658974\n",
            "[60,    25] loss: 0.556313\n",
            "[61,    25] loss: 0.596412\n",
            "[62,    25] loss: 0.660945\n",
            "[63,    25] loss: 0.682479\n",
            "[64,    25] loss: 0.531285\n",
            "[65,    25] loss: 0.496244\n",
            "[66,    25] loss: 0.475930\n",
            "[67,    25] loss: 0.476761\n",
            "[68,    25] loss: 0.443123\n",
            "[69,    25] loss: 0.461528\n",
            "[70,    25] loss: 0.475455\n",
            "[71,    25] loss: 0.478623\n",
            "[72,    25] loss: 0.515619\n",
            "[73,    25] loss: 0.550069\n",
            "[74,    25] loss: 0.642038\n",
            "[75,    25] loss: 0.642291\n",
            "[76,    25] loss: 0.591523\n",
            "[77,    25] loss: 0.555003\n",
            "[78,    25] loss: 0.462851\n",
            "[79,    25] loss: 0.468413\n",
            "[80,    25] loss: 0.430115\n",
            "[81,    25] loss: 0.442488\n",
            "[82,    25] loss: 0.434730\n",
            "[83,    25] loss: 0.490059\n",
            "[84,    25] loss: 0.624793\n",
            "[85,    25] loss: 0.971055\n",
            "[86,    25] loss: 0.715485\n",
            "[87,    25] loss: 0.557741\n",
            "[88,    25] loss: 0.532230\n",
            "[89,    25] loss: 0.528191\n",
            "[90,    25] loss: 0.478445\n",
            "[91,    25] loss: 0.443872\n",
            "[92,    25] loss: 0.420325\n",
            "[93,    25] loss: 0.408895\n",
            "[94,    25] loss: 0.420372\n",
            "[95,    25] loss: 0.520205\n",
            "[96,    25] loss: 0.464192\n",
            "[97,    25] loss: 0.593820\n",
            "[98,    25] loss: 0.702944\n",
            "[99,    25] loss: 0.561978\n",
            "[100,    25] loss: 0.557403\n",
            "[101,    25] loss: 0.551503\n",
            "[102,    25] loss: 0.528239\n",
            "[103,    25] loss: 0.461783\n",
            "[104,    25] loss: 0.434515\n",
            "[105,    25] loss: 0.418736\n",
            "[106,    25] loss: 0.424286\n",
            "[107,    25] loss: 0.391251\n",
            "[108,    25] loss: 0.414705\n",
            "[109,    25] loss: 0.389514\n",
            "[110,    25] loss: 0.382451\n",
            "[111,    25] loss: 0.383935\n",
            "[112,    25] loss: 0.403485\n",
            "[113,    25] loss: 0.377999\n",
            "[114,    25] loss: 0.385602\n",
            "[115,    25] loss: 0.369874\n",
            "[116,    25] loss: 0.437395\n",
            "[117,    25] loss: 0.502439\n",
            "[118,    25] loss: 0.486914\n",
            "[119,    25] loss: 0.697512\n",
            "[120,    25] loss: 0.597891\n",
            "[121,    25] loss: 0.651330\n",
            "[122,    25] loss: 0.589912\n",
            "[123,    25] loss: 0.513485\n",
            "[124,    25] loss: 0.481211\n",
            "[125,    25] loss: 0.666805\n",
            "[126,    25] loss: 0.506453\n",
            "[127,    25] loss: 0.487059\n",
            "[128,    25] loss: 0.482518\n",
            "[129,    25] loss: 0.504513\n",
            "[130,    25] loss: 0.679862\n",
            "[131,    25] loss: 0.594693\n",
            "[132,    25] loss: 0.458686\n",
            "[133,    25] loss: 0.390294\n",
            "[134,    25] loss: 0.400653\n",
            "[135,    25] loss: 0.406846\n",
            "[136,    25] loss: 0.509662\n",
            "[137,    25] loss: 0.441605\n",
            "[138,    25] loss: 0.412230\n",
            "[139,    25] loss: 0.362530\n",
            "[140,    25] loss: 0.367897\n",
            "[141,    25] loss: 0.388873\n",
            "[142,    25] loss: 0.413242\n",
            "[143,    25] loss: 0.533498\n",
            "[144,    25] loss: 0.515735\n",
            "[145,    25] loss: 0.454020\n",
            "[146,    25] loss: 0.413333\n",
            "[147,    25] loss: 0.352172\n",
            "[148,    25] loss: 0.419963\n",
            "[149,    25] loss: 0.520461\n",
            "[150,    25] loss: 0.528808\n",
            "[151,    25] loss: 0.423955\n",
            "[152,    25] loss: 0.393458\n",
            "[153,    25] loss: 0.369664\n",
            "[154,    25] loss: 0.343054\n",
            "[155,    25] loss: 0.340412\n",
            "[156,    25] loss: 0.346861\n",
            "[157,    25] loss: 0.490357\n",
            "[158,    25] loss: 0.454552\n",
            "[159,    25] loss: 0.422686\n",
            "[160,    25] loss: 0.382107\n",
            "[161,    25] loss: 0.364557\n",
            "[162,    25] loss: 0.336211\n",
            "[163,    25] loss: 0.319080\n",
            "[164,    25] loss: 0.351427\n",
            "[165,    25] loss: 0.345600\n",
            "[166,    25] loss: 0.615442\n",
            "[167,    25] loss: 0.827915\n",
            "[168,    25] loss: 0.753273\n",
            "[169,    25] loss: 0.453378\n",
            "[170,    25] loss: 0.417052\n",
            "[171,    25] loss: 0.349415\n",
            "[172,    25] loss: 0.358831\n",
            "[173,    25] loss: 0.378861\n",
            "[174,    25] loss: 0.351738\n",
            "[175,    25] loss: 0.394866\n",
            "[176,    25] loss: 0.418170\n",
            "[177,    25] loss: 0.444970\n",
            "[178,    25] loss: 0.374931\n",
            "[179,    25] loss: 0.319533\n",
            "[180,    25] loss: 0.307474\n",
            "[181,    25] loss: 0.315808\n",
            "[182,    25] loss: 0.385652\n",
            "[183,    25] loss: 0.394550\n",
            "[184,    25] loss: 0.357726\n",
            "[185,    25] loss: 0.427528\n",
            "[186,    25] loss: 0.469863\n",
            "[187,    25] loss: 0.390695\n",
            "[188,    25] loss: 0.369848\n",
            "[189,    25] loss: 0.369800\n",
            "[190,    25] loss: 0.335893\n",
            "[191,    25] loss: 0.320621\n",
            "[192,    25] loss: 0.280412\n",
            "[193,    25] loss: 0.284024\n",
            "[194,    25] loss: 0.272617\n",
            "[195,    25] loss: 0.262754\n",
            "[196,    25] loss: 0.284277\n",
            "[197,    25] loss: 0.320905\n",
            "[198,    25] loss: 0.349849\n",
            "[199,    25] loss: 0.390363\n",
            "[200,    25] loss: 0.631285\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.039064118452370164\n",
            "Test Accuracy -  72.5\n",
            "Sparsity in fc1.weight: 21.63%\n",
            "Sparsity in fc2.weight: 30.47%\n",
            "Sparsity in fc3.weight: 26.56%\n",
            "Sparsity in fc4.weight: 28.12%\n",
            "Sparsity in fc5.weight: 5.75%\n",
            "Global sparsity: 20.01%\n",
            "Avg Loss during Testing -  0.06395818032324314\n",
            "Test Accuracy -  58.75\n",
            "[1,    25] loss: 6.540466\n",
            "[2,    25] loss: 5.436354\n",
            "[3,    25] loss: 4.969994\n",
            "[4,    25] loss: 4.302237\n",
            "[5,    25] loss: 3.786924\n",
            "[6,    25] loss: 3.480606\n",
            "[7,    25] loss: 3.261239\n",
            "[8,    25] loss: 3.095112\n",
            "[9,    25] loss: 3.363581\n",
            "[10,    25] loss: 3.247057\n",
            "[11,    25] loss: 3.242419\n",
            "[12,    25] loss: 3.020200\n",
            "[13,    25] loss: 2.741356\n",
            "[14,    25] loss: 3.015407\n",
            "[15,    25] loss: 3.362067\n",
            "[16,    25] loss: 2.918644\n",
            "[17,    25] loss: 2.793092\n",
            "[18,    25] loss: 2.972362\n",
            "[19,    25] loss: 3.829231\n",
            "[20,    25] loss: 3.207570\n",
            "[21,    25] loss: 3.245684\n",
            "[22,    25] loss: 2.947157\n",
            "[23,    25] loss: 3.009401\n",
            "[24,    25] loss: 3.139057\n",
            "[25,    25] loss: 3.768376\n",
            "[26,    25] loss: 3.122150\n",
            "[27,    25] loss: 2.869666\n",
            "[28,    25] loss: 2.854607\n",
            "[29,    25] loss: 2.805028\n",
            "[30,    25] loss: 2.818079\n",
            "[31,    25] loss: 3.102603\n",
            "[32,    25] loss: 3.030939\n",
            "[33,    25] loss: 2.859605\n",
            "[34,    25] loss: 3.009518\n",
            "[35,    25] loss: 3.093626\n",
            "[36,    25] loss: 2.957746\n",
            "[37,    25] loss: 2.937615\n",
            "[38,    25] loss: 3.208638\n",
            "[39,    25] loss: 3.007759\n",
            "[40,    25] loss: 2.982788\n",
            "[41,    25] loss: 3.348874\n",
            "[42,    25] loss: 3.077906\n",
            "[43,    25] loss: 3.092052\n",
            "[44,    25] loss: 3.043968\n",
            "[45,    25] loss: 3.082571\n",
            "[46,    25] loss: 3.120056\n",
            "[47,    25] loss: 3.035004\n",
            "[48,    25] loss: 3.190034\n",
            "[49,    25] loss: 3.290392\n",
            "[50,    25] loss: 3.075120\n",
            "[51,    25] loss: 3.093578\n",
            "[52,    25] loss: 2.866581\n",
            "[53,    25] loss: 3.067156\n",
            "[54,    25] loss: 3.244784\n",
            "[55,    25] loss: 3.005378\n",
            "[56,    25] loss: 3.078056\n",
            "[57,    25] loss: 3.201639\n",
            "[58,    25] loss: 3.017537\n",
            "[59,    25] loss: 2.922754\n",
            "[60,    25] loss: 2.883317\n",
            "[61,    25] loss: 2.966500\n",
            "[62,    25] loss: 3.171063\n",
            "[63,    25] loss: 3.232501\n",
            "[64,    25] loss: 3.755188\n",
            "[65,    25] loss: 3.457631\n",
            "[66,    25] loss: 3.573289\n",
            "[67,    25] loss: 3.232758\n",
            "[68,    25] loss: 3.193588\n",
            "[69,    25] loss: 3.282673\n",
            "[70,    25] loss: 3.218308\n",
            "[71,    25] loss: 3.171549\n",
            "[72,    25] loss: 2.913133\n",
            "[73,    25] loss: 3.078599\n",
            "[74,    25] loss: 3.240676\n",
            "[75,    25] loss: 3.040933\n",
            "[76,    25] loss: 3.218668\n",
            "[77,    25] loss: 3.131394\n",
            "[78,    25] loss: 3.325925\n",
            "[79,    25] loss: 3.413610\n",
            "[80,    25] loss: 3.459303\n",
            "[81,    25] loss: 3.267729\n",
            "[82,    25] loss: 3.227707\n",
            "[83,    25] loss: 3.288144\n",
            "[84,    25] loss: 3.526072\n",
            "[85,    25] loss: 3.232963\n",
            "[86,    25] loss: 3.348306\n",
            "[87,    25] loss: 3.134660\n",
            "[88,    25] loss: 3.323010\n",
            "[89,    25] loss: 3.233774\n",
            "[90,    25] loss: 3.399324\n",
            "[91,    25] loss: 3.206555\n",
            "[92,    25] loss: 3.122338\n",
            "[93,    25] loss: 3.179711\n",
            "[94,    25] loss: 3.209473\n",
            "[95,    25] loss: 3.292058\n",
            "[96,    25] loss: 3.340082\n",
            "[97,    25] loss: 3.369692\n",
            "[98,    25] loss: 3.427158\n",
            "[99,    25] loss: 3.823158\n",
            "[100,    25] loss: 3.253434\n",
            "[101,    25] loss: 3.264550\n",
            "[102,    25] loss: 3.492514\n",
            "[103,    25] loss: 3.580839\n",
            "[104,    25] loss: 3.528618\n",
            "[105,    25] loss: 3.886691\n",
            "[106,    25] loss: 3.798491\n",
            "[107,    25] loss: 4.239042\n",
            "[108,    25] loss: 3.825351\n",
            "[109,    25] loss: 3.566620\n",
            "[110,    25] loss: 3.624373\n",
            "[111,    25] loss: 3.741943\n",
            "[112,    25] loss: 3.864279\n",
            "[113,    25] loss: 3.901933\n",
            "[114,    25] loss: 3.742925\n",
            "[115,    25] loss: 3.617520\n",
            "[116,    25] loss: 3.540928\n",
            "[117,    25] loss: 3.375355\n",
            "[118,    25] loss: 3.478252\n",
            "[119,    25] loss: 3.421909\n",
            "[120,    25] loss: 3.685674\n",
            "[121,    25] loss: 3.687308\n",
            "[122,    25] loss: 3.665634\n",
            "[123,    25] loss: 3.575362\n",
            "[124,    25] loss: 3.345522\n",
            "[125,    25] loss: 3.466681\n",
            "[126,    25] loss: 3.651172\n",
            "[127,    25] loss: 3.424062\n",
            "[128,    25] loss: 3.345436\n",
            "[129,    25] loss: 3.684380\n",
            "[130,    25] loss: 3.553203\n",
            "[131,    25] loss: 3.577108\n",
            "[132,    25] loss: 3.473307\n",
            "[133,    25] loss: 3.485029\n",
            "[134,    25] loss: 3.469663\n",
            "[135,    25] loss: 3.561695\n",
            "[136,    25] loss: 3.571905\n",
            "[137,    25] loss: 3.725858\n",
            "[138,    25] loss: 3.527432\n",
            "[139,    25] loss: 3.973376\n",
            "[140,    25] loss: 3.657386\n",
            "[141,    25] loss: 3.659363\n",
            "[142,    25] loss: 3.551441\n",
            "[143,    25] loss: 3.824189\n",
            "[144,    25] loss: 4.730012\n",
            "[145,    25] loss: 3.659549\n",
            "[146,    25] loss: 3.637397\n",
            "[147,    25] loss: 3.570515\n",
            "[148,    25] loss: 3.722774\n",
            "[149,    25] loss: 3.775218\n",
            "[150,    25] loss: 3.636832\n",
            "[151,    25] loss: 3.785327\n",
            "[152,    25] loss: 4.145601\n",
            "[153,    25] loss: 4.042083\n",
            "[154,    25] loss: 4.077668\n",
            "[155,    25] loss: 3.866225\n",
            "[156,    25] loss: 3.998926\n",
            "[157,    25] loss: 4.031764\n",
            "[158,    25] loss: 3.957507\n",
            "[159,    25] loss: 4.002590\n",
            "[160,    25] loss: 4.028308\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.12012229412794113\n",
            "Test Accuracy -  12.1875\n",
            "L1 values -  [0.039064118452370164, 0.06395818032324314]\n",
            "L1 values -  [0.039064118452370164, 0.12012229412794113]\n",
            "Sparsity in fc1.weight: 21.39%\n",
            "Sparsity in fc2.weight: 15.43%\n",
            "Sparsity in fc3.weight: 17.19%\n",
            "Sparsity in fc4.weight: 20.31%\n",
            "Sparsity in fc5.weight: 20.25%\n",
            "Global sparsity: 20.01%\n",
            "Avg Loss during Testing -  0.32684038281440736\n",
            "Test Accuracy -  5.625\n",
            "[1,    25] loss: 8.843783\n",
            "[2,    25] loss: 5.132613\n",
            "[3,    25] loss: 4.451422\n",
            "[4,    25] loss: 3.941952\n",
            "[5,    25] loss: 4.137353\n",
            "[6,    25] loss: 4.237016\n",
            "[7,    25] loss: 3.972259\n",
            "[8,    25] loss: 4.237489\n",
            "[9,    25] loss: 4.166774\n",
            "[10,    25] loss: 4.131470\n",
            "[11,    25] loss: 3.988705\n",
            "[12,    25] loss: 3.739095\n",
            "[13,    25] loss: 3.864455\n",
            "[14,    25] loss: 3.709703\n",
            "[15,    25] loss: 3.914238\n",
            "[16,    25] loss: 3.886284\n",
            "[17,    25] loss: 4.314689\n",
            "[18,    25] loss: 3.955258\n",
            "[19,    25] loss: 3.652113\n",
            "[20,    25] loss: 3.744421\n",
            "[21,    25] loss: 4.095287\n",
            "[22,    25] loss: 3.486253\n",
            "[23,    25] loss: 3.718517\n",
            "[24,    25] loss: 3.535287\n",
            "[25,    25] loss: 3.503842\n",
            "[26,    25] loss: 3.642682\n",
            "[27,    25] loss: 3.942742\n",
            "[28,    25] loss: 3.814899\n",
            "[29,    25] loss: 3.762875\n",
            "[30,    25] loss: 3.705351\n",
            "[31,    25] loss: 3.726601\n",
            "[32,    25] loss: 3.322521\n",
            "[33,    25] loss: 3.407306\n",
            "[34,    25] loss: 3.247402\n",
            "[35,    25] loss: 3.368507\n",
            "[36,    25] loss: 3.532176\n",
            "[37,    25] loss: 4.427217\n",
            "[38,    25] loss: 3.789768\n",
            "[39,    25] loss: 3.538545\n",
            "[40,    25] loss: 3.489981\n",
            "[41,    25] loss: 3.573353\n",
            "[42,    25] loss: 3.689265\n",
            "[43,    25] loss: 3.544200\n",
            "[44,    25] loss: 3.446758\n",
            "[45,    25] loss: 3.589172\n",
            "[46,    25] loss: 3.626611\n",
            "[47,    25] loss: 3.434067\n",
            "[48,    25] loss: 3.251382\n",
            "[49,    25] loss: 3.275172\n",
            "[50,    25] loss: 3.675655\n",
            "[51,    25] loss: 3.364672\n",
            "[52,    25] loss: 3.396644\n",
            "[53,    25] loss: 3.613345\n",
            "[54,    25] loss: 3.890607\n",
            "[55,    25] loss: 3.726269\n",
            "[56,    25] loss: 3.531065\n",
            "[57,    25] loss: 3.384817\n",
            "[58,    25] loss: 3.420884\n",
            "[59,    25] loss: 3.528943\n",
            "[60,    25] loss: 3.940215\n",
            "[61,    25] loss: 3.632949\n",
            "[62,    25] loss: 3.423648\n",
            "[63,    25] loss: 3.465725\n",
            "[64,    25] loss: 3.357043\n",
            "[65,    25] loss: 3.454328\n",
            "[66,    25] loss: 3.482855\n",
            "[67,    25] loss: 3.479933\n",
            "[68,    25] loss: 3.461311\n",
            "[69,    25] loss: 3.307581\n",
            "[70,    25] loss: 3.682388\n",
            "[71,    25] loss: 3.491590\n",
            "[72,    25] loss: 3.405821\n",
            "[73,    25] loss: 3.617238\n",
            "[74,    25] loss: 3.584106\n",
            "[75,    25] loss: 3.623751\n",
            "[76,    25] loss: 3.376576\n",
            "[77,    25] loss: 3.536621\n",
            "[78,    25] loss: 3.802177\n",
            "[79,    25] loss: 3.565262\n",
            "[80,    25] loss: 3.783268\n",
            "[81,    25] loss: 3.633185\n",
            "[82,    25] loss: 3.615195\n",
            "[83,    25] loss: 3.771924\n",
            "[84,    25] loss: 3.653516\n",
            "[85,    25] loss: 3.530365\n",
            "[86,    25] loss: 3.410599\n",
            "[87,    25] loss: 3.442949\n",
            "[88,    25] loss: 4.094438\n",
            "[89,    25] loss: 4.128401\n",
            "[90,    25] loss: 3.911589\n",
            "[91,    25] loss: 3.935843\n",
            "[92,    25] loss: 3.733799\n",
            "[93,    25] loss: 3.820844\n",
            "[94,    25] loss: 4.196105\n",
            "[95,    25] loss: 4.027033\n",
            "[96,    25] loss: 4.175414\n",
            "[97,    25] loss: 3.975827\n",
            "[98,    25] loss: 3.765937\n",
            "[99,    25] loss: 3.678007\n",
            "[100,    25] loss: 4.065205\n",
            "[101,    25] loss: 3.785147\n",
            "[102,    25] loss: 4.107969\n",
            "[103,    25] loss: 4.007933\n",
            "[104,    25] loss: 3.943985\n",
            "[105,    25] loss: 4.240521\n",
            "[106,    25] loss: 3.820556\n",
            "[107,    25] loss: 4.164508\n",
            "[108,    25] loss: 4.066358\n",
            "[109,    25] loss: 3.988694\n",
            "[110,    25] loss: 3.979882\n",
            "[111,    25] loss: 4.180812\n",
            "[112,    25] loss: 4.196578\n",
            "[113,    25] loss: 3.847357\n",
            "[114,    25] loss: 3.940105\n",
            "[115,    25] loss: 3.758748\n",
            "[116,    25] loss: 3.811171\n",
            "[117,    25] loss: 3.743740\n",
            "[118,    25] loss: 3.775775\n",
            "[119,    25] loss: 3.824121\n",
            "[120,    25] loss: 4.027179\n",
            "[121,    25] loss: 4.108306\n",
            "[122,    25] loss: 4.015238\n",
            "[123,    25] loss: 3.774995\n",
            "[124,    25] loss: 4.047093\n",
            "[125,    25] loss: 3.833156\n",
            "[126,    25] loss: 4.136769\n",
            "[127,    25] loss: 4.551267\n",
            "[128,    25] loss: 4.038125\n",
            "[129,    25] loss: 4.005723\n",
            "[130,    25] loss: 3.880440\n",
            "[131,    25] loss: 3.849624\n",
            "[132,    25] loss: 3.893380\n",
            "[133,    25] loss: 3.851928\n",
            "[134,    25] loss: 3.843759\n",
            "[135,    25] loss: 4.182498\n",
            "[136,    25] loss: 3.927743\n",
            "[137,    25] loss: 3.874417\n",
            "[138,    25] loss: 3.772508\n",
            "[139,    25] loss: 3.893997\n",
            "[140,    25] loss: 4.062333\n",
            "[141,    25] loss: 3.923741\n",
            "[142,    25] loss: 3.872014\n",
            "[143,    25] loss: 3.951568\n",
            "[144,    25] loss: 3.956537\n",
            "[145,    25] loss: 3.868504\n",
            "[146,    25] loss: 3.973943\n",
            "[147,    25] loss: 4.067729\n",
            "[148,    25] loss: 3.977885\n",
            "[149,    25] loss: 4.118619\n",
            "[150,    25] loss: 4.180416\n",
            "[151,    25] loss: 3.883773\n",
            "[152,    25] loss: 4.282447\n",
            "[153,    25] loss: 4.167441\n",
            "[154,    25] loss: 4.336442\n",
            "[155,    25] loss: 4.222592\n",
            "[156,    25] loss: 4.295883\n",
            "[157,    25] loss: 4.098181\n",
            "[158,    25] loss: 3.993975\n",
            "[159,    25] loss: 4.005309\n",
            "[160,    25] loss: 3.978665\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.12333316281437874\n",
            "Test Accuracy -  8.125\n",
            "L2 values -  [0.039064118452370164, 0.32684038281440736]\n",
            "L2 values -  [0.039064118452370164, 0.12333316281437874]\n",
            "Sparsity in fc1.weight: 42.63%\n",
            "Sparsity in fc2.weight: 57.81%\n",
            "Sparsity in fc3.weight: 54.30%\n",
            "Sparsity in fc4.weight: 67.19%\n",
            "Sparsity in fc5.weight: 13.00%\n",
            "Global sparsity: 40.01%\n",
            "Avg Loss during Testing -  0.1790763571858406\n",
            "Test Accuracy -  28.125\n",
            "[1,    25] loss: 5.908042\n",
            "[2,    25] loss: 4.111068\n",
            "[3,    25] loss: 3.509089\n",
            "[4,    25] loss: 2.926763\n",
            "[5,    25] loss: 2.922529\n",
            "[6,    25] loss: 2.582225\n",
            "[7,    25] loss: 2.299910\n",
            "[8,    25] loss: 2.240416\n",
            "[9,    25] loss: 2.292712\n",
            "[10,    25] loss: 2.468711\n",
            "[11,    25] loss: 2.389773\n",
            "[12,    25] loss: 2.241427\n",
            "[13,    25] loss: 2.205942\n",
            "[14,    25] loss: 1.956483\n",
            "[15,    25] loss: 2.012900\n",
            "[16,    25] loss: 2.073808\n",
            "[17,    25] loss: 2.032490\n",
            "[18,    25] loss: 1.894202\n",
            "[19,    25] loss: 2.058704\n",
            "[20,    25] loss: 2.027423\n",
            "[21,    25] loss: 2.194605\n",
            "[22,    25] loss: 1.974318\n",
            "[23,    25] loss: 2.024091\n",
            "[24,    25] loss: 2.089760\n",
            "[25,    25] loss: 1.908841\n",
            "[26,    25] loss: 1.911355\n",
            "[27,    25] loss: 2.180152\n",
            "[28,    25] loss: 2.133272\n",
            "[29,    25] loss: 2.073916\n",
            "[30,    25] loss: 2.086742\n",
            "[31,    25] loss: 1.950111\n",
            "[32,    25] loss: 1.839260\n",
            "[33,    25] loss: 1.902157\n",
            "[34,    25] loss: 2.072046\n",
            "[35,    25] loss: 1.995359\n",
            "[36,    25] loss: 1.948438\n",
            "[37,    25] loss: 2.449354\n",
            "[38,    25] loss: 2.353831\n",
            "[39,    25] loss: 2.449061\n",
            "[40,    25] loss: 2.345946\n",
            "[41,    25] loss: 2.037701\n",
            "[42,    25] loss: 2.406000\n",
            "[43,    25] loss: 2.599202\n",
            "[44,    25] loss: 2.308508\n",
            "[45,    25] loss: 2.197569\n",
            "[46,    25] loss: 2.162763\n",
            "[47,    25] loss: 2.211134\n",
            "[48,    25] loss: 2.555666\n",
            "[49,    25] loss: 2.224155\n",
            "[50,    25] loss: 2.161563\n",
            "[51,    25] loss: 2.287558\n",
            "[52,    25] loss: 2.393938\n",
            "[53,    25] loss: 2.390991\n",
            "[54,    25] loss: 2.177458\n",
            "[55,    25] loss: 2.028173\n",
            "[56,    25] loss: 2.579841\n",
            "[57,    25] loss: 2.395817\n",
            "[58,    25] loss: 2.353921\n",
            "[59,    25] loss: 2.115054\n",
            "[60,    25] loss: 2.262303\n",
            "[61,    25] loss: 2.265206\n",
            "[62,    25] loss: 2.076138\n",
            "[63,    25] loss: 2.155163\n",
            "[64,    25] loss: 2.365761\n",
            "[65,    25] loss: 2.538816\n",
            "[66,    25] loss: 2.015148\n",
            "[67,    25] loss: 2.358689\n",
            "[68,    25] loss: 2.153522\n",
            "[69,    25] loss: 2.171631\n",
            "[70,    25] loss: 2.143193\n",
            "[71,    25] loss: 2.421894\n",
            "[72,    25] loss: 2.467604\n",
            "[73,    25] loss: 2.333138\n",
            "[74,    25] loss: 2.438952\n",
            "[75,    25] loss: 2.221156\n",
            "[76,    25] loss: 2.089380\n",
            "[77,    25] loss: 2.069741\n",
            "[78,    25] loss: 2.157717\n",
            "[79,    25] loss: 2.463891\n",
            "[80,    25] loss: 2.232998\n",
            "[81,    25] loss: 2.132949\n",
            "[82,    25] loss: 2.255497\n",
            "[83,    25] loss: 2.189507\n",
            "[84,    25] loss: 2.291364\n",
            "[85,    25] loss: 2.288203\n",
            "[86,    25] loss: 2.687285\n",
            "[87,    25] loss: 2.557589\n",
            "[88,    25] loss: 2.514791\n",
            "[89,    25] loss: 2.623435\n",
            "[90,    25] loss: 2.262032\n",
            "[91,    25] loss: 2.801136\n",
            "[92,    25] loss: 2.991551\n",
            "[93,    25] loss: 3.017997\n",
            "[94,    25] loss: 2.672640\n",
            "[95,    25] loss: 2.656399\n",
            "[96,    25] loss: 2.509651\n",
            "[97,    25] loss: 2.651561\n",
            "[98,    25] loss: 2.493842\n",
            "[99,    25] loss: 2.402322\n",
            "[100,    25] loss: 2.514546\n",
            "[101,    25] loss: 2.399728\n",
            "[102,    25] loss: 2.530497\n",
            "[103,    25] loss: 2.536428\n",
            "[104,    25] loss: 2.386868\n",
            "[105,    25] loss: 2.506316\n",
            "[106,    25] loss: 2.757225\n",
            "[107,    25] loss: 2.416176\n",
            "[108,    25] loss: 2.552494\n",
            "[109,    25] loss: 2.516227\n",
            "[110,    25] loss: 2.706959\n",
            "[111,    25] loss: 2.460375\n",
            "[112,    25] loss: 2.510086\n",
            "[113,    25] loss: 2.313436\n",
            "[114,    25] loss: 2.536091\n",
            "[115,    25] loss: 2.612384\n",
            "[116,    25] loss: 2.481112\n",
            "[117,    25] loss: 2.694999\n",
            "[118,    25] loss: 2.572694\n",
            "[119,    25] loss: 2.464156\n",
            "[120,    25] loss: 2.302587\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.07874030135571956\n",
            "Test Accuracy -  28.125\n",
            "L1 values -  [0.039064118452370164, 0.06395818032324314, 0.1790763571858406]\n",
            "L1 values -  [0.039064118452370164, 0.12012229412794113, 0.07874030135571956]\n",
            "Sparsity in fc1.weight: 39.06%\n",
            "Sparsity in fc2.weight: 40.62%\n",
            "Sparsity in fc3.weight: 42.97%\n",
            "Sparsity in fc4.weight: 43.75%\n",
            "Sparsity in fc5.weight: 40.50%\n",
            "Global sparsity: 40.01%\n",
            "Avg Loss during Testing -  0.3727020531892776\n",
            "Test Accuracy -  1.875\n",
            "[1,    25] loss: 8.799817\n",
            "[2,    25] loss: 5.030626\n",
            "[3,    25] loss: 4.223553\n",
            "[4,    25] loss: 3.953551\n",
            "[5,    25] loss: 3.928237\n",
            "[6,    25] loss: 3.572411\n",
            "[7,    25] loss: 3.682133\n",
            "[8,    25] loss: 3.477470\n",
            "[9,    25] loss: 3.399474\n",
            "[10,    25] loss: 3.366212\n",
            "[11,    25] loss: 3.692805\n",
            "[12,    25] loss: 3.288076\n",
            "[13,    25] loss: 3.344707\n",
            "[14,    25] loss: 3.422812\n",
            "[15,    25] loss: 3.328101\n",
            "[16,    25] loss: 3.238011\n",
            "[17,    25] loss: 3.120665\n",
            "[18,    25] loss: 3.189121\n",
            "[19,    25] loss: 2.911603\n",
            "[20,    25] loss: 3.024576\n",
            "[21,    25] loss: 3.018746\n",
            "[22,    25] loss: 3.133189\n",
            "[23,    25] loss: 2.920648\n",
            "[24,    25] loss: 3.052274\n",
            "[25,    25] loss: 3.017805\n",
            "[26,    25] loss: 2.947453\n",
            "[27,    25] loss: 3.016182\n",
            "[28,    25] loss: 2.995142\n",
            "[29,    25] loss: 3.234365\n",
            "[30,    25] loss: 3.077156\n",
            "[31,    25] loss: 3.435469\n",
            "[32,    25] loss: 3.351183\n",
            "[33,    25] loss: 3.105195\n",
            "[34,    25] loss: 3.109390\n",
            "[35,    25] loss: 3.106876\n",
            "[36,    25] loss: 3.141844\n",
            "[37,    25] loss: 3.234662\n",
            "[38,    25] loss: 3.172205\n",
            "[39,    25] loss: 3.151535\n",
            "[40,    25] loss: 3.171925\n",
            "[41,    25] loss: 3.509467\n",
            "[42,    25] loss: 3.354774\n",
            "[43,    25] loss: 3.708507\n",
            "[44,    25] loss: 3.253757\n",
            "[45,    25] loss: 3.285144\n",
            "[46,    25] loss: 3.203851\n",
            "[47,    25] loss: 3.337871\n",
            "[48,    25] loss: 3.285315\n",
            "[49,    25] loss: 3.287227\n",
            "[50,    25] loss: 3.268205\n",
            "[51,    25] loss: 3.219968\n",
            "[52,    25] loss: 3.268483\n",
            "[53,    25] loss: 3.135760\n",
            "[54,    25] loss: 3.060981\n",
            "[55,    25] loss: 3.380741\n",
            "[56,    25] loss: 3.425843\n",
            "[57,    25] loss: 3.182113\n",
            "[58,    25] loss: 3.148073\n",
            "[59,    25] loss: 3.223709\n",
            "[60,    25] loss: 3.079836\n",
            "[61,    25] loss: 3.338508\n",
            "[62,    25] loss: 3.544407\n",
            "[63,    25] loss: 3.316752\n",
            "[64,    25] loss: 3.135083\n",
            "[65,    25] loss: 3.142244\n",
            "[66,    25] loss: 3.207096\n",
            "[67,    25] loss: 3.299088\n",
            "[68,    25] loss: 3.140553\n",
            "[69,    25] loss: 3.119123\n",
            "[70,    25] loss: 3.353316\n",
            "[71,    25] loss: 3.996370\n",
            "[72,    25] loss: 4.609380\n",
            "[73,    25] loss: 4.625319\n",
            "[74,    25] loss: 4.523889\n",
            "[75,    25] loss: 4.583299\n",
            "[76,    25] loss: 4.001177\n",
            "[77,    25] loss: 3.967661\n",
            "[78,    25] loss: 3.806633\n",
            "[79,    25] loss: 3.930349\n",
            "[80,    25] loss: 3.884763\n",
            "[81,    25] loss: 3.744891\n",
            "[82,    25] loss: 3.753392\n",
            "[83,    25] loss: 3.617146\n",
            "[84,    25] loss: 3.724647\n",
            "[85,    25] loss: 3.869460\n",
            "[86,    25] loss: 3.681503\n",
            "[87,    25] loss: 3.509055\n",
            "[88,    25] loss: 3.775916\n",
            "[89,    25] loss: 3.670832\n",
            "[90,    25] loss: 3.549221\n",
            "[91,    25] loss: 3.555266\n",
            "[92,    25] loss: 3.600176\n",
            "[93,    25] loss: 3.705058\n",
            "[94,    25] loss: 3.824298\n",
            "[95,    25] loss: 3.730389\n",
            "[96,    25] loss: 3.747814\n",
            "[97,    25] loss: 3.642334\n",
            "[98,    25] loss: 3.721858\n",
            "[99,    25] loss: 3.569583\n",
            "[100,    25] loss: 3.484092\n",
            "[101,    25] loss: 3.564468\n",
            "[102,    25] loss: 3.454770\n",
            "[103,    25] loss: 3.581901\n",
            "[104,    25] loss: 3.394207\n",
            "[105,    25] loss: 3.551526\n",
            "[106,    25] loss: 3.777400\n",
            "[107,    25] loss: 3.651208\n",
            "[108,    25] loss: 3.723465\n",
            "[109,    25] loss: 3.714538\n",
            "[110,    25] loss: 3.612573\n",
            "[111,    25] loss: 3.871353\n",
            "[112,    25] loss: 3.794518\n",
            "[113,    25] loss: 4.103981\n",
            "[114,    25] loss: 3.915553\n",
            "[115,    25] loss: 3.936939\n",
            "[116,    25] loss: 3.909316\n",
            "[117,    25] loss: 3.792569\n",
            "[118,    25] loss: 3.784296\n",
            "[119,    25] loss: 3.737908\n",
            "[120,    25] loss: 3.867235\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.14181091859936715\n",
            "Test Accuracy -  4.6875\n",
            "L2 values -  [0.039064118452370164, 0.32684038281440736, 0.3727020531892776]\n",
            "L2 values -  [0.039064118452370164, 0.12333316281437874, 0.14181091859936715]\n",
            "Sparsity in fc1.weight: 64.16%\n",
            "Sparsity in fc2.weight: 81.84%\n",
            "Sparsity in fc3.weight: 82.03%\n",
            "Sparsity in fc4.weight: 85.94%\n",
            "Sparsity in fc5.weight: 24.12%\n",
            "Global sparsity: 59.99%\n",
            "Avg Loss during Testing -  0.29676737189292907\n",
            "Test Accuracy -  5.625\n",
            "[1,    25] loss: 7.555471\n",
            "[2,    25] loss: 4.137530\n",
            "[3,    25] loss: 2.958059\n",
            "[4,    25] loss: 2.652803\n",
            "[5,    25] loss: 2.539743\n",
            "[6,    25] loss: 2.340090\n",
            "[7,    25] loss: 2.118355\n",
            "[8,    25] loss: 2.345450\n",
            "[9,    25] loss: 2.024687\n",
            "[10,    25] loss: 2.046627\n",
            "[11,    25] loss: 2.202558\n",
            "[12,    25] loss: 2.111316\n",
            "[13,    25] loss: 1.909424\n",
            "[14,    25] loss: 1.804354\n",
            "[15,    25] loss: 1.853248\n",
            "[16,    25] loss: 1.958156\n",
            "[17,    25] loss: 1.849823\n",
            "[18,    25] loss: 1.749905\n",
            "[19,    25] loss: 1.696657\n",
            "[20,    25] loss: 1.654596\n",
            "[21,    25] loss: 1.692328\n",
            "[22,    25] loss: 1.964705\n",
            "[23,    25] loss: 1.757553\n",
            "[24,    25] loss: 1.798894\n",
            "[25,    25] loss: 1.697093\n",
            "[26,    25] loss: 1.726897\n",
            "[27,    25] loss: 1.800776\n",
            "[28,    25] loss: 1.723899\n",
            "[29,    25] loss: 1.737224\n",
            "[30,    25] loss: 1.723291\n",
            "[31,    25] loss: 1.617050\n",
            "[32,    25] loss: 1.590937\n",
            "[33,    25] loss: 1.556410\n",
            "[34,    25] loss: 1.581843\n",
            "[35,    25] loss: 1.684618\n",
            "[36,    25] loss: 1.506528\n",
            "[37,    25] loss: 1.588028\n",
            "[38,    25] loss: 1.690244\n",
            "[39,    25] loss: 1.565505\n",
            "[40,    25] loss: 1.685849\n",
            "[41,    25] loss: 1.596264\n",
            "[42,    25] loss: 1.670172\n",
            "[43,    25] loss: 1.608923\n",
            "[44,    25] loss: 1.708697\n",
            "[45,    25] loss: 1.656457\n",
            "[46,    25] loss: 1.588746\n",
            "[47,    25] loss: 1.538829\n",
            "[48,    25] loss: 1.494376\n",
            "[49,    25] loss: 1.533651\n",
            "[50,    25] loss: 1.549322\n",
            "[51,    25] loss: 1.507789\n",
            "[52,    25] loss: 1.414855\n",
            "[53,    25] loss: 1.553380\n",
            "[54,    25] loss: 1.463677\n",
            "[55,    25] loss: 1.435740\n",
            "[56,    25] loss: 1.390747\n",
            "[57,    25] loss: 1.448767\n",
            "[58,    25] loss: 1.561528\n",
            "[59,    25] loss: 1.630617\n",
            "[60,    25] loss: 1.506746\n",
            "[61,    25] loss: 1.573940\n",
            "[62,    25] loss: 1.560784\n",
            "[63,    25] loss: 1.607057\n",
            "[64,    25] loss: 1.464733\n",
            "[65,    25] loss: 1.623070\n",
            "[66,    25] loss: 1.493891\n",
            "[67,    25] loss: 1.582578\n",
            "[68,    25] loss: 1.340511\n",
            "[69,    25] loss: 1.320307\n",
            "[70,    25] loss: 1.466895\n",
            "[71,    25] loss: 1.328278\n",
            "[72,    25] loss: 1.458639\n",
            "[73,    25] loss: 1.618394\n",
            "[74,    25] loss: 1.417207\n",
            "[75,    25] loss: 1.436871\n",
            "[76,    25] loss: 1.674542\n",
            "[77,    25] loss: 1.498132\n",
            "[78,    25] loss: 1.558462\n",
            "[79,    25] loss: 1.474338\n",
            "[80,    25] loss: 1.510200\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.04445531293749809\n",
            "Test Accuracy -  57.8125\n",
            "L1 values -  [0.039064118452370164, 0.06395818032324314, 0.1790763571858406, 0.29676737189292907]\n",
            "L1 values -  [0.039064118452370164, 0.12012229412794113, 0.07874030135571956, 0.04445531293749809]\n",
            "Sparsity in fc1.weight: 59.28%\n",
            "Sparsity in fc2.weight: 60.74%\n",
            "Sparsity in fc3.weight: 60.16%\n",
            "Sparsity in fc4.weight: 60.16%\n",
            "Sparsity in fc5.weight: 61.25%\n",
            "Global sparsity: 59.99%\n",
            "Avg Loss during Testing -  0.34391200840473174\n",
            "Test Accuracy -  1.5625\n",
            "[1,    25] loss: 7.435650\n",
            "[2,    25] loss: 4.855195\n",
            "[3,    25] loss: 4.431209\n",
            "[4,    25] loss: 4.263333\n",
            "[5,    25] loss: 4.090265\n",
            "[6,    25] loss: 3.924379\n",
            "[7,    25] loss: 3.889906\n",
            "[8,    25] loss: 3.830383\n",
            "[9,    25] loss: 3.548529\n",
            "[10,    25] loss: 3.567152\n",
            "[11,    25] loss: 3.603506\n",
            "[12,    25] loss: 3.454468\n",
            "[13,    25] loss: 3.383658\n",
            "[14,    25] loss: 3.438450\n",
            "[15,    25] loss: 3.410523\n",
            "[16,    25] loss: 3.306200\n",
            "[17,    25] loss: 3.352992\n",
            "[18,    25] loss: 3.353895\n",
            "[19,    25] loss: 3.427318\n",
            "[20,    25] loss: 3.364118\n",
            "[21,    25] loss: 3.332238\n",
            "[22,    25] loss: 3.485780\n",
            "[23,    25] loss: 3.378696\n",
            "[24,    25] loss: 3.266069\n",
            "[25,    25] loss: 3.272864\n",
            "[26,    25] loss: 3.293520\n",
            "[27,    25] loss: 3.681542\n",
            "[28,    25] loss: 3.322970\n",
            "[29,    25] loss: 3.551365\n",
            "[30,    25] loss: 3.357682\n",
            "[31,    25] loss: 3.335964\n",
            "[32,    25] loss: 3.331895\n",
            "[33,    25] loss: 3.325134\n",
            "[34,    25] loss: 3.336562\n",
            "[35,    25] loss: 3.336735\n",
            "[36,    25] loss: 3.221247\n",
            "[37,    25] loss: 3.156722\n",
            "[38,    25] loss: 3.244179\n",
            "[39,    25] loss: 3.271992\n",
            "[40,    25] loss: 3.164403\n",
            "[41,    25] loss: 3.311858\n",
            "[42,    25] loss: 3.161203\n",
            "[43,    25] loss: 3.205662\n",
            "[44,    25] loss: 3.175251\n",
            "[45,    25] loss: 3.545120\n",
            "[46,    25] loss: 3.681831\n",
            "[47,    25] loss: 3.396013\n",
            "[48,    25] loss: 3.191164\n",
            "[49,    25] loss: 3.145370\n",
            "[50,    25] loss: 3.111716\n",
            "[51,    25] loss: 3.085130\n",
            "[52,    25] loss: 3.099139\n",
            "[53,    25] loss: 3.326562\n",
            "[54,    25] loss: 3.157579\n",
            "[55,    25] loss: 3.269559\n",
            "[56,    25] loss: 3.347920\n",
            "[57,    25] loss: 3.128466\n",
            "[58,    25] loss: 3.181730\n",
            "[59,    25] loss: 3.410315\n",
            "[60,    25] loss: 3.273359\n",
            "[61,    25] loss: 3.219964\n",
            "[62,    25] loss: 3.126142\n",
            "[63,    25] loss: 3.127718\n",
            "[64,    25] loss: 3.447570\n",
            "[65,    25] loss: 3.339324\n",
            "[66,    25] loss: 3.346311\n",
            "[67,    25] loss: 3.236729\n",
            "[68,    25] loss: 3.396494\n",
            "[69,    25] loss: 3.337402\n",
            "[70,    25] loss: 3.306799\n",
            "[71,    25] loss: 3.183305\n",
            "[72,    25] loss: 3.169511\n",
            "[73,    25] loss: 3.103884\n",
            "[74,    25] loss: 3.056369\n",
            "[75,    25] loss: 3.053911\n",
            "[76,    25] loss: 3.071708\n",
            "[77,    25] loss: 3.163147\n",
            "[78,    25] loss: 3.063722\n",
            "[79,    25] loss: 3.245696\n",
            "[80,    25] loss: 3.285845\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.09698345437645912\n",
            "Test Accuracy -  15.0\n",
            "L2 values -  [0.039064118452370164, 0.32684038281440736, 0.3727020531892776, 0.34391200840473174]\n",
            "L2 values -  [0.039064118452370164, 0.12333316281437874, 0.14181091859936715, 0.09698345437645912]\n",
            "Sparsity in fc1.weight: 86.52%\n",
            "Sparsity in fc2.weight: 94.34%\n",
            "Sparsity in fc3.weight: 95.31%\n",
            "Sparsity in fc4.weight: 98.44%\n",
            "Sparsity in fc5.weight: 46.25%\n",
            "Global sparsity: 79.99%\n",
            "Avg Loss during Testing -  0.4044028490781784\n",
            "Test Accuracy -  1.875\n",
            "[1,    25] loss: 8.265346\n",
            "[2,    25] loss: 4.889651\n",
            "[3,    25] loss: 4.711883\n",
            "[4,    25] loss: 4.769772\n",
            "[5,    25] loss: 4.757798\n",
            "[6,    25] loss: 4.630229\n",
            "[7,    25] loss: 4.577440\n",
            "[8,    25] loss: 4.552368\n",
            "[9,    25] loss: 4.599579\n",
            "[10,    25] loss: 4.586277\n",
            "[11,    25] loss: 4.516948\n",
            "[12,    25] loss: 4.469846\n",
            "[13,    25] loss: 4.457837\n",
            "[14,    25] loss: 4.478587\n",
            "[15,    25] loss: 4.443443\n",
            "[16,    25] loss: 4.461503\n",
            "[17,    25] loss: 4.463650\n",
            "[18,    25] loss: 4.403113\n",
            "[19,    25] loss: 4.393199\n",
            "[20,    25] loss: 4.478090\n",
            "[21,    25] loss: 4.415962\n",
            "[22,    25] loss: 4.407845\n",
            "[23,    25] loss: 4.367985\n",
            "[24,    25] loss: 4.409354\n",
            "[25,    25] loss: 4.401479\n",
            "[26,    25] loss: 4.349153\n",
            "[27,    25] loss: 4.378455\n",
            "[28,    25] loss: 4.351082\n",
            "[29,    25] loss: 4.389549\n",
            "[30,    25] loss: 4.355578\n",
            "[31,    25] loss: 4.352857\n",
            "[32,    25] loss: 4.328725\n",
            "[33,    25] loss: 4.356020\n",
            "[34,    25] loss: 4.340399\n",
            "[35,    25] loss: 4.334945\n",
            "[36,    25] loss: 4.341308\n",
            "[37,    25] loss: 4.309885\n",
            "[38,    25] loss: 4.320286\n",
            "[39,    25] loss: 4.330720\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.13602148592472077\n",
            "Test Accuracy -  2.1875\n",
            "L1 values -  [0.039064118452370164, 0.06395818032324314, 0.1790763571858406, 0.29676737189292907, 0.4044028490781784]\n",
            "L1 values -  [0.039064118452370164, 0.12012229412794113, 0.07874030135571956, 0.04445531293749809, 0.13602148592472077]\n",
            "Sparsity in fc1.weight: 80.13%\n",
            "Sparsity in fc2.weight: 79.49%\n",
            "Sparsity in fc3.weight: 76.56%\n",
            "Sparsity in fc4.weight: 82.81%\n",
            "Sparsity in fc5.weight: 80.62%\n",
            "Global sparsity: 79.99%\n",
            "Avg Loss during Testing -  0.2707553431391716\n",
            "Test Accuracy -  1.25\n",
            "[1,    25] loss: 7.144750\n",
            "[2,    25] loss: 4.878119\n",
            "[3,    25] loss: 4.480099\n",
            "[4,    25] loss: 4.286806\n",
            "[5,    25] loss: 4.149090\n",
            "[6,    25] loss: 4.074239\n",
            "[7,    25] loss: 4.050911\n",
            "[8,    25] loss: 3.974727\n",
            "[9,    25] loss: 3.946841\n",
            "[10,    25] loss: 3.857674\n",
            "[11,    25] loss: 3.852112\n",
            "[12,    25] loss: 3.796048\n",
            "[13,    25] loss: 3.824957\n",
            "[14,    25] loss: 3.902831\n",
            "[15,    25] loss: 3.763918\n",
            "[16,    25] loss: 3.772427\n",
            "[17,    25] loss: 3.753634\n",
            "[18,    25] loss: 3.787657\n",
            "[19,    25] loss: 3.711445\n",
            "[20,    25] loss: 3.703798\n",
            "[21,    25] loss: 3.698109\n",
            "[22,    25] loss: 3.637513\n",
            "[23,    25] loss: 3.878336\n",
            "[24,    25] loss: 3.792961\n",
            "[25,    25] loss: 3.707000\n",
            "[26,    25] loss: 3.728138\n",
            "[27,    25] loss: 3.636707\n",
            "[28,    25] loss: 3.647810\n",
            "[29,    25] loss: 3.647707\n",
            "[30,    25] loss: 3.552806\n",
            "[31,    25] loss: 3.555792\n",
            "[32,    25] loss: 3.630965\n",
            "[33,    25] loss: 3.649746\n",
            "[34,    25] loss: 3.605010\n",
            "[35,    25] loss: 3.652864\n",
            "[36,    25] loss: 3.567247\n",
            "[37,    25] loss: 3.620842\n",
            "[38,    25] loss: 3.589753\n",
            "[39,    25] loss: 3.539223\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.11285362020134926\n",
            "Test Accuracy -  12.5\n",
            "L2 values -  [0.039064118452370164, 0.32684038281440736, 0.3727020531892776, 0.34391200840473174, 0.2707553431391716]\n",
            "L2 values -  [0.039064118452370164, 0.12333316281437874, 0.14181091859936715, 0.09698345437645912, 0.11285362020134926]\n",
            "Sparsity in fc1.weight: 95.12%\n",
            "Sparsity in fc2.weight: 98.63%\n",
            "Sparsity in fc3.weight: 100.00%\n",
            "Sparsity in fc4.weight: 100.00%\n",
            "Sparsity in fc5.weight: 66.62%\n",
            "Global sparsity: 90.01%\n",
            "Avg Loss during Testing -  0.29185267984867097\n",
            "Test Accuracy -  1.875\n",
            "[1,    25] loss: 6.536619\n",
            "[2,    25] loss: 4.808361\n",
            "[3,    25] loss: 4.685478\n",
            "[4,    25] loss: 4.670853\n",
            "[5,    25] loss: 4.700170\n",
            "[6,    25] loss: 4.707152\n",
            "[7,    25] loss: 4.741133\n",
            "[8,    25] loss: 4.695720\n",
            "[9,    25] loss: 4.728060\n",
            "[10,    25] loss: 4.711367\n",
            "[11,    25] loss: 4.706424\n",
            "[12,    25] loss: 4.721643\n",
            "[13,    25] loss: 4.679191\n",
            "[14,    25] loss: 4.698053\n",
            "[15,    25] loss: 4.689831\n",
            "[16,    25] loss: 4.669631\n",
            "[17,    25] loss: 4.663095\n",
            "[18,    25] loss: 4.663203\n",
            "[19,    25] loss: 4.682935\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.14465386867523194\n",
            "Test Accuracy -  1.25\n",
            "L1 values -  [0.039064118452370164, 0.06395818032324314, 0.1790763571858406, 0.29676737189292907, 0.4044028490781784, 0.29185267984867097]\n",
            "L1 values -  [0.039064118452370164, 0.12012229412794113, 0.07874030135571956, 0.04445531293749809, 0.13602148592472077, 0.14465386867523194]\n",
            "Sparsity in fc1.weight: 90.23%\n",
            "Sparsity in fc2.weight: 90.04%\n",
            "Sparsity in fc3.weight: 90.62%\n",
            "Sparsity in fc4.weight: 88.28%\n",
            "Sparsity in fc5.weight: 89.50%\n",
            "Global sparsity: 90.01%\n",
            "Avg Loss during Testing -  0.26085077226161957\n",
            "Test Accuracy -  1.25\n",
            "[1,    25] loss: 6.738685\n",
            "[2,    25] loss: 4.823217\n",
            "[3,    25] loss: 4.611006\n",
            "[4,    25] loss: 4.586077\n",
            "[5,    25] loss: 4.514666\n",
            "[6,    25] loss: 4.498238\n",
            "[7,    25] loss: 4.448327\n",
            "[8,    25] loss: 4.407063\n",
            "[9,    25] loss: 4.378394\n",
            "[10,    25] loss: 4.363904\n",
            "[11,    25] loss: 4.333740\n",
            "[12,    25] loss: 4.361195\n",
            "[13,    25] loss: 4.302128\n",
            "[14,    25] loss: 4.318020\n",
            "[15,    25] loss: 4.253281\n",
            "[16,    25] loss: 4.211563\n",
            "[17,    25] loss: 4.316865\n",
            "[18,    25] loss: 4.230020\n",
            "[19,    25] loss: 4.257864\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.1309281289577484\n",
            "Test Accuracy -  3.4375\n",
            "L2 values -  [0.039064118452370164, 0.32684038281440736, 0.3727020531892776, 0.34391200840473174, 0.2707553431391716, 0.26085077226161957]\n",
            "L2 values -  [0.039064118452370164, 0.12333316281437874, 0.14181091859936715, 0.09698345437645912, 0.11285362020134926, 0.1309281289577484]\n"
          ]
        }
      ],
      "source": [
        "# Full cycle\n",
        "\n",
        "parameters_to_prune = (\n",
        "    (net.fc1, 'weight'),\n",
        "    (net.fc2, 'weight'),\n",
        "    (net.fc3, 'weight'),\n",
        "    (net.fc4, 'weight'),\n",
        "    (net.fc5, 'weight'),\n",
        ")\n",
        "\n",
        "batch_loss = 25\n",
        "epoch = 200\n",
        "\n",
        "results_base_l1 = []\n",
        "results_finetune_l1 = []\n",
        "\n",
        "results_base_l2 = []\n",
        "results_finetune_l2 = []\n",
        "\n",
        "# Network\n",
        "net = Net().to(device)\n",
        "print_sparsity(net)\n",
        "\n",
        "# criterion = nn.MSELoss()\n",
        "# optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
        "train(net, train_loader, epoch, optimizer, criterion, batch_loss)\n",
        "\n",
        "train(net, train_loader, epoch, optimizer, criterion, batch_loss)\n",
        "loss_mse, _ = test(net, test_loader, criterion)\n",
        "\n",
        "results_base_l1.append(loss_mse)\n",
        "results_finetune_l1.append(loss_mse)\n",
        "results_base_l2.append(loss_mse)\n",
        "results_finetune_l2.append(loss_mse)\n",
        "\n",
        "PATH = './cifar_net.pth'\n",
        "torch.save(net.state_dict(), PATH)\n",
        "\n",
        "prune_values = [0.2, 0.4, 0.6, 0.8, 0.9]\n",
        "pruning_methods = [0,1]\n",
        "\n",
        "\n",
        "for prune_value in prune_values:\n",
        "  for pruning_method in pruning_methods:\n",
        "  # Each prune value experiment is independent\n",
        "    net = Net().to(device)\n",
        "    net.load_state_dict(torch.load(PATH))\n",
        "    \n",
        "    parameters_to_prune = (\n",
        "    (net.fc1, 'weight'),\n",
        "    (net.fc2, 'weight'),\n",
        "    (net.fc3, 'weight'),\n",
        "    (net.fc4, 'weight'),\n",
        "    (net.fc5, 'weight'),\n",
        "    )\n",
        "\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.1)\n",
        "    \n",
        "    if pruning_method == 0:\n",
        "      prune.global_unstructured(\n",
        "          parameters_to_prune,\n",
        "          pruning_method=prune.L1Unstructured,\n",
        "          amount=prune_value,\n",
        "      )\n",
        "    else:\n",
        "      prune.global_unstructured(\n",
        "          parameters_to_prune,\n",
        "          pruning_method=prune.RandomUnstructured,\n",
        "          amount=prune_value,\n",
        "      )\n",
        "\n",
        "    print_sparsity(net)\n",
        "    \n",
        "    base_loss, _ = test(net, test_loader, criterion)\n",
        "\n",
        "    train(net, train_loader, int(epoch * (1-prune_value)), optimizer, criterion, batch_loss)\n",
        "    finetune_loss, _ = test(net, test_loader, criterion)\n",
        "    \n",
        "    if pruning_method == 0:\n",
        "      results_base_l1.append(base_loss)\n",
        "      results_finetune_l1.append(finetune_loss)\n",
        "\n",
        "      print('L1 values - ', results_base_l1)\n",
        "      print('L1 values - ', results_finetune_l1)\n",
        "    else:\n",
        "      results_base_l2.append(base_loss)\n",
        "      results_finetune_l2.append(finetune_loss)\n",
        "\n",
        "      print('L2 values - ', results_base_l2)\n",
        "      print('L2 values - ', results_finetune_l2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qqf2WvBVmsJH",
        "outputId": "68fb6f37-ba5b-4d76-f354-f7ead56c5efc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.039064118452370164,\n",
              " 0.06395818032324314,\n",
              " 0.1790763571858406,\n",
              " 0.29676737189292907,\n",
              " 0.4044028490781784,\n",
              " 0.29185267984867097]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "results_base_l1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPPioan3msJH",
        "outputId": "db939170-0c4c-4b19-c2f4-47f0797a2b82"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.039064118452370164,\n",
              " 0.12012229412794113,\n",
              " 0.07874030135571956,\n",
              " 0.04445531293749809,\n",
              " 0.13602148592472077,\n",
              " 0.14465386867523194]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "results_finetune_l1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yk59mUmgmsJI",
        "outputId": "a2bf81dd-6569-4e0a-c2a1-846f695c8c0c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.039064118452370164,\n",
              " 0.32684038281440736,\n",
              " 0.3727020531892776,\n",
              " 0.34391200840473174,\n",
              " 0.2707553431391716,\n",
              " 0.26085077226161957]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "results_base_l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxKo0dkkmsJI",
        "outputId": "4cccd650-90e2-4fa3-afd9-ddfaf7d6c1e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.039064118452370164,\n",
              " 0.12333316281437874,\n",
              " 0.14181091859936715,\n",
              " 0.09698345437645912,\n",
              " 0.11285362020134926,\n",
              " 0.1309281289577484]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "results_finetune_l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AO_G4qPmsJI"
      },
      "outputs": [],
      "source": [
        "x_coord = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "OiqOqIO8msJI",
        "outputId": "94e5b885-d563-43a8-9203-d0c8611a9f00"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('Housing Dataset with 5 layer NN and different pruning techniques')\n",
        "plt.plot(x_coord, results_base_l1, label = 'L1 without finetuning')\n",
        "plt.plot(x_coord, results_finetune_l1, label = 'L1 finetuned')\n",
        "plt.plot(x_coord, results_base_l2, label = 'Random without finetuning')\n",
        "plt.plot(x_coord, results_finetune_l2, label = 'Random finetuned')\n",
        "plt.ylabel('Cross Entropy loss')\n",
        "plt.xlabel('Sparsity')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBf7zmvBot2t"
      },
      "source": [
        "### 10 layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gc4t-rWqot2u"
      },
      "outputs": [],
      "source": [
        "def print_sparsity(net):\n",
        "  print(\n",
        "      \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc1.weight == 0))\n",
        "          / float(net.fc1.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Sparsity in fc2.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc2.weight == 0))\n",
        "          / float(net.fc2.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Sparsity in fc3.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc3.weight == 0))\n",
        "          / float(net.fc3.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Sparsity in fc4.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc4.weight == 0))\n",
        "          / float(net.fc4.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Sparsity in fc5.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc5.weight == 0))\n",
        "          / float(net.fc5.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "\n",
        "  print(\n",
        "      \"Sparsity in fc6.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc6.weight == 0))\n",
        "          / float(net.fc6.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Sparsity in fc7.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc7.weight == 0))\n",
        "          / float(net.fc7.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Sparsity in fc8.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc8.weight == 0))\n",
        "          / float(net.fc8.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Sparsity in fc9.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc9.weight == 0))\n",
        "          / float(net.fc9.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Sparsity in fc10.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc10.weight == 0))\n",
        "          / float(net.fc10.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "\n",
        "  print(\n",
        "      \"Global sparsity: {:.2f}%\".format(\n",
        "          100. * float(\n",
        "              + torch.sum(net.fc1.weight == 0)\n",
        "              + torch.sum(net.fc2.weight == 0)\n",
        "              + torch.sum(net.fc3.weight == 0)\n",
        "              + torch.sum(net.fc4.weight == 0)\n",
        "              + torch.sum(net.fc5.weight == 0)\n",
        "              + torch.sum(net.fc6.weight == 0)\n",
        "              + torch.sum(net.fc7.weight == 0)\n",
        "              + torch.sum(net.fc8.weight == 0)\n",
        "              + torch.sum(net.fc9.weight == 0)\n",
        "              + torch.sum(net.fc10.weight == 0)\n",
        "          )\n",
        "          / float(\n",
        "              + net.fc1.weight.nelement()\n",
        "              + net.fc2.weight.nelement()\n",
        "              + net.fc3.weight.nelement()\n",
        "              + net.fc4.weight.nelement()\n",
        "              + net.fc5.weight.nelement()\n",
        "              + net.fc6.weight.nelement()\n",
        "              + net.fc7.weight.nelement()\n",
        "              + net.fc8.weight.nelement()\n",
        "              + net.fc9.weight.nelement()\n",
        "              + net.fc10.weight.nelement()\n",
        "          )\n",
        "      )\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yAYT3Mgot2v"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(64, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 32)\n",
        "        self.fc4 = nn.Linear(32, 16)\n",
        "        self.fc5 = nn.Linear(16, 16)\n",
        "        self.fc6 = nn.Linear(16, 8)\n",
        "        self.fc7 = nn.Linear(8, 8)\n",
        "        self.fc8 = nn.Linear(8, 4)\n",
        "        self.fc9 = nn.Linear(4, 4)\n",
        "        self.fc10 = nn.Linear(4, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x)) \n",
        "        x = torch.tanh(self.fc3(x))\n",
        "        x = torch.tanh(self.fc4(x))\n",
        "        x = torch.tanh(self.fc5(x))\n",
        "        x = torch.tanh(self.fc6(x)) \n",
        "        x = torch.tanh(self.fc7(x))\n",
        "        x = torch.tanh(self.fc8(x))\n",
        "        x = torch.tanh(self.fc9(x))\n",
        "        x = self.fc10(x)\n",
        "        return x\n",
        "\n",
        "net = Net().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVgVvHOqot2v"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = nn.MSELoss()\n",
        "# # optimizer = optim.SGD(net.parameters(), lr=10)\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKfrukPOot2v",
        "outputId": "5b37be26-d2a2-43bf-8331-6e4bad352a45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg Loss during Testing -  0.14640987068414688\n",
            "Test Accuracy -  2.5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.14640987068414688, array(2.5, dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "# Testing before training\n",
        "test(net, test_loader, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xf9NFJEXot2w",
        "outputId": "fae6d043-bfd0-4d9e-8a6e-2c2722f40518"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sparsity in fc1.weight: 0.00%\n",
            "Sparsity in fc2.weight: 0.00%\n",
            "Sparsity in fc3.weight: 0.00%\n",
            "Sparsity in fc4.weight: 0.00%\n",
            "Sparsity in fc5.weight: 0.00%\n",
            "Global sparsity: 0.00%\n",
            "[1,    27] loss: 4.646023\n",
            "[2,    27] loss: 4.636933\n",
            "[3,    27] loss: 4.624580\n",
            "[4,    27] loss: 4.556397\n",
            "[5,    27] loss: 4.500441\n",
            "[6,    27] loss: 4.468856\n",
            "[7,    27] loss: 4.449550\n",
            "[8,    27] loss: 4.391895\n",
            "[9,    27] loss: 4.364040\n",
            "[10,    27] loss: 4.346049\n",
            "[11,    27] loss: 4.327953\n",
            "[12,    27] loss: 4.312538\n",
            "[13,    27] loss: 4.290895\n",
            "[14,    27] loss: 4.279556\n",
            "[15,    27] loss: 4.253607\n",
            "[16,    27] loss: 4.248405\n",
            "[17,    27] loss: 4.233179\n",
            "[18,    27] loss: 4.204756\n",
            "[19,    27] loss: 4.198934\n",
            "[20,    27] loss: 4.177586\n",
            "[21,    27] loss: 4.190125\n",
            "[22,    27] loss: 4.155558\n",
            "[23,    27] loss: 4.157257\n",
            "[24,    27] loss: 4.141148\n",
            "[25,    27] loss: 4.132850\n",
            "[26,    27] loss: 4.116157\n",
            "[27,    27] loss: 4.111962\n",
            "[28,    27] loss: 4.106843\n",
            "[29,    27] loss: 4.096178\n",
            "[30,    27] loss: 4.085420\n",
            "[31,    27] loss: 4.069962\n",
            "[32,    27] loss: 4.067651\n",
            "[33,    27] loss: 4.049924\n",
            "[34,    27] loss: 4.052167\n",
            "[35,    27] loss: 4.047558\n",
            "[36,    27] loss: 4.017159\n",
            "[37,    27] loss: 4.040591\n",
            "[38,    27] loss: 3.992583\n",
            "[39,    27] loss: 3.988009\n",
            "[40,    27] loss: 3.980576\n",
            "[41,    27] loss: 3.973715\n",
            "[42,    27] loss: 3.962096\n",
            "[43,    27] loss: 3.952235\n",
            "[44,    27] loss: 3.957974\n",
            "[45,    27] loss: 3.927101\n",
            "[46,    27] loss: 3.927296\n",
            "[47,    27] loss: 3.915264\n",
            "[48,    27] loss: 3.917266\n",
            "[49,    27] loss: 3.931146\n",
            "[50,    27] loss: 3.903007\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.12222350165247917\n",
            "Test Accuracy -  3.75\n",
            "Sparsity in fc1.weight: 22.83%\n",
            "Sparsity in fc2.weight: 21.24%\n",
            "Sparsity in fc3.weight: 16.99%\n",
            "Sparsity in fc4.weight: 19.92%\n",
            "Sparsity in fc5.weight: 13.67%\n",
            "Global sparsity: 21.18%\n",
            "Avg Loss during Testing -  0.12220958024263381\n",
            "Test Accuracy -  3.4375\n",
            "[1,    27] loss: 4.808547\n",
            "[2,    27] loss: 4.698545\n",
            "[3,    27] loss: 4.705962\n",
            "[4,    27] loss: 4.692715\n",
            "[5,    27] loss: 4.715440\n",
            "[6,    27] loss: 4.697839\n",
            "[7,    27] loss: 4.706112\n",
            "[8,    27] loss: 4.664643\n",
            "[9,    27] loss: 4.665012\n",
            "[10,    27] loss: 4.663378\n",
            "[11,    27] loss: 4.659976\n",
            "[12,    27] loss: 4.686271\n",
            "[13,    27] loss: 4.699401\n",
            "[14,    27] loss: 4.679870\n",
            "[15,    27] loss: 4.658773\n",
            "[16,    27] loss: 4.663368\n",
            "[17,    27] loss: 4.684900\n",
            "[18,    27] loss: 4.676664\n",
            "[19,    27] loss: 4.702152\n",
            "[20,    27] loss: 4.686939\n",
            "[21,    27] loss: 4.682426\n",
            "[22,    27] loss: 4.702954\n",
            "[23,    27] loss: 4.682419\n",
            "[24,    27] loss: 4.676290\n",
            "[25,    27] loss: 4.672780\n",
            "[26,    27] loss: 4.695859\n",
            "[27,    27] loss: 4.686575\n",
            "[28,    27] loss: 4.678480\n",
            "[29,    27] loss: 4.662441\n",
            "[30,    27] loss: 4.669583\n",
            "[31,    27] loss: 4.691655\n",
            "[32,    27] loss: 4.679029\n",
            "[33,    27] loss: 4.692274\n",
            "[34,    27] loss: 4.693590\n",
            "[35,    27] loss: 4.684470\n",
            "[36,    27] loss: 4.703003\n",
            "[37,    27] loss: 4.693046\n",
            "[38,    27] loss: 4.709138\n",
            "[39,    27] loss: 4.678609\n",
            "[40,    27] loss: 4.687996\n",
            "[41,    27] loss: 4.655287\n",
            "[42,    27] loss: 4.679896\n",
            "[43,    27] loss: 4.681068\n",
            "[44,    27] loss: 4.689649\n",
            "[45,    27] loss: 4.699397\n",
            "[46,    27] loss: 4.685172\n",
            "[47,    27] loss: 4.677667\n",
            "[48,    27] loss: 4.703890\n",
            "[49,    27] loss: 4.702293\n",
            "[50,    27] loss: 4.676976\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.14660850614309312\n",
            "Test Accuracy -  1.25\n",
            "L1 values -  [0.12222350165247917, 0.12220958024263381]\n",
            "L1 values -  [0.12222350165247917, 0.14660850614309312]\n",
            "Sparsity in fc1.weight: 20.68%\n",
            "Sparsity in fc2.weight: 20.31%\n",
            "Sparsity in fc3.weight: 19.43%\n",
            "Sparsity in fc4.weight: 19.34%\n",
            "Sparsity in fc5.weight: 20.70%\n",
            "Global sparsity: 20.34%\n",
            "Avg Loss during Testing -  0.13278350979089737\n",
            "Test Accuracy -  1.875\n",
            "[1,    27] loss: 4.833347\n",
            "[2,    27] loss: 4.690399\n",
            "[3,    27] loss: 4.673168\n",
            "[4,    27] loss: 4.671888\n",
            "[5,    27] loss: 4.658931\n",
            "[6,    27] loss: 4.663985\n",
            "[7,    27] loss: 4.672368\n",
            "[8,    27] loss: 4.681188\n",
            "[9,    27] loss: 4.702376\n",
            "[10,    27] loss: 4.681821\n",
            "[11,    27] loss: 4.673834\n",
            "[12,    27] loss: 4.664270\n",
            "[13,    27] loss: 4.675119\n",
            "[14,    27] loss: 4.658267\n",
            "[15,    27] loss: 4.677385\n",
            "[16,    27] loss: 4.666099\n",
            "[17,    27] loss: 4.673273\n",
            "[18,    27] loss: 4.664946\n",
            "[19,    27] loss: 4.665622\n",
            "[20,    27] loss: 4.690443\n",
            "[21,    27] loss: 4.678228\n",
            "[22,    27] loss: 4.695796\n",
            "[23,    27] loss: 4.677919\n",
            "[24,    27] loss: 4.658972\n",
            "[25,    27] loss: 4.670120\n",
            "[26,    27] loss: 4.677641\n",
            "[27,    27] loss: 4.670905\n",
            "[28,    27] loss: 4.681442\n",
            "[29,    27] loss: 4.674983\n",
            "[30,    27] loss: 4.668981\n",
            "[31,    27] loss: 4.675916\n",
            "[32,    27] loss: 4.677909\n",
            "[33,    27] loss: 4.655291\n",
            "[34,    27] loss: 4.670667\n",
            "[35,    27] loss: 4.672749\n",
            "[36,    27] loss: 4.656659\n",
            "[37,    27] loss: 4.671476\n",
            "[38,    27] loss: 4.661579\n",
            "[39,    27] loss: 4.671006\n",
            "[40,    27] loss: 4.685783\n",
            "[41,    27] loss: 4.666969\n",
            "[42,    27] loss: 4.680703\n",
            "[43,    27] loss: 4.697551\n",
            "[44,    27] loss: 4.668697\n",
            "[45,    27] loss: 4.685041\n",
            "[46,    27] loss: 4.685506\n",
            "[47,    27] loss: 4.690866\n",
            "[48,    27] loss: 4.675824\n",
            "[49,    27] loss: 4.679086\n",
            "[50,    27] loss: 4.674219\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.14689913690090178\n",
            "Test Accuracy -  2.5\n",
            "L2 values -  [0.12222350165247917, 0.13278350979089737]\n",
            "L2 values -  [0.12222350165247917, 0.14689913690090178]\n",
            "Sparsity in fc1.weight: 44.29%\n",
            "Sparsity in fc2.weight: 43.95%\n",
            "Sparsity in fc3.weight: 36.33%\n",
            "Sparsity in fc4.weight: 35.94%\n",
            "Sparsity in fc5.weight: 28.12%\n",
            "Global sparsity: 42.11%\n",
            "Avg Loss during Testing -  0.12239801809191704\n",
            "Test Accuracy -  2.8125\n",
            "[1,    27] loss: 4.856500\n",
            "[2,    27] loss: 4.711068\n",
            "[3,    27] loss: 4.667553\n",
            "[4,    27] loss: 4.682691\n",
            "[5,    27] loss: 4.659505\n",
            "[6,    27] loss: 4.663951\n",
            "[7,    27] loss: 4.699683\n",
            "[8,    27] loss: 4.671263\n",
            "[9,    27] loss: 4.694401\n",
            "[10,    27] loss: 4.735633\n",
            "[11,    27] loss: 4.685625\n",
            "[12,    27] loss: 4.702941\n",
            "[13,    27] loss: 4.715181\n",
            "[14,    27] loss: 4.683823\n",
            "[15,    27] loss: 4.662300\n",
            "[16,    27] loss: 4.674218\n",
            "[17,    27] loss: 4.683549\n",
            "[18,    27] loss: 4.702433\n",
            "[19,    27] loss: 4.690957\n",
            "[20,    27] loss: 4.666927\n",
            "[21,    27] loss: 4.674069\n",
            "[22,    27] loss: 4.690784\n",
            "[23,    27] loss: 4.657670\n",
            "[24,    27] loss: 4.679978\n",
            "[25,    27] loss: 4.671864\n",
            "[26,    27] loss: 4.669181\n",
            "[27,    27] loss: 4.714695\n",
            "[28,    27] loss: 4.687163\n",
            "[29,    27] loss: 4.681671\n",
            "[30,    27] loss: 4.659385\n",
            "[31,    27] loss: 4.669470\n",
            "[32,    27] loss: 4.692517\n",
            "[33,    27] loss: 4.670576\n",
            "[34,    27] loss: 4.671027\n",
            "[35,    27] loss: 4.694133\n",
            "[36,    27] loss: 4.688008\n",
            "[37,    27] loss: 4.675034\n",
            "[38,    27] loss: 4.690199\n",
            "[39,    27] loss: 4.679941\n",
            "[40,    27] loss: 4.688258\n",
            "[41,    27] loss: 4.672241\n",
            "[42,    27] loss: 4.675791\n",
            "[43,    27] loss: 4.710005\n",
            "[44,    27] loss: 4.676191\n",
            "[45,    27] loss: 4.678731\n",
            "[46,    27] loss: 4.665605\n",
            "[47,    27] loss: 4.678839\n",
            "[48,    27] loss: 4.671831\n",
            "[49,    27] loss: 4.677627\n",
            "[50,    27] loss: 4.683799\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.14563148617744445\n",
            "Test Accuracy -  0.625\n",
            "L1 values -  [0.12222350165247917, 0.12220958024263381, 0.12239801809191704]\n",
            "L1 values -  [0.12222350165247917, 0.14660850614309312, 0.14563148617744445]\n",
            "Sparsity in fc1.weight: 41.31%\n",
            "Sparsity in fc2.weight: 39.70%\n",
            "Sparsity in fc3.weight: 39.16%\n",
            "Sparsity in fc4.weight: 37.50%\n",
            "Sparsity in fc5.weight: 41.41%\n",
            "Global sparsity: 40.37%\n",
            "Avg Loss during Testing -  0.1425493150949478\n",
            "Test Accuracy -  1.25\n",
            "[1,    27] loss: 4.802306\n",
            "[2,    27] loss: 4.684891\n",
            "[3,    27] loss: 4.664570\n",
            "[4,    27] loss: 4.660596\n",
            "[5,    27] loss: 4.655878\n",
            "[6,    27] loss: 4.686370\n",
            "[7,    27] loss: 4.678031\n",
            "[8,    27] loss: 4.673131\n",
            "[9,    27] loss: 4.658532\n",
            "[10,    27] loss: 4.665131\n",
            "[11,    27] loss: 4.671647\n",
            "[12,    27] loss: 4.663696\n",
            "[13,    27] loss: 4.684185\n",
            "[14,    27] loss: 4.691147\n",
            "[15,    27] loss: 4.658710\n",
            "[16,    27] loss: 4.659439\n",
            "[17,    27] loss: 4.665548\n",
            "[18,    27] loss: 4.667038\n",
            "[19,    27] loss: 4.665968\n",
            "[20,    27] loss: 4.660408\n",
            "[21,    27] loss: 4.681471\n",
            "[22,    27] loss: 4.691487\n",
            "[23,    27] loss: 4.674861\n",
            "[24,    27] loss: 4.685169\n",
            "[25,    27] loss: 4.659985\n",
            "[26,    27] loss: 4.664786\n",
            "[27,    27] loss: 4.662524\n",
            "[28,    27] loss: 4.650245\n",
            "[29,    27] loss: 4.661631\n",
            "[30,    27] loss: 4.652512\n",
            "[31,    27] loss: 4.658696\n",
            "[32,    27] loss: 4.649871\n",
            "[33,    27] loss: 4.671313\n",
            "[34,    27] loss: 4.661053\n",
            "[35,    27] loss: 4.664028\n",
            "[36,    27] loss: 4.685240\n",
            "[37,    27] loss: 4.666446\n",
            "[38,    27] loss: 4.674079\n",
            "[39,    27] loss: 4.678272\n",
            "[40,    27] loss: 4.669125\n",
            "[41,    27] loss: 4.664265\n",
            "[42,    27] loss: 4.662970\n",
            "[43,    27] loss: 4.671929\n",
            "[44,    27] loss: 4.674254\n",
            "[45,    27] loss: 4.673538\n",
            "[46,    27] loss: 4.678293\n",
            "[47,    27] loss: 4.664513\n",
            "[48,    27] loss: 4.652522\n",
            "[49,    27] loss: 4.668620\n",
            "[50,    27] loss: 4.656968\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.1449314534664154\n",
            "Test Accuracy -  0.625\n",
            "L2 values -  [0.12222350165247917, 0.13278350979089737, 0.1425493150949478]\n",
            "L2 values -  [0.12222350165247917, 0.14689913690090178, 0.1449314534664154]\n",
            "Sparsity in fc1.weight: 64.87%\n",
            "Sparsity in fc2.weight: 66.46%\n",
            "Sparsity in fc3.weight: 55.76%\n",
            "Sparsity in fc4.weight: 57.03%\n",
            "Sparsity in fc5.weight: 47.66%\n",
            "Global sparsity: 63.04%\n",
            "Avg Loss during Testing -  0.12350205928087235\n",
            "Test Accuracy -  3.75\n",
            "[1,    27] loss: 4.812375\n",
            "[2,    27] loss: 4.727670\n",
            "[3,    27] loss: 4.699126\n",
            "[4,    27] loss: 4.726238\n",
            "[5,    27] loss: 4.714574\n",
            "[6,    27] loss: 4.674950\n",
            "[7,    27] loss: 4.675997\n",
            "[8,    27] loss: 4.686972\n",
            "[9,    27] loss: 4.664274\n",
            "[10,    27] loss: 4.676752\n",
            "[11,    27] loss: 4.684906\n",
            "[12,    27] loss: 4.717673\n",
            "[13,    27] loss: 4.695073\n",
            "[14,    27] loss: 4.717275\n",
            "[15,    27] loss: 4.765070\n",
            "[16,    27] loss: 4.718272\n",
            "[17,    27] loss: 4.714982\n",
            "[18,    27] loss: 4.710581\n",
            "[19,    27] loss: 4.701852\n",
            "[20,    27] loss: 4.722338\n",
            "[21,    27] loss: 4.754420\n",
            "[22,    27] loss: 4.709176\n",
            "[23,    27] loss: 4.712272\n",
            "[24,    27] loss: 4.719765\n",
            "[25,    27] loss: 4.708003\n",
            "[26,    27] loss: 4.696072\n",
            "[27,    27] loss: 4.713777\n",
            "[28,    27] loss: 4.719785\n",
            "[29,    27] loss: 4.716492\n",
            "[30,    27] loss: 4.703858\n",
            "[31,    27] loss: 4.721051\n",
            "[32,    27] loss: 4.720880\n",
            "[33,    27] loss: 4.695017\n",
            "[34,    27] loss: 4.730515\n",
            "[35,    27] loss: 4.746596\n",
            "[36,    27] loss: 4.692952\n",
            "[37,    27] loss: 4.718590\n",
            "[38,    27] loss: 4.682242\n",
            "[39,    27] loss: 4.723809\n",
            "[40,    27] loss: 4.715200\n",
            "[41,    27] loss: 4.708149\n",
            "[42,    27] loss: 4.698076\n",
            "[43,    27] loss: 4.704022\n",
            "[44,    27] loss: 4.709270\n",
            "[45,    27] loss: 4.681964\n",
            "[46,    27] loss: 4.702048\n",
            "[47,    27] loss: 4.709584\n",
            "[48,    27] loss: 4.726580\n",
            "[49,    27] loss: 4.717103\n",
            "[50,    27] loss: 4.717998\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.14613125324249268\n",
            "Test Accuracy -  0.625\n",
            "L1 values -  [0.12222350165247917, 0.12220958024263381, 0.12239801809191704, 0.12350205928087235]\n",
            "L1 values -  [0.12222350165247917, 0.14660850614309312, 0.14563148617744445, 0.14613125324249268]\n",
            "Sparsity in fc1.weight: 60.28%\n",
            "Sparsity in fc2.weight: 59.62%\n",
            "Sparsity in fc3.weight: 59.96%\n",
            "Sparsity in fc4.weight: 58.59%\n",
            "Sparsity in fc5.weight: 59.38%\n",
            "Global sparsity: 59.93%\n",
            "Avg Loss during Testing -  0.14497094452381135\n",
            "Test Accuracy -  1.5625\n",
            "[1,    27] loss: 4.702165\n",
            "[2,    27] loss: 4.653519\n",
            "[3,    27] loss: 4.657260\n",
            "[4,    27] loss: 4.645854\n",
            "[5,    27] loss: 4.660234\n",
            "[6,    27] loss: 4.638584\n",
            "[7,    27] loss: 4.648863\n",
            "[8,    27] loss: 4.660296\n",
            "[9,    27] loss: 4.656302\n",
            "[10,    27] loss: 4.641516\n",
            "[11,    27] loss: 4.661518\n",
            "[12,    27] loss: 4.660185\n",
            "[13,    27] loss: 4.651232\n",
            "[14,    27] loss: 4.657243\n",
            "[15,    27] loss: 4.650389\n",
            "[16,    27] loss: 4.653616\n",
            "[17,    27] loss: 4.644589\n",
            "[18,    27] loss: 4.645654\n",
            "[19,    27] loss: 4.674599\n",
            "[20,    27] loss: 4.654858\n",
            "[21,    27] loss: 4.656664\n",
            "[22,    27] loss: 4.659259\n",
            "[23,    27] loss: 4.651917\n",
            "[24,    27] loss: 4.655715\n",
            "[25,    27] loss: 4.653582\n",
            "[26,    27] loss: 4.652496\n",
            "[27,    27] loss: 4.650330\n",
            "[28,    27] loss: 4.675755\n",
            "[29,    27] loss: 4.657179\n",
            "[30,    27] loss: 4.663956\n",
            "[31,    27] loss: 4.655056\n",
            "[32,    27] loss: 4.634076\n",
            "[33,    27] loss: 4.651682\n",
            "[34,    27] loss: 4.637867\n",
            "[35,    27] loss: 4.645983\n",
            "[36,    27] loss: 4.653445\n",
            "[37,    27] loss: 4.655050\n",
            "[38,    27] loss: 4.647804\n",
            "[39,    27] loss: 4.656655\n",
            "[40,    27] loss: 4.657379\n",
            "[41,    27] loss: 4.665083\n",
            "[42,    27] loss: 4.649359\n",
            "[43,    27] loss: 4.665521\n",
            "[44,    27] loss: 4.646732\n",
            "[45,    27] loss: 4.653651\n",
            "[46,    27] loss: 4.644313\n",
            "[47,    27] loss: 4.659605\n",
            "[48,    27] loss: 4.639597\n",
            "[49,    27] loss: 4.648430\n",
            "[50,    27] loss: 4.650119\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.14417732805013656\n",
            "Test Accuracy -  0.0\n",
            "L2 values -  [0.12222350165247917, 0.13278350979089737, 0.1425493150949478, 0.14497094452381135]\n",
            "L2 values -  [0.12222350165247917, 0.14689913690090178, 0.1449314534664154, 0.14417732805013656]\n",
            "Sparsity in fc1.weight: 84.81%\n",
            "Sparsity in fc2.weight: 87.16%\n",
            "Sparsity in fc3.weight: 76.37%\n",
            "Sparsity in fc4.weight: 83.20%\n",
            "Sparsity in fc5.weight: 68.75%\n",
            "Global sparsity: 83.71%\n",
            "Avg Loss during Testing -  0.14508503079414367\n",
            "Test Accuracy -  1.25\n",
            "[1,    27] loss: 4.862026\n",
            "[2,    27] loss: 4.691314\n",
            "[3,    27] loss: 4.697062\n",
            "[4,    27] loss: 4.699403\n",
            "[5,    27] loss: 4.662358\n",
            "[6,    27] loss: 4.689540\n",
            "[7,    27] loss: 4.685445\n",
            "[8,    27] loss: 4.688689\n",
            "[9,    27] loss: 4.684633\n",
            "[10,    27] loss: 4.696206\n",
            "[11,    27] loss: 4.663887\n",
            "[12,    27] loss: 4.694842\n",
            "[13,    27] loss: 4.684173\n",
            "[14,    27] loss: 4.683911\n",
            "[15,    27] loss: 4.683725\n",
            "[16,    27] loss: 4.663149\n",
            "[17,    27] loss: 4.672388\n",
            "[18,    27] loss: 4.679981\n",
            "[19,    27] loss: 4.678066\n",
            "[20,    27] loss: 4.680830\n",
            "[21,    27] loss: 4.673644\n",
            "[22,    27] loss: 4.669794\n",
            "[23,    27] loss: 4.663980\n",
            "[24,    27] loss: 4.659678\n",
            "[25,    27] loss: 4.666624\n",
            "[26,    27] loss: 4.670935\n",
            "[27,    27] loss: 4.681929\n",
            "[28,    27] loss: 4.674497\n",
            "[29,    27] loss: 4.688545\n",
            "[30,    27] loss: 4.733829\n",
            "[31,    27] loss: 4.674367\n",
            "[32,    27] loss: 4.667173\n",
            "[33,    27] loss: 4.687535\n",
            "[34,    27] loss: 4.697086\n",
            "[35,    27] loss: 4.694622\n",
            "[36,    27] loss: 4.687201\n",
            "[37,    27] loss: 4.688250\n",
            "[38,    27] loss: 4.659971\n",
            "[39,    27] loss: 4.675784\n",
            "[40,    27] loss: 4.674248\n",
            "[41,    27] loss: 4.669322\n",
            "[42,    27] loss: 4.660635\n",
            "[43,    27] loss: 4.677836\n",
            "[44,    27] loss: 4.691477\n",
            "[45,    27] loss: 4.685072\n",
            "[46,    27] loss: 4.674793\n",
            "[47,    27] loss: 4.670945\n",
            "[48,    27] loss: 4.677397\n",
            "[49,    27] loss: 4.678665\n",
            "[50,    27] loss: 4.696560\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.14722002297639847\n",
            "Test Accuracy -  0.9375\n",
            "L1 values -  [0.12222350165247917, 0.12220958024263381, 0.12239801809191704, 0.12350205928087235, 0.14508503079414367]\n",
            "L1 values -  [0.12222350165247917, 0.14660850614309312, 0.14563148617744445, 0.14613125324249268, 0.14722002297639847]\n",
            "Sparsity in fc1.weight: 79.71%\n",
            "Sparsity in fc2.weight: 80.47%\n",
            "Sparsity in fc3.weight: 80.08%\n",
            "Sparsity in fc4.weight: 80.27%\n",
            "Sparsity in fc5.weight: 83.20%\n",
            "Global sparsity: 80.10%\n",
            "Avg Loss during Testing -  0.14513591527938843\n",
            "Test Accuracy -  2.1875\n",
            "[1,    27] loss: 4.691181\n",
            "[2,    27] loss: 4.645167\n",
            "[3,    27] loss: 4.652541\n",
            "[4,    27] loss: 4.643543\n",
            "[5,    27] loss: 4.644702\n",
            "[6,    27] loss: 4.651740\n",
            "[7,    27] loss: 4.645728\n",
            "[8,    27] loss: 4.642415\n",
            "[9,    27] loss: 4.645695\n",
            "[10,    27] loss: 4.655970\n",
            "[11,    27] loss: 4.652596\n",
            "[12,    27] loss: 4.642947\n",
            "[13,    27] loss: 4.649257\n",
            "[14,    27] loss: 4.642955\n",
            "[15,    27] loss: 4.650166\n",
            "[16,    27] loss: 4.652383\n",
            "[17,    27] loss: 4.643121\n",
            "[18,    27] loss: 4.641131\n",
            "[19,    27] loss: 4.653952\n",
            "[20,    27] loss: 4.657822\n",
            "[21,    27] loss: 4.656373\n",
            "[22,    27] loss: 4.643980\n",
            "[23,    27] loss: 4.643157\n",
            "[24,    27] loss: 4.640923\n",
            "[25,    27] loss: 4.642343\n",
            "[26,    27] loss: 4.647018\n",
            "[27,    27] loss: 4.656426\n",
            "[28,    27] loss: 4.650658\n",
            "[29,    27] loss: 4.640514\n",
            "[30,    27] loss: 4.647768\n",
            "[31,    27] loss: 4.648059\n",
            "[32,    27] loss: 4.645273\n",
            "[33,    27] loss: 4.649465\n",
            "[34,    27] loss: 4.660848\n",
            "[35,    27] loss: 4.650743\n",
            "[36,    27] loss: 4.655601\n",
            "[37,    27] loss: 4.651290\n",
            "[38,    27] loss: 4.652799\n",
            "[39,    27] loss: 4.645972\n",
            "[40,    27] loss: 4.663557\n",
            "[41,    27] loss: 4.655992\n",
            "[42,    27] loss: 4.647834\n",
            "[43,    27] loss: 4.659188\n",
            "[44,    27] loss: 4.644481\n",
            "[45,    27] loss: 4.658752\n",
            "[46,    27] loss: 4.656408\n",
            "[47,    27] loss: 4.641120\n",
            "[48,    27] loss: 4.647844\n",
            "[49,    27] loss: 4.649469\n",
            "[50,    27] loss: 4.658203\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.1438053220510483\n",
            "Test Accuracy -  2.5\n",
            "L2 values -  [0.12222350165247917, 0.13278350979089737, 0.1425493150949478, 0.14497094452381135, 0.14513591527938843]\n",
            "L2 values -  [0.12222350165247917, 0.14689913690090178, 0.1449314534664154, 0.14417732805013656, 0.1438053220510483]\n",
            "Sparsity in fc1.weight: 93.80%\n",
            "Sparsity in fc2.weight: 96.04%\n",
            "Sparsity in fc3.weight: 90.92%\n",
            "Sparsity in fc4.weight: 94.73%\n",
            "Sparsity in fc5.weight: 83.20%\n",
            "Global sparsity: 93.72%\n",
            "Avg Loss during Testing -  0.1502710148692131\n",
            "Test Accuracy -  1.25\n",
            "[1,    27] loss: 4.792039\n",
            "[2,    27] loss: 4.762392\n",
            "[3,    27] loss: 4.701168\n",
            "[4,    27] loss: 4.671809\n",
            "[5,    27] loss: 4.654570\n",
            "[6,    27] loss: 4.668284\n",
            "[7,    27] loss: 4.649156\n",
            "[8,    27] loss: 4.673659\n",
            "[9,    27] loss: 4.670925\n",
            "[10,    27] loss: 4.653348\n",
            "[11,    27] loss: 4.653093\n",
            "[12,    27] loss: 4.663510\n",
            "[13,    27] loss: 4.654985\n",
            "[14,    27] loss: 4.663836\n",
            "[15,    27] loss: 4.666276\n",
            "[16,    27] loss: 4.667378\n",
            "[17,    27] loss: 4.661181\n",
            "[18,    27] loss: 4.668189\n",
            "[19,    27] loss: 4.669474\n",
            "[20,    27] loss: 4.679644\n",
            "[21,    27] loss: 4.658516\n",
            "[22,    27] loss: 4.700016\n",
            "[23,    27] loss: 4.681543\n",
            "[24,    27] loss: 4.664328\n",
            "[25,    27] loss: 4.673875\n",
            "[26,    27] loss: 4.682050\n",
            "[27,    27] loss: 4.706149\n",
            "[28,    27] loss: 4.657002\n",
            "[29,    27] loss: 4.676677\n",
            "[30,    27] loss: 4.652519\n",
            "[31,    27] loss: 4.662870\n",
            "[32,    27] loss: 4.682778\n",
            "[33,    27] loss: 4.688378\n",
            "[34,    27] loss: 4.674733\n",
            "[35,    27] loss: 4.665818\n",
            "[36,    27] loss: 4.683619\n",
            "[37,    27] loss: 4.676671\n",
            "[38,    27] loss: 4.703785\n",
            "[39,    27] loss: 4.662168\n",
            "[40,    27] loss: 4.673796\n",
            "[41,    27] loss: 4.697807\n",
            "[42,    27] loss: 4.706579\n",
            "[43,    27] loss: 4.682117\n",
            "[44,    27] loss: 4.667211\n",
            "[45,    27] loss: 4.649881\n",
            "[46,    27] loss: 4.674569\n",
            "[47,    27] loss: 4.684795\n",
            "[48,    27] loss: 4.650291\n",
            "[49,    27] loss: 4.684462\n",
            "[50,    27] loss: 4.672424\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.1473870635032654\n",
            "Test Accuracy -  1.5625\n",
            "L1 values -  [0.12222350165247917, 0.12220958024263381, 0.12239801809191704, 0.12350205928087235, 0.14508503079414367, 0.1502710148692131]\n",
            "L1 values -  [0.12222350165247917, 0.14660850614309312, 0.14563148617744445, 0.14613125324249268, 0.14722002297639847, 0.1473870635032654]\n",
            "Sparsity in fc1.weight: 89.99%\n",
            "Sparsity in fc2.weight: 90.72%\n",
            "Sparsity in fc3.weight: 90.62%\n",
            "Sparsity in fc4.weight: 88.48%\n",
            "Sparsity in fc5.weight: 90.23%\n",
            "Global sparsity: 90.17%\n",
            "Avg Loss during Testing -  0.14485737681388855\n",
            "Test Accuracy -  0.625\n",
            "[1,    27] loss: 4.689298\n",
            "[2,    27] loss: 4.649041\n",
            "[3,    27] loss: 4.643307\n",
            "[4,    27] loss: 4.637767\n",
            "[5,    27] loss: 4.643957\n",
            "[6,    27] loss: 4.649332\n",
            "[7,    27] loss: 4.631337\n",
            "[8,    27] loss: 4.648169\n",
            "[9,    27] loss: 4.640899\n",
            "[10,    27] loss: 4.649723\n",
            "[11,    27] loss: 4.650399\n",
            "[12,    27] loss: 4.647317\n",
            "[13,    27] loss: 4.643440\n",
            "[14,    27] loss: 4.646320\n",
            "[15,    27] loss: 4.651532\n",
            "[16,    27] loss: 4.642845\n",
            "[17,    27] loss: 4.649042\n",
            "[18,    27] loss: 4.643389\n",
            "[19,    27] loss: 4.650519\n",
            "[20,    27] loss: 4.650134\n",
            "[21,    27] loss: 4.645541\n",
            "[22,    27] loss: 4.648473\n",
            "[23,    27] loss: 4.643175\n",
            "[24,    27] loss: 4.642739\n",
            "[25,    27] loss: 4.642564\n",
            "[26,    27] loss: 4.649113\n",
            "[27,    27] loss: 4.652817\n",
            "[28,    27] loss: 4.646777\n",
            "[29,    27] loss: 4.644532\n",
            "[30,    27] loss: 4.646309\n",
            "[31,    27] loss: 4.650755\n",
            "[32,    27] loss: 4.652086\n",
            "[33,    27] loss: 4.643446\n",
            "[34,    27] loss: 4.645989\n",
            "[35,    27] loss: 4.647724\n",
            "[36,    27] loss: 4.646691\n",
            "[37,    27] loss: 4.647016\n",
            "[38,    27] loss: 4.635026\n",
            "[39,    27] loss: 4.649765\n",
            "[40,    27] loss: 4.645354\n",
            "[41,    27] loss: 4.642391\n",
            "[42,    27] loss: 4.640673\n",
            "[43,    27] loss: 4.647650\n",
            "[44,    27] loss: 4.644131\n",
            "[45,    27] loss: 4.649692\n",
            "[46,    27] loss: 4.651553\n",
            "[47,    27] loss: 4.640394\n",
            "[48,    27] loss: 4.637725\n",
            "[49,    27] loss: 4.639965\n",
            "[50,    27] loss: 4.650602\n",
            "Finished Training\n",
            "Avg Loss during Testing -  0.14461893141269683\n",
            "Test Accuracy -  1.25\n",
            "L2 values -  [0.12222350165247917, 0.13278350979089737, 0.1425493150949478, 0.14497094452381135, 0.14513591527938843, 0.14485737681388855]\n",
            "L2 values -  [0.12222350165247917, 0.14689913690090178, 0.1449314534664154, 0.14417732805013656, 0.1438053220510483, 0.14461893141269683]\n"
          ]
        }
      ],
      "source": [
        "# Full cycle\n",
        "\n",
        "parameters_to_prune = (\n",
        "    (net.fc1, 'weight'),\n",
        "    (net.fc2, 'weight'),\n",
        "    (net.fc3, 'weight'),\n",
        "    (net.fc4, 'weight'),\n",
        "    (net.fc5, 'weight'),\n",
        "    (net.fc6, 'weight'),\n",
        "    (net.fc7, 'weight'),\n",
        "    (net.fc8, 'weight'),\n",
        "    (net.fc9, 'weight'),\n",
        "    (net.fc10, 'weight'),\n",
        "    )\n",
        "\n",
        "batch_loss = 27\n",
        "epoch = 50\n",
        "\n",
        "results_base_l1 = []\n",
        "results_finetune_l1 = []\n",
        "\n",
        "results_base_l2 = []\n",
        "results_finetune_l2 = []\n",
        "\n",
        "# Network\n",
        "net = Net().to(device)\n",
        "print_sparsity(net)\n",
        "\n",
        "# criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "train(net, train_loader, epoch, optimizer, criterion, batch_loss)\n",
        "loss_mse, _ = test(net, test_loader, criterion)\n",
        "\n",
        "results_base_l1.append(loss_mse)\n",
        "results_finetune_l1.append(loss_mse)\n",
        "results_base_l2.append(loss_mse)\n",
        "results_finetune_l2.append(loss_mse)\n",
        "\n",
        "PATH = './cifar_net.pth'\n",
        "torch.save(net.state_dict(), PATH)\n",
        "\n",
        "prune_values = [0.2, 0.4, 0.6, 0.8, 0.9]\n",
        "pruning_methods = [0,1]\n",
        "\n",
        "\n",
        "for prune_value in prune_values:\n",
        "  for pruning_method in pruning_methods:\n",
        "  # Each prune value experiment is independent\n",
        "    net = Net().to(device)\n",
        "    net.load_state_dict(torch.load(PATH))\n",
        "    \n",
        "    parameters_to_prune = (\n",
        "    (net.fc1, 'weight'),\n",
        "    (net.fc2, 'weight'),\n",
        "    (net.fc3, 'weight'),\n",
        "    (net.fc4, 'weight'),\n",
        "    (net.fc5, 'weight'),\n",
        "    (net.fc6, 'weight'),\n",
        "    (net.fc7, 'weight'),\n",
        "    (net.fc8, 'weight'),\n",
        "    (net.fc9, 'weight'),\n",
        "    (net.fc10, 'weight'),\n",
        "    )\n",
        "    \n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.1)\n",
        "    \n",
        "    if pruning_method == 0:\n",
        "      prune.global_unstructured(\n",
        "          parameters_to_prune,\n",
        "          pruning_method=prune.L1Unstructured,\n",
        "          amount=prune_value,\n",
        "      )\n",
        "    else:\n",
        "      prune.global_unstructured(\n",
        "          parameters_to_prune,\n",
        "          pruning_method=prune.RandomUnstructured,\n",
        "          amount=prune_value,\n",
        "      )\n",
        "\n",
        "    print_sparsity(net)\n",
        "    \n",
        "    base_loss, _ = test(net, test_loader, criterion)\n",
        "\n",
        "    train(net, train_loader, epoch, optimizer, criterion, batch_loss)\n",
        "    finetune_loss, _ = test(net, test_loader, criterion)\n",
        "    \n",
        "    if pruning_method == 0:\n",
        "      results_base_l1.append(base_loss)\n",
        "      results_finetune_l1.append(finetune_loss)\n",
        "\n",
        "      print('L1 values - ', results_base_l1)\n",
        "      print('L1 values - ', results_finetune_l1)\n",
        "    else:\n",
        "      results_base_l2.append(base_loss)\n",
        "      results_finetune_l2.append(finetune_loss)\n",
        "\n",
        "      print('L2 values - ', results_base_l2)\n",
        "      print('L2 values - ', results_finetune_l2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OX9TdJXfot2w",
        "outputId": "4c085bf5-0aeb-4eb3-a3bc-b5b2894334d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.12222350165247917,\n",
              " 0.12220958024263381,\n",
              " 0.12239801809191704,\n",
              " 0.12350205928087235,\n",
              " 0.14508503079414367,\n",
              " 0.1502710148692131]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "results_base_l1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXz5mcZUot2w",
        "outputId": "b649bfe1-61ad-4167-d835-688066468baf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.12222350165247917,\n",
              " 0.14660850614309312,\n",
              " 0.14563148617744445,\n",
              " 0.14613125324249268,\n",
              " 0.14722002297639847,\n",
              " 0.1473870635032654]"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "results_finetune_l1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEhS8001ot2x",
        "outputId": "d6388195-be57-4c64-9413-ba3cb1e8633c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.12222350165247917,\n",
              " 0.13278350979089737,\n",
              " 0.1425493150949478,\n",
              " 0.14497094452381135,\n",
              " 0.14513591527938843,\n",
              " 0.14485737681388855]"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "results_base_l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0874MtSHot2x",
        "outputId": "92e3714e-a307-4160-b933-1bb97fc2bb1e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.12222350165247917,\n",
              " 0.14689913690090178,\n",
              " 0.1449314534664154,\n",
              " 0.14417732805013656,\n",
              " 0.1438053220510483,\n",
              " 0.14461893141269683]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "results_finetune_l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShFTcOGGot2x"
      },
      "outputs": [],
      "source": [
        "x_coord = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "w-iMHwYwot2x",
        "outputId": "8b78b9d0-0b95-4895-88de-eb2522a9f7a6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbIAAAEWCAYAAAAD/hLkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd3xUVfr/389kkkx6gNCDJHRCCAFCaIKICljA1bUuKigslkXsbVfXsut39WddXRGVpliwr+haEBVBRClK72joJUB6MsmU8/vj3hkmIWUCGSblvF+vec3cc88957llzuee+ohSCo1Go9FoGiqWYBug0Wg0Gs2poIVMo9FoNA0aLWQajUajadBoIdNoNBpNg0YLmUaj0WgaNFrINBqNRtOgaXRCJiIzROShYNvRWBCRjSIyopr9i0Vk8mk0yZPvCBHZe7rzbaiIiBKRLn7GfURE3jR/nyEihSISYm63FpElIlIgIs+IwRwRyRGRFYE8h4ZAQyh/fO/vSRw7TES21rVNp0rAhExEskTk3AphE0Xkh0DlCaCUukkp9Y+6TldEkszCoND8HBKRz0TkvFqkEfDzr+t8lFK9lFKLzXRP+g9gHh8mIh+Yz4aqKJBmofikiBw1P0+KiJzaGQQe81zWi4jFJ+yfIjLX/O15dj6vcNybIvLI6bW2diildiulopVSLjNoCnAEiFVK3QWcCZwHJCqlMk+nbT7X1Xo6862O01D+BPVclVJLlVLdg2lDZTS6GtlpIF4pFQ30Ab4GPhaRicE1qUHxA3ANcLCSfVOAP2Bc2zRgLHDj6TOtemooRNoBV9WQxEARGVKHJgWDjsAmdXwlhY5AllKqqLYJBbtQrkh9s0dTC5RSAfkAWcC5FcImAj/4bPcEFgO5wEZgnM++xcDkyo4FBHgOOAzkA+uBVHPfXOCf5u8RwF7gLjPuAeB6nzRbAJ+aaawE/ulrXwXbkwAFWCuE3w0cAizm9v3ATqAA2ARc4nOudsAFFAK5ZviFwK+mDXuAR3zStgFvAkfNa7QSaG3uiwNmmee0z7Q9pKp8Kth8NrDeZ/trYKXP9lLgD773ERgDlAEOM921PvfpH8Ay85wXAgl+PB97gREVwn4EpvhsTwJ+quL4EcBen+2qrnsYcAzo7RO3FVAMtDS3LwLWmNf4RyCtwnN8H7AOKK14/804yoyz3bPfvB9zKzw79wHf+Rz3pu/9rpBmZ+Bb894fAd7CeInytetu06484F3A5rP/HvPZ2A/cYObfpYq8koHvzWv3NfAf4M2Kzz3Gf8thPgeFGC8Zvs/aoydzPYFBZrxcYK3vc1Hd8wXsNm0rND+DKzm3R4APzOtTAPwC9KnBnnLXitqVKbWJW5vyp9JzNe/tZiAH+Aro6HNML/N+HsMoo/7qc03eA94wr8lGIMOfZ4sT/3d9zWtaYMab73P+Eyuej++1BcKBp81zOwTMACLMfQnAZ+YzcQyjTLJUWZ7UVOCc7IcahAwIBXYAf8UobEaaF6O7zwNclZCNBlYD8Rii1hNoW8WD5AQeM/O7AKMAa2bun29+IoEUDCGprZB1MsN7mtuXY7ydW4ArgSIf2yq7sSOA3mb8NPOGekTkRowHPRJDpPpjNOkAfAy8AkRhFMwrgBuryqdCnhEYBVCCeV0OYYhhjLmvBGhR8T5i/AHerJDWYgwB6WYeuxh4wo/nozIhywMG+mxnAAVVHD+C8n+o6q77dOBJn7i3AZ/6/BEPAwPNazzBPOdwn/NfA3TA/JNVYosCumI8k5PNsMqELMa8zp7rWZ2QdcFosgsHWgJLgOcr/L9WmOfcHKMwu8ncN8a8p6nm8/E21QvZcuBZM6/hGP/DE4Ss4v+rsmetttcTaI8h1heY9+48c9vzkrGYKp6virZVcW6PYIjvZRjP+t3A70BoVfe34rWidmVKbeKeUvkDXIxRhvbEEOAHgR/NfTEYwnkXxgtxDOZ/y7wmdtOeEOBf+LwwUv2zNQLzf4dRbu8C7jDP7zLzWvsrZM8BC8w8YjDKun+Z+/6FIWyh5mcYIFXd50A3Lf5XRHI9H4wCxcMgIBrjoSxTSn2LocBX+5GuA+PEe2Cc3Gal1IFq4j6mlHIopT7HeJvpbnZe/xF4WClVrJTaBLx+Eue43/xuDqCUel8ptV8p5VZKvYvxll5l34FSarFSar0Zfx3wDnCWj+0tMG68Sym1WimVLyKtMR7C25VSRUqpwxgPRU1NW548SzDeAIdjiONajDfeoRj3ZbtS6mgtrsEcpdQ2M933gPRaHOtLNIaYecgDov3pJ6vhur8OXO2TzrXAPPP3FOAVpdTP5jV+HePNfJBP8i8opfaY51elCcBDwEMiElZFnBLgcQyRq+l8diilvlZKlSqlsjGE5qwK0V4wz/kYRiHgue5XYNyTDcpo8nukqnxE5AxgAPCQmdcSM62TpbbX8xrgc6XU5+a9+xpYhfF8ezjV52u1UuoDpZQD4zraqrHHHyotU2oTt47Kn5swCv7NSikn8H9Auoh0xKgVH1RKPaOUsiulCpRSP/sc+4N5zV0Y/4U+FdKu6tnyZRCGyDxvnt8HGOVKjZj/xSnAHUqpY0qpAtN+TxnmANpi1DAdyuibU1WlF2gh+4NSKt7zAW7x2dcO2KOUcvuE7cJ4Q6sWU/T+A7wEHBaRV0UktoroR82b7KEYo8BsifEWs8dnn+9vf/HYewxARK4TkTU+4p2KUfOpFBEZKCLfiUi2iORhPJye+PMwmgvmi8h+Efl/IhKK0S8RChzwyecVjJqZv3yP8XY13Py9GKOgPMvcrg2+/V2e63syFAK+9zEWKKzuAfZQ3XU3/8DFwAgR6YFR21lgHtoRuKvCC1cHjOfTg1/PhVlQ7aX6fr2ZQGsRGVvD+bQWkfkisk9E8jFqbxWfo6que7sKNu+qJqt2QI4q38dVXfyaqO317AhcXiH+mRiFmIdTfb68+Znlzd5q7PGHqsqU2sSti/KnI/Bvn+t2DKOFqj3GNd9ZzbEVr6mtQh+hP9e8HbCvwv/T32enJUZNdLWP/V+a4QBPYdQ2F4rIbyJyf3WJBXOwx36gg+9IL+AMjKYXMJqGIn32tfE9WCn1glKqP0aVvBtGn0BtyMao9if6hHWoZRoAl2A0pWw134ReA6ZiNM3FAxswHi4w3tor8jZGodpBKRWHUZ0WAPNN5FGlVAowBOMt6zqMB74Uo6/A86IQq5TqVU0+FakoZN9Ts5D5k+6psJHyb4Z9zLBq8eO6g/G2ew1GbewDpZTdDN8DPO77wqWUilRKveNzbG3O+28YzeWRle1USpUBj2L0+1RX0/w/M9/eSqlY03Z/R3AeoPyzfEYNcZuJSJSf8WuittdzDzCvQvwopdQTfuTl733xXguzvEnkeEtKZekUU03ZU0fUtvyp7Fz3YHQn+F67CKXUj+a+TnVnbqUcANpXaDHxfXbKleEi4nsdj2C0UPTysT1OGQPpMGuQdymlOgHjgDtF5JyqDAmmkHneku8VkVBzKPZYjDZjMNqtLxWRSDHmv0zyHCgiA8yaTCjGxbIDbmqBWaX+CHjEzKMHhkj4hfnGPBV4GHjAfNOLwnjgss0412PUDDwcAhIrND3FAMeUUnYRyQT+5JPH2SLS22yGyMeobruV0Yy6EHhGRGJFxCIinUXkrGryqciPGM0hmcAKpdRGjDe8gRj9MZVxCEiq8PJRK0QkXERs5maYiNh8/ghvYDyw7UWkHUb7/lw/kq3puoNRo7kEQxDe8Al/DbjJfJ5ERKJE5EIRiTmZ81PGVIUNGH1DVTEPo3lrTDVxYjBqqHki0p7avai9B0wUkRQRicR4RquydxdGU96jYkyPOBPjf3iy1PZ6vgmMFZHRIhJiPg8jRCSxivi+ZGP872sqsPuLyKVmjeN2jJfAn6qJvwb4k2nPGE5s0j1lTqL8qexcZwAPiEgvABGJE5HLzX2fAW1F5HbzPxcjIgPr+DSWY4jxNLMMv5Ty3ShrgV4ikm7+5x/x7DDLy9eA50SklWl/exEZbf6+SES6mGVDHsaAoirL+KAJmflmOhY4H0OdpwPXKaW2mFGewxgddQjjbfotn8NjMS5CDkZV9ihGVbS2TMUY/XcQo3B5B+Mhr45cESnCGCl5AXC5Umq2eU6bgGcwbvAhjEEcy3yO/RajhnFQRI6YYbcAj4lIAfB3jELIQxuMEVf5GB2u33O8b+c6jM7WTRjX4QOON8dUlk85zKakX4CN5r3AtHuXMvrcKuN98/uoiPxSRZya2IrxJtYeo9m0BENAwWge/RTj2m4A/meGVYsf1x2l1B6M81UYI6A84auAP2M0VedgNGdMPMlz8/AgZp9pFfa6MO51lXEwam39MP7E/8Mo9PxCKfUF8DzGc7DD/K6OP2G8wBzDEL03qo9ebd61up7mfbkYoxabjVGTuAc/yialVDFGn+Mys3lqUBVRP8EYAJSDUSO/1Owvq4rbMMqmXGA88N+abDlJ/C5/KjtXpdTHwJMYXQ/5GP+Z8834BRgDZ8aa6W/HGK1cZ5jlxqUY9/cYxjX+yGf/NoyBLovM/CvObb0P4/n4ybR/Ecf7Grua24UY/+vpSqnvqrJF/Oh+aDKIyJNAG6VUdW/TmgaKiMwG9iulHgy2LZrTgxgTzrsopa4Jti010RjKHzEWAdh7uv9jTXoCoFmdD8OoAQzAaL487cstaQKPiCRhvD32Da4lGo2BLn/qjiYtZBj9EO9gjL45hNE89UlQLdLUOSLyD4y5Lv9SSv0ebHs0GhNd/tQRumlRo9FoNA0avdaiRqPRaBo0TaJpMSEhQSUlJQXbDI1Go2lQrF69+ohSqmXNMYNLkxCypKQkVq1aFWwzNBqNpkEhIqeyystpQzctajQajaZBo4VMo9FoNA0aLWQajUajadBoIdNoNBpNg0YLmUaj0WgaNFrINBqNRtOg0UKm0Wg0mgZNQIVMRMaIyFYR2VGZh08RGS4iv4iIU0Quq7DPJYbH3zUissAnPFlEfjbTfLcGn1sajUbTJDlSWMpjn27C7nAF25SAEzAhM51BvoThHycFuFpEUipE243hy+btSpIoUUqlm59xPuFPAs8ppbpg+BeaVMmxGo1G02QpsDuYOGcFb6/YxY7DhcE2J+AEskaWCexQSv1mOmCbj+FAz4tSKksptQ4/vTub3kJHYjiRBMPh5h/qzmSNRqNp2NgdLia/vootBwp4+Zr+pLaPC7ZJASeQQtYew9urh71mmL/YRGSViPwkIh6xagHkKqWcNaUpIlPM41dlZ2fX1naNRqNpcDhdbqa+/Ssrso7xzBV9OLt7q2CbdFqoz2stdlRK7RORTsC3IrIew+27XyilXgVeBcjIyNC+ajQaTaPG7Vbc++E6Fm0+xD8u7sXF6bWpNzRsAlkj2wd08NlONMP8Qim1z/z+DViM4dn3KBAvIh4BrlWaGo1G0xhRSvHP/23mo1/2ced53bh2cFKwTTqtBFLIVgJdzVGGYcBVwIIajgFARJqJSLj5OwEYCmxShhfQ7wDPCMcJaI+qGo2mifOfb3cwe9nvXD80iVtHdgm2OaedgAmZ2Y81FfgK2Ay8p5TaKCKPicg4ABEZICJ7gcuBV0Rko3l4T2CViKzFEK4nlFKbzH33AXeKyA6MPrNZgToHjUajqe/M+2kXz3y9jUv7tuehC1MwxsQ1LcSo5DRuMjIylPZHptFoGhufrNnH7e+u4ZwerXj5mv6EhtRt3UREViulMuo00QCgV/bQaDSaBsh3Ww9z13trGZDUnP/8qV+di1hDoumeuUaj0TRQVmUd4+Y3V9O9TQwzJ2RgCw0JtklBRQuZRqPRNCA2H8jnhrkraRcXwes3ZBJrCw22SUFHC5lGo9E0EHYdLeLaWSuICrfyxqRMEqLDg21SvUALmUaj0TQADuXbuWbWz7jcbuZNyiSxWWSwTao3aCHTaDSaek5ucRnXzVrBscIy5l6fSZdWMcE2qV5Rn5eo0mg0miZPcZmTG+au5PcjRcy5fgB9OsQH26R6h66RaTQaTT2lzOnmxnmrWbMnlxeu7svQLgnBNqleomtkGo1GUw9xuRV3vLeGpduP8P/+mMaY1DbBNqneomtkGo1GU89QSvHQJxv437oD/PWCHlwxoEPNBzVhtJBpNBpNPePphVt5++fd3DyiM1OGdw62OfUe3bSo0Wg09YiZS3/jpe92cnXmGdw7urt/BykFLgc4S8BhN76dpeAogZY9INQWWKODjBYyjUajqWvcLnAUHxcTp72G71JwlrBx1yGcm/cyt7WV4SExyEd2H3EqH9crWJ5v5a7clr+shJbdTu/5n2a0kGk0Gk1tcLug8BDk7YP8veb3Psjba37vM/ZTe88ivYCuoWGElkYg2yOMmpTV59sWW3471AZW81Mxruc7tm2dX4L6hhYyjUaj8aAUFB2pWqDy90HBAXA7yx8XGgVx7SG2PXTtCTHtIDwGQiNqEBobhEawYm8xk97aSNd2zZk3eTBh4bporg36amn8QpWVsf/BB7GEh2NL7Y0ttRe2bt2QUL1gqaaBoBTYc6sWqLy9kL8fXKXljwsJMwQqLhE6Dj0uWHGJ5nd7sMXDSTq0XLc3l+vnr6N9Qjyzrx9IlBaxWqOvmMYvCr5bTP6CT7FERpL7/gcASFgY4T16EJGaii01lYjeqYR16oSENG2XEpogUVZUfXNf/j4oKyx/jIRATFtDjNr1hZ4XQWyisR2XaPyOSjhpkaqJHYcLmThnJc2iwpg3aSDxkWEByaexo4VM4xe577+HNS6cLvdk4HDEYM92U7K3EHtWNnn//Zict98GQCIjsaX0JKJXKrbevYlI7UXoGWcgFj3TQ3MKOEsrqT1V2LbnnnhcdGuj1tSyG3QeeWJtKqYNWILz4rUvt4RrZ/2MRYQ3Jw2kdWzjHlkYSLSQaWrEceAARct+JCElH/n9W8JKjhHmdhIbCaSA6gll+VbsRc0oyXNhP7iJnDW/opzGKCpLpA1bj85EpPbG1jeTiLQ0rO3aIQF6y9U0MFxOo9+pyua+fVCUfeJxEc3NmlMHOGPQic19Me3AWj9rOEcLS7l21s8Uljp5d8pgkhKigm1Sg0YLmaZGcj/6CJQiLjUa7twAYoGSY0bhU3AQKThAeMFBwvP3E1dwEAoOoPKKKN1/jJKjVuzHQrH/ns/RXzfAG/MBCIkQbO2jiUhqha1bEhG9emLt0NV4Q45pC5EtQNfiGj5uNxQdrmGE38ETh46Hxx4XpLZ9ygtUbCLEtoOwhunGpMDuYMKcFezLKeHNyQNJaRcbbJMaPFrINNWi3G7yPnifqNalhA2/HkLMRyYqwfi06V3pcQLYXE5sRdlewXMf3UPp1i2UbPsd+++HsO8t4MjOHbBoJ/AN1ggXtuZlRDR3YGvhxnZGM6wt2x4Xt8q+I5oFrP9CUwkuJ5QVQGkhlBYYfU6lBcanKLtCjWov5B8At6N8GlbbcVHqNMIQKY9AeZr+bI2zcLc7XPz5jVVsOVDAq9f1Z0BS82Cb1CjQQqaplqLly3EcOESrIUXQ95raHRxiNeawmPNYLEDEUIjwieIuLsa+YR32X36mZN1a7Fu2U7j+iLlXERqfjS3hMBHxP2GLycfW3EFIqM/8HKuteqHzfIc3Yf9NzlJDeMpMwSktNAUo3+e3ue39XUGkPOHOkurzsoSa9zwROgw8sbkvNhEimzfJlw+ny83Ut3/lp9+O8fyV6Yzs0TrYJjUatJBpqiX3/fcJCYfoMzOheXKdp2+JjCQycxCRmYO8Ya6CAuwbN2LfsIGS9Ruwb9hAwY59gDF6LCyxDbbkVkQkxmJLCMEWZ8dSmg0H18O2heAoOjGjsOgKAleF6IVGnHjs6UYpYxWHciJSlQD5EcdV5l++1ghD8MOjjesVHms24UX7hMecGMfzOyoBolrpJuFKcLsV9324nkWbD/HouF78oW/7YJvUqNBCpqkSZ04OhYu+Ib5TIZbM609bviExMUQNGkTUoOPi5jx2DPvGjZSsX499w0aK168nf8la84AQwrt0wZY6hojevbF1S8bWNgaxHwGzz67c954VxnfF+UIAtrgKAtf2RMGLbn3iIAKljOHfVdVk/KntlBYcb7ZTLv8uVlh0BaGJhviOPkLjhwCFxxjfIbo4CARKKR7/fDMf/rKX28/tyoQhScE2qdGhn1xNleQvWIByOolPCYMeFwbVFmvz5kQPG0b0sGHeMMehw9g3bvCKW+E335L34UcASGgo4d27Y+udas5zO5/wzp0Qq/nIKwUlOZULnef7yFJjIELFVRwAIhMgIh7Kio+LkT9LEonFFJUKQhPd+kRx8QhMVWIUFq1rPw2Al77bwawffmfikCRuO6drsM1plGgh01SKUorcd9/F1sKB7eyrwBoebJNOILR1K0JbjyRm5EjAsNmxbz/2Deu9zZL5n35G7jvGSEmJiMDWsye21F5Gza1XKmFJPZDWKVVn4nZD8dHKxc6eayxNFF5RmKoRoNDIJtk/1FSZ99Munl64jUv6tufvF6XoKScBQpSq/cKWficuMgb4NxACzFRKPVFh/3DgeSANuEop9UGF/bHAJuC/SqmpZthioC3g6XUepZQ6XJ0dGRkZatWqVad+Qk2IkrVrybryKtoMyKXZs4uhpZ/uJOoZyu2mbNcu7Bs2HO9z27QJZbcDYImOxtar13FxS00ltH17XeBoTpkFa/dz2/xfGdm9FTOu7U9oSMOrPYvIaqVURrDtqImA1chEJAR4CTgP2AusFJEFSqlNPtF2AxOBu6tI5h/AkkrCxyultDIFkNz3P0CsEDukd4MVMQCxWAhPTiY8OZm4sWMBUE4npTt/M4Rtg9EsmfPGPI45jGHiIfHx2FJTfZolexPaulUwT0NzGnArNy63C4fbgUu5cLldOJUTp9v4eMI8+z1hFfc73U7W7TvGy9/toVdyK+68sAtH7YeJCYsh0hqpX5ICQCCbFjOBHUqp3wBEZD5wMUYNCwClVJa57wRHOiLSH2gNfAnU+zeCxoS7qIj8/31KbIdiQgafvkEepwuxWrF174atezfi/3gpYCyKbN+2vZy4HX31NXAZgy6sLVti6927XM3N2qxZME+jXnHMfozs4myjUFdOb4Hu+9tb6FcIc7gduNyucvudbmf5MJ/jTvYYb5iv6Kjj2+6q/HmdJOGJsAu46vPjYYIQHRpNdJjxiQmNMX6HRnvDY8Jijscxw2PCyscLCdKyWvWVQApZe2CPz/ZeYKA/B4qIBXgGuAY4t5Ioc0TEBXwI/FNV0j4qIlOAKQBnnHFG7Sxv4uR/+SXuklLiewikXBxsc04LEhZGRGovIlJ70YwrAXCXlGDfsgX7+g3moJINFH73nTFQBLC2akVIQgus8c0IaWZ+mjfD6vntDY/HGh+PhNXP5ZJOhmP2Y6w6uIqVB1ey8uBKdubtrPM8rBYroZZQQiSEEEsIVrESYgkpH2axYhUrVovVGxYWEkakNbJcWLl4lhBC5Hg6njDf/Z68yoWZaZTb7xtm/p61dBefrj3A9OtSiQh3UOgopLDM+BQ4CozfjkIKygoochSRXZzN747fvfudlQ0uqkCENeK4CNYgiCM6jCAuPK7O7099or4O9rgF+FwptbeSavh4pdQ+EYnBELJrgTcqRlJKvQq8CkYfWYDtbVTkvjufsFgnEef8scEuA1QXWCIiiOzbl8i+fb1hrsJC7Bs3Yd+wgdIdO3AdO4YzN4eyvXtx5eTgLiioOr3oaEKaNzeFzUf8PGLXrEJYXFy9WWw5157L6kOrWXFwBSsOrmBH7g7AKFD7terH2M5j6RjbsZx4VCkUZpjv74r7G2qNI9/u4Ktf93BhSj/O69S35gMqoJSi1FV6XPxMwfNse8SvoiDmleaxr3CfN57dZfemueAPC7SQnQL7gA4+24lmmD8MBoaJyC1ANBAmIoVKqfuVUvsAlFIFIvI2RhPmCUKmOTlKd+ygZN0GWqUXIf0nBNucekdIdDRRAzOJGphZ6X5VVoYrLw9nTg6unFxcOTm4cnNwHjt2fDsnB2d2Nvbt23Dl5KJKqlgtw2IhJC7uRLGLb3ZcECuInyUqqk76YPJK81h9aLW3xrUtZxsKRYQ1gvSW6VzY6UIyWmfQK6EXoRbtk87Duyv2UFTmYtKZnU7qeBHBZrVhs9pIiEio9fHuoiJKd+6keOsWCrdtpnT7DtpeFH1StjQkAilkK4GuIpKMIWBXAX/y50Cl1HjPbxGZCGQope4XESsQr5Q6IiKhwEXAojq3vAmT+8EHYIG4QV2gbVqwzWlwSFgY1pYtsbZs6fcx7pISXLmGyDmP5XjFzpWbU04QHbv3YF+7DmduLjgclScWGoo1Pt5s5mxeXvyqqP1ZbDYKygq8Na5VB1ex5dgWFIrwkHDSW6Xzl/S/kNk2k9QWqYSGaOGqDKfLzZxlvzMwuTm9EwNbA3Lb7ZTu3EnZjh2Ubt9O6fYdlO7YgWPf8bqChIcT3rkzklcILfx/HhsiARMypZRTRKYCX2EMv5+tlNooIo8Bq5RSC0RkAPAx0AwYKyKPKqV6VZNsOPCVKWIhGCL2WqDOoamhysrI+/gjYtqVYD2z8Q3yqK9YIiKwREQQ2ratX/GVUrgLC4/X7nxrf74CeCyH0i1bKc7JwZWX5+3bq0hZmIW8CDeFEdAx0kLnZs2JbdWLVu260K59d8LdLQkpbYY1Lxqx5KHi4rRn8Er4YsNB9ufZefTi1DpL011WRtnvv5tCtd377di9x3s/JTSUsORkItLTib/8MsK7diW8SxdCExObjJPbgM4jqy/oeWT+kf/lV+y7/XY6jCwi+rmNTXuh3UZGoT2ftb8tY8PO5ezM+pWjB7OILnYTb7fQSSXQwR1Py7JwoopcuHPzjP6+wsIq07PExp7Q12eJicZii8ASYUO837bKwyIisNg8+20NXhiVUvxh+o/kFZfx7V0jsFhq17yrHA7Kdu8uV7sq3b6dsl27vCNnCQkhLCnJK1ThXboQ3q0rYWeccXzFmjqmyc8j0zQ8ct+bjzXSTdR5Y7WINXCKHcWsObyGFQdXsPLQSjYe2YhLubBarKR1SWPAmaMZ0GYAfVr2wWat3DOxKivDmZtr1vaO+dT+ytcAHYcPYd+yBd0CZRwAACAASURBVHdREW67vepmz+qwWg1BizCFz2ZDPGLnG+YRvoiaBNJXSH22A1RD+WV3Dmv35PLYxb2qFTHlcuHYs8crVF7R+v3349fNYiGsQwfCunYhZvQoU7S6EpachKURjXytS7SQaQBw7N9P0fKfSEgpQjImBtscTS0pcZaw5vAa7+CMDUc24FROrGIlNSGVG1Jv8ApXZKh/I1ElLIzQVq0IbVW7yeDK4cBdWooqKcFtt+MuKUHZ7bhL7Ch7Ce4SO257Ccpean5Xsq/EjttuR5WU4Dp6DIc3vhHmLikxlg+rJRIaWrlA+oaFVyKUNYjnu59vJ1FKubRnc5Rpl2P/fkOsPKK1YwdlO39DlR5frDo0MZHwLl2IPmu4t6YV1qkTFlvlLxeaytFCpgEg96OPQUHcgHaQWO9bEpo8dqedtdlrvcK17sg6nG4nIRJCrxa9mNBrApltMklvle63cNUVEhpKSGgoRAdutJxSChwOUyhNEfSInEc8S0urFshKwpz5+eUF2Eyvqr5FX643P3s+/psREBJyvEkQsLZpQ3jXrkQNHGQIVtcuhHfqhCUqKjAXqImhhUyDcrnIfX8+Ua3thI24TS9qWw8pdZWyLnudV7jWZq/F4XZgEQspzVO4NuVaBrQeQL/W/YgKbfyFo4hAWBghYWGExAbOm7RSClVWVnXt0l7K+8u28/Pm/TwwMpkYcRr7ysoITWzvrWWFxOim+kCihUxD0fKfcB46QuszHZB2RbDN0QBlrjLWH1nvHQ6/Nnstpa5SLGKhR/MejO85ngFtBtCvVT+iwxr/PKFgISJIeDiEh1NZ71peiYNnFpcy6uJMkq5MP+32aQy0kGnIfW8+IeGK6PNGG27oNacdh8vBhqMbWHHAGJyx9vBa7C47gtCjeQ+u6H4FmW0y6de6H7FhgauBaGrHuyt3mxOg6957usZ/tJA1cZw5ORR88y3NOxdhyZwYbHOaDA63g41HNrLq0CpWHFjBmuw1lDiNFT66N+vOZd0uY0CbAfRv3b/RLy/UUHG63MxdlsWgTs1Jba/vUTDRQtbEyfvkE3C5ievbApLODLY5jRan28mmo5u8fVy/HP7FK1xd4rtwSZdLGNBmABmtM4i3xQfZWo0/eCZAP1aHE6A1J4cWsiaMUorc+W9ja1GG7bwb9SCPOsTldrHl2BZjHpcpXEWOIgA6x3Xm4s4XG8LVJoPmNt2c29BQSjFz6W8kJ0Qxsof2VRdstJA1Yexr11KWtYc2mXZI92sZTE0VuNwutuZs9da4Vh9aTaHDWBkjOS6ZC5MvZEBbo8Z1MovBauoXq3flsHZvHv+oYQK05vSghawJk/Pee4hVEXvuWRCt3yprg1KKbTnbvG5NVh9aTUGZ4cIlKTaJMcljyGyTSUbrDFpGNu4FW5siM5f+TlxEKH/snxhsUzRoIWuyuAqLyP/8f4YX6CE3BNucBoVbufnnT//k/W3vA9AhpgOjOo4io00GA1oPoHVU6yBbqAkku44W8dWmg9x8Vmciw3QRWh/Qd6GJUvDlFyh7GfFpMdDp7GCb02BQSvF/P/8f7297n+tSruPalGtpE9Um2GZpTiNzlmVhtQgThiQF2xSNSY3uZ0XkNhGJFYNZIvKLiIw6HcZpAkfu/LcIi3UQMeZaaKDeeE83SimeXPkk7259l+tTr+fujLu1iDUx8kocvLdqD2PT2tE6Vq+HWF/wx4/6DUqpfGAUht+wa4EnAmqVJqCUbt9OyYYtxHcuQfpdG2xzGgRKKZ5a9RRvbX6L61Ku445+d9SJJ2ZNw2L+it0Ul7m4QU+Arlf4I2Sef+sFwDyl1EafME0DJPf998GiiDt7IMS1D7Y59R6lFM+tfo55m+Yxvud47s64W4tYE8ThcjP3xywGd2qhJ0DXM/wRstUishBDyL4SkRig9v4TNPUCd1kZeR9/SEx7O9bhk4JtTr1HKcWLv77InI1zuLL7ldw34D4tYk2ULzYc5ECencnDdG2svuHPYI9JQDrwm1KqWESaY3gs0DRACr/9FldBMfFDw6Cr7uqsiZfXvsxr61/jsm6X8deBf9Ui1kTxTIDulBDF2d31VJX6hj81ssHAVqVUrohcAzwI5AXWLE2gyH3nLayRLqIuuBpCGrZ7+UAzY+0MXl77Mpd0uYSHBj2ERfz5u2gaI6t25bBubx7Xn5msJ0DXQ/z5Z74MFItIH+AuYCfwRkCt0gQEx759FK1YRXxyMZIxIdjm1Gtmrp/JS2teYlzncTwy5BEtYk2cmUt/Iz4ylD/2033K9RF//p1OpZQCLgb+o5R6CdBe4hoguR9+BAriz+oNzTsF25x6y9wNc/n3L//mguQLeGzIY1rEmji7jhaxcNMhxg88Q0+Arqf4c1cKROQBjGH3w0TEAug2qQaG4QX6HaLa2AkdOTnY5tRb5m2axzOrn2FM0hgeP/NxQvQcuyaPZwL0dYOTgm2Kpgr8edW8EijFmE92EEgEngqoVZo6p+jH5Tizc4jvYYEeFwXbnHrJ25vf5v+t/H+c1/E8/jXsX1gt+u27qZNXbE6A7qMnQNdnahQyU7zeAuJE5CLArpTSfWQNjNz5bxES7ib6osvAGh5sc+od7219j3+t+BcjO4zkyeFPahHTAPDOSmMCtPYAXb/xZ4mqK4AVwOXAFcDPInJZoA3T1B3OY8coWLyEuKRiLAP1AsEV+XDbh/zjp38wInEET5/1NKEW3XKuMSZAv/5jFkM6t6BXOz0Buj7jz2vn34ABSqnDACLSElgEfBBIwzR1h8cLdPyQLtCye7DNqVd8vP1jHl3+KGe2P5NnRjxDaB1OSXA4HOzduxe73V5naWpOH8VlTh47qxkJ0WFs3rw52OYEFJvNRmJiIqGhDfMlzh8hs3hEzOQo/vWtISJjgH8DIcBMpdQTFfYPB54H0oCrlFIfVNgfC2wC/quUmmqG9QfmAhHA58Bt5qhKTSUopch9Zx4RLcoIH/3nYJtTr/h056c8/OPDDGo7iOfPfp6wkLA6TX/v3r3ExMSQlJSkJ1I3MJRS7MguJNoN3VpHN+r7p5Ti6NGj7N27l+TkhtmE6o8gfSkiX4nIRBGZCPwPQ0CqRURCgJeA84EU4GoRSakQbTcwEXi7imT+ASypEPYy8Gegq/kZ48c5NFlK1qyhbPcB4ru7IeXiYJtTb/j8t895cNmDZLbJ5IWRLxAeUvf9hna7nRYtWjTqQrCxUlzmoqTMRUJ0WKO/fyJCixYtGnTLgT+DPe4BXsWoNaUBryql7vMj7Uxgh1LqN6VUGTAfYy6ab9pZSql1VLJ2o1nzag0s9AlrC8QqpX4ya2FvAH/ww5YmS+78txGrIuaCsRAWFWxz6gVfZn3JAz88QP/W/XnxnBexWQM3Gq2xF4KNleyCUkIsQrPIuq2l11ca+nPqVxOhUupDpdSd5udjP9NuD+zx2d5rhtWIOVftGeDuStLcezJpNkVchUXkf/kVsWcUEzJULxAM8PWur7l/yf2kt0znPyP/Q4Q1ItgmBYzo6OgTwpYsWUK/fv2wWq188MHJd3NfcMEF5Obmkpuby/Tp073hixcv5qKL6mZ6x+LFi/nxxx8r3VdaWsq5555Leno67777LpMnT2bTpk11kk+pw0W+3UGLqHC/l6M6lfw1p06VfWQiUgBU1vckgFJKxQbMKrgF+Fwptfdk3xREZAowBeCMM86oQ9MaDvmff44qddAsMxHa9gm2OUHn293fcu/395KakMr0c6cTGRoZbJNOO2eccQZz587l6aefPqV0Pv/c6F3Iyspi+vTp3HLLLXVhXjkWL15MdHQ0Q4YMOWHfr7/+CsCaNWsAuPLKK+ssnyNFZUZzW7T/tbGZM2eedP6aU6fKGplSKkYpFVvJJ8ZPEdsHdPDZTjTD/GEwMFVEsoCngetE5Anz+ER/0lRKvaqUylBKZbRs2dLPbBsXuW+/TlisA9uFujb2/Z7vuev7u0hpkcKMc2cQFdo0m1mTkpJIS0vDYqm6Meapp57ihRdeAOCOO+5g5MiRAHz77beMHz/em86RI0e4//772blzJ+np6dxzzz0AFBYWctlll9GjRw/Gjx+PZyzWN998Q9++fenduzc33HADpaWl5dICWLVqFSNGjCArK4sZM2bw3HPPkZ6eztKlS732HT58mGuuuYaVK1eSnp7Ozp07GTFiBKtWrQKMmujf/vY3+vTpw6BBgzh06BAA2dnZ/PGPf2TAgAEMGDCAZcuWnZDP4sXfc8uUSSz7+jNCQyze9MAQvBEjRlR6bv7kv3PnTgYNGkTv3r158MEHK60xa06OQM76XAl0FZFkDLG5CviTPwcqpcZ7fpsDTDKUUveb2/kiMgj4GbgOeLGO7W4U2Ldtw75lJ636O5HelwfbnKDyw74fuGPxHXRv1p2Xz3uZ6LDTX4A8+ulGNu3Pr9M0U9rF8vDYXnWaJsCwYcN45plnmDZtGqtWraK0tBSHw8HSpUsZPnx4ubhPPPEEGzZs8NaMFi9ezK+//srGjRtp164dQ4cOZdmyZWRkZDBx4kS++eYbunXrxnXXXcfLL7/M7bffXqkNSUlJ3HTTTURHR3P33eV7GFq1asXMmTN5+umn+eyzz044tqioiEGDBvH4449z77338tprr/Hggw9y2223cccdd3DmmWeye/duRo8ezebNm8vlc7jADuoVYm2VD0Ov7NzOPPNMv/O/7bbbuPrqq5kxY4bf90NTMwFbDVUp5QSmAl8Bm4H3lFIbReQxERkHICIDRGQvxmTrV0Rkox9J3wLMBHZgrMT/RUBOoIGT9947hhfoC84DWyBbges3P+7/kdu+vY0u8V145bxXiA1rutfCX/r378/q1avJz88nPDycwYMHs2rVKpYuXcqwYcNqPD4zM5PExEQsFgvp6elkZWWxdetWkpOT6datGwATJkxgyZKKA5LrhrCwMG8/Xf/+/cnKygJg0aJFTJ06lfT0dMaNG0d+fj6FhYXe49xKcbSwDGuIEGatvGis7Nz8zX/58uVcfrnxUvmnP/n1Tq/xk4Cuw6OU+pwKQ/WVUn/3+b2S8k2FlaUxF2PemGd7FZBal3Y2NtxlZeT997+mF+imO3fs5wM/M+3baSTFJfHqea8SFx681RkCUXMKFKGhoSQnJzN37lyGDBlCWloa3333HTt27KBnz541Hh8efnwqQ0hICE6ns9r4VqsVt9sYuFwXQ8BDQ0O9o/B883e73fz000/YbJWPUs0vceBwuYmyhXvtcbvdlJWVeeP4c25V5a8JHP4sUXWriDQ7HcZo6obCb77BVWgnvl8LSBwQbHOCwsqDK7n121vpENOB10a9RrwtPtgmNSiGDRvG008/zfDhwxk2bBgzZsygb9++JwzTjomJoaCgoMb0unfvTlZWFjt27ABg3rx5nHXWWYDRjLh69WoAPvzww1qn7S+jRo3ixReP90R4mkNjYmLIz88nu6CUcGsIXTsne+1ZsGABDoejTvIfNGiQ9/zmz59fJ2lqDPxpWmwNrBSR90RkjDT0CQdNgNy35mKNdBJ18SRogrfrl0O/8Jdv/kLbqLa8Nuo1mtuaB9ukoFBcXExiYqL38+yzz7Jy5UoSExN5//33ufHGG+nVq/Ka4rBhwzhw4ACDBw+mdevW2Gy2SpsVW7RowdChQ0lNTfUO9qgMm83GnDlzuPzyy+nduzcWi4WbbroJgIcffpjbbruNjIwMQkKOu80ZO3YsH3/88QmDPU6WF154gVWrVpGWlkZKSoq3n2rs2LF89PHHjB05hO1rVzBlyhS+//57+vTpw/Lly4mKqpuBQc8//zzPPvssaWlp7Nixg7g4vX5jXSH+rO5kitco4HogA3gPmKWU2hlY8+qGjIwM5RlR1Ngp27uPneeeS0LvIlq+vgYim1YhvubwGm78+kZaRbZizpg5JEQkBM2WzZs3+9UUpwk+WUeKKC5z0qNNrN9zx2pLcXExERERiAjz58/nnXfe4ZNPPglIXidDZc+riKxWSmUEySS/8auPTCmlROQgcBBwAs2AD0Tka6XUvYE0UFM78j54F1DEjx7e5ERsffZ6bl50MwkRCcwaPSuoIqZpOHgmQLeKsQVMxABWr17N1KlTUUoRHx/P7NmzA5ZXU6NGIROR2zCGuR/BGC14j1LKYa6+sR3QQlZPMLxAv0tUm1JCz5kSbHNOKxuPbuTGr28kPjyeWaNn0SqyVbBN0jQQTmYC9MkwbNgw1q5dG9A8mir+1MiaA5cqpXb5Biql3KajTU09oejHH3Eezaf16BhIqnmYdGNh89HNTFk4hdjwWGaPnk2bqDbBNknTQHC63OQUlREfEeqdAK1peNQoZEqph0Wkn4hcjLFk1TKl1C/mvsbtpKeBkfvWXELCXcRc0nQGeWw9tpUpX08hMjSSWaNn0Ta6bbBN0jQgjhWX4VaKhGjtNb0h48/w+4eA14EWQAIwR0QeDLRhmtrhPHqUgiXLiUsuRfpfG2xzTgvbc7bz54V/JiwkjNmjZtM+Wq8frfEfzwTo6HArEWEhNR+gqbf407R4DdBHKWUHMNc8XAP8M5CGaWpH3scfgVsRP7I/xLQOtjkB57fc35i8cDJWi5XZo2fTIbZDzQdpND7kmROg2zdrvB4Qmgr+NArvB3ynwofj/+K/mtNAOS/Q598cbHMCzu95vzNp4SQEYdboWXSM7Rhsk+olp+LG5YUXXqBnz56MHz+eBQsW8MQTT1QZtzoqunkJJL4L99aEUooj5gTomPCALnCkOQ34cwfzgI0i8jVGH9l5wAoReQFAKTUtgPZp/KDk1zWU7cum7Vnh0HlksM0JKLvzdzP5q8m4lZvZo2eTHNcwXbMHC3/duEyfPp1FixaRmGisIDdu3LiTys8jZIFw83IqFJW5KHG4aB8f0eCdSmr8q5F9DPwV+A5YDPwN+ARYbX40QSb3rblYrG5iL7kaLI23rX9PwR5u+OoGHG4HM0fNpHN852Cb1ODwx43LTTfdxG+//cb555/Pc889x9y5c5k6dSoAEydOZNq0aQwZMoROnTqVq9U99dRTDBgwgLS0NB5++GGAE9y8VHS8OXXqVObOneu17eGHH6Zfv3707t2bLVu2AMZq8jfccAOZmZn07dvXO4m4pKSEq666ip49e3LJJZdQUlLi93U4UlCKtQl5gG7s+DNq8XURCQO6mUFblVJ1s/iY5pRxFRaS//U3xJ1hxzJoYrDNCRj7Cvcx6atJ2F12Zo2aRddmXYNtUu344n44uL5u02zTG84/uSa/6pgxYwZffvkl3333HQkJCV6h8XDgwAF++OEHtmzZwrhx47jssstYuHAh27dvZ8WKFSilGDduHEuWLKnUzUt1JCQk8MsvvzB9+nSefvppZs6cyeOPP87IkSOZPXs2ubm5ZGZmcu655/LKK68QGRnJ5s2bWbduHf369fPr/E7XBGjN6cOfCdEjMEYtZmF4h+4gIhOUUoHxwaCpFfmffYYqcxF/Vi+Ib5wDHg4UHmDSV5ModBQyc9RMujfvHmyTmjR/+MMfsFgspKSkeJ1GLly4kIULF9K3b1/AcK65ffv2Wntnv/TSSwHD/clHH33kTXvBggXe5lC73c7u3btZsmQJ06YZPRtpaWmkpaX5lceRwtMzAVpz+vCnj+wZYJRSaiuAiHQD3gH6B9IwjX/kvjWH8DgHtrE3BduUgHCw6CCTFk4ivzSf10a9RkqLlGCbdHIEoOYULHxdmXjWalVK8cADD3DjjTeWi1vRX5evyxY40W2LJ21f9ydKKT788EO6dz/1Fxiny01OsZ4A3djw506GekQMQCm1DajcfarmtGLfug379t3E97Ag3c8Ptjl1zuHiw0xeOJlj9mPMOG8GvRIajk+vpsbo0aOZPXu211Hlvn37OHz48AmuWDp27MimTZsoLS0lNzeXb775xq+0X3zxRa9o/vrrrwAMHz6ct99+G4ANGzawbt26GtPyToCO0ROgGxP+1MhWi8hM4E1zezzQNJaSr+fkvvM6YlHEXvJHCGlc7xZHSo4weeFksouzeeW8V0hr6V+zkeY4HjcuHu68806GDRvGJZdcQk5ODp9++ikPP/wwGzf645i9ekaNGsXmzZsZPHgwYAz9f/PNN+ncubPXzcv555/PU089xRVXXEFqairJycnepsjqeOihh7j99ttJS0vD7XaTnJzMZ599xs0338z1119Pz5496dmzJ/37V99IVG4CdGjjHRTVFKnRjYuIhAN/Ac40g5YC05VSpQG2rc5ojG5c3GVl7BicSWTzXBLnfwMtGs8IvqMlR5n01ST2F+3n5XNfpn/rhtmKrd241C9yisvYc6yY5IQoYmyN68WvLmi0blxEJARYq5TqATx7ekzS+EPh11/jKiol/qJOjUrEcuw5TF44mX2F+5h+7vQGK2Ka+oVnArTNGkK0ngDd6Ki2j0wp5QK2ikjthh5pAk7uvJmERjqJurTxDPLIK83jzwv/zJ6CPbx4zosMaDMg2CZpGglFpcYE6BYxYXoCdCPEn1eTZhgre6wAijyBSqmTm+qvOWXK9u6laM0WEvoqJKVx3AaPiP2e9zsvjnyRQW0HBdskTSPiSGEpVouFZhF6yH1jxB8heyjgVmhqRd78NwFF/LgLIdRWY/z6Tn5ZPjd+fSM7cnfw/NnPM6T9kGCbpGlEeCdAx+oJ0I0Vf4TsAqXUfb4BIvIk8H1gTNJUh3K5yP3wA6LalhJ6TsNvViwsK+Tmr29ma85WnhvxHMMThwfbJE0j40hhqTEBOkrXxhor/swjO6+SsMY3aamBUPTDDzhziogf0A5aNewRcUWOIm5edDObjm7i6eFPM6LDiGCbpGlkGBOgHTTTE6AbNVXeWRG5WUTWA91FZJ3P53egjheN0/hL7huvGV6gL58SbFNOiWJHMbcsuoX1R9bz5PAnOafjOcE2qdEREhJCeno6qampjB07ltzc3DpJ13cR4dPJ3//+dxYtWgTA888/T3FxsXdfZS5rAI4V1W4CdFZWlneSdWXcc8899OrVi3vuuYcZM2bwxhtv1OIM/M/HH04l/0aHUqrSDxAHJGEsR9XR59O8qmPq66d///6qMeDIzlabevZUBy/ppFRpYbDNOWmKHcXq+i+vV2mvp6kvfvsi2OYEjE2bNgU1/6ioKO/v6667Tv3zn/+sk3TnzJmj/vKXv9RJWidLx44dVXZ2tnfb91w9uNxutWl/ntp5uMDvdL/77jt14YUXVrk/NjZWOZ3O2hl7EvkEg8qeV2CVqgdleE2fKmtkSqk8pVSWUupqYC/gwPBHFq2H4weHvA/fNbxAXzgSwqKCbc5JYXfaufXbW1l1cBWPn/k4Y5LHBNukJsHgwYPZt8/wh7tixQoGDx5M3759GTJkCFu3GivQzZ07l0svvZQxY8bQtWtX7r33Xu/xc+bMoVu3bmRmZrJs2TJveFZWFiNHjiQtLY1zzjmH3bt3A4a7l5tvvplBgwbRqVMnFi9ezA033EDPnj2ZOHHiCfatXLnSu2DwJ598QkREBGVlZdjtdjp16uRN84MPPuCFF15g//79nH322Zx99tneNP72t7/Rp08fBg0axKFDh8grdpCVlcX1l4+t1D5fFzSeGt3999/P0qVLSU9P57nnnitn47hx4ygsLKR///68++67PPLII96FjEeMGMF9991HZmYm3bp1Y+nSpQC4XC7uuecer3ubV155pdJ8KtZyL7roIq+ngOjo6BPODfAr/+LiYq644gpSUlK45JJLGDhwoN/ORxsS/qx+PxV4BDgEeFb7VECNawaJyBjg30AIMFMp9USF/cOB5820rlJKfWCGd8Twg2bBWNfxRaXUDHPfYqAt4HE+NEopdbgmWxo6Sily579FREIZ4efXLyeF/lLqKuX2725nxYEV/GPoP7io00U1H9RIeHLFk2w5tqVO0+zRvAf3Zd5XYzyXy8U333zDpEmTjON69GDp0qVYrVYWLVrEX//6Vz788EMA1qxZw6+//kp4eDjdu3fn1ltvxWq18vDDD7N69Wri4uI4++yzvUtL3XrrrUyYMIEJEyYwe/Zspk2bxn//+18AcnJyWL58OQsWLGDcuHEsW7aMmTNnMmDAANasWUN6errXxr59+3pdvSxdupTU1FRWrlyJ0+lk4MCB5c5n2rRpPPvss143M2D4LBs0aBCPP/449957L6+++ipXTbmdpx6+j+snTmDixIkn2FcZTzzxBE8//TSfffbZCfsWLFhAdHS0185HHnmk3H6n08mKFSv4/PPPefTRR1m0aBGzZs0iLi6OlStXUlpaytChQxk1atQJ+VR0leNLxXN77bXXePDBB0+IV1n+06dPp1mzZmzatIkNGzaUu+aNCX9GLd4OdFdKHa1NwuaqIC9hDBbZC6wUkQVKqU0+0XYDE4G7Kxx+ABislCoVkWhgg3nsfnP/eKVU43utqIaSX36h7EAObc9rDu1qXp+uvlHmKuOO7+5g2f5lPDbkMS7ucnGwTWr0lJSUkJ6ezr59++jZsyfnnWeM28rLy2PChAls374dEcHhOO5e8JxzziEuLg6AlJQUdu3axZEjRxgxYgQtW7YE4Morr2Tbtm0ALF++3Otu5dprry1Xixs7diwiQu/evWndujW9e/cGoFevXmRlZZUrVK1WK507d2bz5s2sWLGCO++8kyVLluByuRg2bFiN5xoWFuZ12Nm/f38+//IrShwu1q5eyfj/LajUvrrG1wWNZ9X/hQsXsm7dOm/tLy8vj+3btxMW5v8Iyorn9vXXX/ud/w8//MBtt90GQGpqqt+ubhoa/gjZHiDvJNLOBHYopX4DEJH5wMWAV8iUUlnmPrfvgUqpMp/NcPwbXdmoyX3jVcML9BXXB9uUWuNwObhr8V0s3beUvw/+O5d0vSTYJp12/Kk51TURERGsWbOG4uJiRo8ezUsvvcS0adN46KGHOPvss/n444/JyspixIgR3mN8XbT4ulI5pX/uCgAAIABJREFUGTxpWSyWculaLJZK0x0+fDhffPEFoaGhnHvuuUycOBGXy8VTTz1VY16hoaHeFTtCQkIoKinDarFQ1SIevu5k3G43ZWVllUesBVW5oHnxxRcZPXp0ubgVHYxW596m4rlVdU8qy7+p4I9A/AYsFpEHROROz8eP49pjiKCHvWaYX4hIBxFZZ6bxpE9tDGCOiKwRkYekivVmRGSKiKwSkVXZ2dn+ZlsvcRUWkv/tD8QmObAMGB9sc2qFw+3gniX3sHjvYv428G9c3u3yYJvU5IiMjOSFF17gmWeewel0kpeXR/v2xl+xuiYtDwMHDuT777/n6NGjOBwO3n//fe++IUOGMH/+fADeeustv2pPVTFs2DCef/55Bg8eTMuWLTl69Chbt24lNTX1hLgV3cP4UuZ043C5aREdVqV9SUlJrF69GjCaDD210urSPRlGjx7Nyy+/7E1/27ZtFBUVnZBPUlISa9aswe12s2fPHlasWFEn+Q8dOpT33nsPgE2bNrF+feMccO6PkO0GvgbCgBifT0BRSu1RSqUBXYAJItLa3DVeKdUbGGZ+rq3i+FeVUhlKqQxPk0hDJf+Tj1AON/Gjh4AtLtjm+I3T7eS+Jffxze5vuD/zfq7qcVWwTWqy9O3bl7S0NN555x3uvfdeHnjgAfr27evXm3vbtm155JFHGDx4MEOHDi23QvqLL77InDlzSEtLY968efz73/8+aRsHDhzIoUOHGD7cmBSflpZG7969K10bccqUKYwZM6bcYA8PBXYHCDSPCqvSvj//+c98//339OnTh+XLlxMVFeXNMyQkhD59+pww2ONkmDx5MikpKfTr14/U1FRuvPFGnE7nCfkMHTqU5ORkUlJSmDZtGv369TvlvAFuueUWsrOzSUlJ4cEHH6RXr17epuPGRI1uXCo9SMSqlKr2HyAig4FHlFKjze0HAJRS/6ok7lzgM89gj0r2zwY+r7hfRCYCGUqpaie1NHQ3Lr9fMBJ1dDfJ781DOg4Otjl+4XQ7+evSv/JF1hfcnXE3E3pNCLZJpx3txuX043S52XKwgPiIUBKbRwbbnKDjcrlwOBzYbDZ27tzJueeey9atWyvto2vIblyqmxD9g8/veRV2+1PvXQl0FZFkEQkDrgIW+GOUiCSKSIT5uxmGL7StIv+/vTuPj6q+Gj/+OZMdyAISAYkKqBQBgSKoIIgWq9IiilKg2Cru4s+f1rb2kdq6FbV1b1FABaqCFbEVpY/ysLRWwEc2FURAlrLIKiR1QkLWmTnPH3MTJiHLBGbNnPfrlRd3vnPvnTOXyZx8773f85VkEWnrtKcAw4Evg9lnvCrbvJmy7fvJ6dUKOS0+Cul6fV5+8/FvWLBzAfeee29CJjETHU0dAN3clZSUMGjQIHr37s3IkSOZMmVKk240iRcN3ewROFCp9knqRitvqqrHuXV/If7b72eq6gYReRT/ILv5ItIf/232rYErReQRVe0BnA08IyLqvNbTqrpeRFoCC50klgQsAV4J7q3GJ/frL/tngR79E+q9ch1DfOrjwf99kPe3v8/d372bm3reFO2QTILwqZJ/pILM9BTSbQZowH/NL57PRgWroUSm9SzX9bjuHah+AHxQq+3BgOXVQF4d2y2mjnFqqnoESJiZFn3l5RQuWEzmqRUkD4z9uxV96uORTx5h/r/nc2fvO7m1163RDskkkMKSSjxeH21bZ0Q7FBNhDSWyHBEZif/0Y46IXOO0C/7yVSbMihYuwFdSSc4lfaDlSdEOp0GqyqQVk3hn6zvc1us27ugd/5X5TfxQVQ4Vl5OeYjNAJ6KG/sc/AkYELF8Z8NzSsEVkqrlnvUJKSw8tRkW+QGtTqCqPr3yct7e8zc09b+auPnfZLLwmoo6Ueyir9JLXOsM+ewmo3kSmqrF/LqsZq9i9m5L122nbPw3pcnGUo6mfqvLk6ieZs3kON3S/gXv63mNfJCbi8ov9A6BzbAbohJTwFTNilXv2DBAlZ9SPwBWb/02qytNrnmb2ptn85Oyf8It+v7AkFkPibRqXZcuW0aNHj+qyWqNGjQpquzJnBuiTWqVWzwBde5qXcAks3GuiJza/IROcer0Uvjeflu0rSPne7dEOp06qyvOfPc/rG19n7HfG8qv+v7IkFmOqSlR9+eWXtGnThhdffDHaITXojTfeYOLEiaxdu5aOHTvWqE7fkAJnBug2ATNARyqRmdhgiSwGHfnoX3jcpeQM7gaZ7aMdzjFUlcmfT2bmlzMZ3XU0vz7/15bEYlysT+Myffp05s6dy29/+1uuu+46du7cWV2aqqG4Fiz4H35w6cWM+8EQxo0dQ3FxcZ3TvAROvPnXv/61Oobx48dz9913M3DgQLp06VIjeT711FPV06889NBD1e2PPfYYXbt2ZdCgQdXHzkRXMNO4/Aj4H1UtEpHfAH2BSar6WdijS1Du16b6Z4Ee8/+iHUqdpq2bxivrX+Has67lgQsesCQWhAOPP075ptBO45J2djfa//rXja4XD9O43HLLLSxfvpzhw4czatSo6urtVeqKKyMjg0cnTeKlN+fRp3N7/vjs0zz77LM8+OCDx0zz0pD9+/ezfPlyvvrqK0aMGMGoUaNYtGgRW7duZdWqVagqI0aMYOnSpbRs2ZI5c+awdu1aPB4Pffv25dxzE2ZEUMwK5j7V36rq2yIyCLgUeAqYCpzf8GbmeHjy8ylavYE2PZOQs2Nv0smXv3iZKeumcNUZV/HggAdxiXXqY1U8TePSmLri+s+33/LVpk3cdM0wUpNdVFRUMGBA00u4XX311bhcLrp37149aeWiRYtYtGhRdcIuLi5m69atFBUVMXLkSFq08Je/GjFiRL37NZETTCLzOv/+EHhZVd8XkUlhjCmhFc55DXyQM3IEuGKrOsHML2cy+fPJDO8ynEcGPmJJrAmC6TmFWrxN4xLMvgLjKi6r5ILBFzP3rTlkpqc0uH3gWYPAKVJq77uq9qyqMnHiRG6/veY16ueff75JcZvICOabaK+IvASMAT4QEZsfLExUFffcuc4s0LF1WnHu5rk89+lzDOs0jEkXTiIpxpKsqV+8TOPSFKpKl+7fZd2nqziweyfgn0m5qqdYe5qUdu3asWnTJnw+H/PmzWt0/5dffjkzZ86kuLgYgL1793Lw4EEuuugi3n33XUpLSykqKuLvf/976N+cabJgEtJo/PUSL1dVN9AGuC+sUSWo0tWrqTh4mJwBnSDntGiHU23xrsVMWjGJi/Iu4rHBj1kSi0PxMI1LU5RVemmR3ZopL01n3Lhx9OrViwEDBvDVV/7rkLWnefn973/P8OHDGThwIB06dGh0/5dddhnjxo1jwIABnHPOOYwaNYqioiL69u3LmDFj6N27N8OGDaN///5hfZ8mOI1O4yIiZwB7VLVcRC7GXwPxdSepxYV4mcZl353XU7R0JWe9PglX39iYgHLV/lXcseQOup/UnVcue4WMZKtjFyybxiV8duQfobTCS7f2mdVjx8yJaZbTuAT4G+AVkTOBl4FTgb+ENaoE5C0q4vDSNWSdobh6XRXtcADYVLCJuz+8m9MyT+PFoS9aEjMxoazSS1GtAdAmsQWTyHzOJJrXAJNV9T6g8b65aZLD77yJepScKy+D5OiX2dl9eDcTlkwgMzWTad+fRnaa1Yk2sSHfGQB9Usvo/56Y2BDMXYuVIvJj4HqOFg5u+BYh02Tuv8wiLbuS9Kt/Fu1QyC/N5/Ylt+NRDzMvnUn7lrE3KNskJo/Xh7ukktYtUkhOsnvOjF8wn4QbgQHAY6q6Q0Q6A7VnjDYnoGzjRsp25ZPTvz3S9syoxlJcUcydS+4kvzSfF4e+SJecLlGNJ941dg3aNE1B1QzQrWwG6FCK989po4lMVTcCvwTWi0hP/Dd+/CHskSUQ96uTEZeSPe6WqMZR7i3nng/vYeu3W3lmyDP0zu0d1XjiXXp6OgUFBXH/JRErfKoUFNsM0KGmqhQUFJCenh7tUI5bMCWqLgZeA3bin1TzVBG5QVVtTrIQ8JWXU7h4GZmne0nqPzZqcXh9XiYum8iqA6t4fNDjDM6LzHig5iwvL489e/Zw6NChaIfSLBwp9/BtSSVtW6Wy6ZAlslBKT08nLy8v2mEct2CukT0DXKaqmwFEpCvwJmAFxkKg6L/n4Sv1knPFYEiJzl9EqsoTq55g8a7F/LLfL7nyjCsb38g0KiUlhc6dO0c7jGZBVRn2x2UALLhnsNX3NDUEc40spSqJAajqFuxmj5Bxz57hnwV69M+jFsO0ddN4a/Nb3NjjRm7ocUPU4jCmPh9vK+CrA0XcNKizJTFzjGB6ZJ+KyHRgtvP4OiD2RxfHgYpduyjZtIfcC1sjHXpGJYa5m+cyZd0URpwxgnvPvTcqMRjTmOnLt9O2VRpX9Tkl2qGYGBRMj+wOYCNwt/OzEZgQzqAShfvVF0CU7LHXR+X1F+1cVF166uGBD9tfuiYmbTtYxL82H+L6AaeTlmzXxsyxGuyRiUgSsE5VuwHPRiakxKAeD4XvL6LlKR5SLhof8ddftX8V9y+7n165vXh6yNOkuOxssYlNM5bvJC3ZxXXnx079URNbGuyRqaoX2Cwi9gkKseJ/LsRzuIKcof0grVXjG4SQlZ4y8aKguJx3PtvDNX3zOMnGjpl6BHONrDWwQURWAUeqGlXVZpQ7Ae7Xpvlngb4usjd5WOkpE0/eWPk15R4fNw/qFO1QTAwLaobosEeRYDyHDlH82Tba9G2JnBa5aSDyS/O5bfFtVnrKxIWySi+vf7KTS76Ty5knZ0Y7HBPD6j21KCJnisiFqvpR4A/+GaP3BLNzEblCRDaLyDYRub+O5y8Skc9ExCMiowLaT3fa14rIBhG5I+C5c0VkvbPPP0kc3qFQOGsqKOSMHgsRCr+q9FRBWYGVnjJxYf66feQXV3DLYPusmoY1dI3seeBwHe2FznMNcm4UeREYBnQHfiwi3Wut9jUwnmOnhdkPDFDVPsD5wP0iUnXf7VTgVuAs5+eKxmKJJaqK+533yMitJO2y2xvfIASs9JSJN6rKjGU76NY+k4FnnBTtcEyMayiRtVPV9bUbnbZOQez7PGCbqm5X1QpgDlBjoi1V3amqXwC+Wu0VqlruPEyrilNEOgBZqrpC/QXsXgeuDiKWmFH6yXIq8kvIGdITMnLC/nqBpacevfBRKz1l4sLybfls/qaIm20AtAlCQ4msoW/ZYG5z6wjsDni8x2kLioicKiJfOPv4g6ruc7YPPK1Z7z5F5DYRWSMia2Kp1p37z5NxpfjI+mn4p2tRVR5f+biVnjJxZ8byHbRtlcYIGwBtgtBQIlsjIrfWbhSRW4BPwxeSn6ruVtVewJnADSLSronbv6yq/VS1X25ubniCbCJvURGHP/mSrK6puLoOCfvrTVs3jblb5nJjTys9ZeLH1m/8A6BvsAHQJkgN3bX4M2CeiFzH0cTVD0gFRgax773AqQGP85y2JlHVfSLyJTAY+NjZzwntM1oOz5nhnwX6mqvDfpNHjdJTfa30lIkfMz/e4R8AfcHp0Q7FxIl6e2Sq+o2qDgQewT+Fy07gEVUdoKoHgtj3auAsEeksIqnAWGB+MEGJSJ6IZDjLrYFBwGZV3Q8cFpELnLsVrwfeC2afscA99y3ScjykX3V3WF/HSk+ZeFVQXM7fPtvLtefm0aZlarTDMXGi0XFkqvoh8GFTd6yqHhG5C1gIJAEzVXWDiDwKrFHV+SLSH5iHf9D1lSLyiKr2AM4GnhERxT8H2tMBN57cCbyK/zrdAucn5pWtX0fZbjftfngG0ip8pzqrSk/1zu1tpadM3Jm94msqPD5uutCmvzHBC2ZA9HFT1Q+AD2q1PRiwvJqapwqr2hcDverZ5xogOqXiT4B7xnP+WaB/elfYXiOw9NQLQ1+w0lMmrpRVepm1Yiff63YyZ54c2bJtJr4FU/3enCBfWRmFH64ms4uLpF4/CMtrWOkpE+/mr3UGQA+y3phpmrD2yIxf0Tuz8ZX7yBlxBbhC/7eDlZ4y8U5Vmb58O93aZzLABkCbJrIeWQS458wipZWHFqN/GfJ9F1UUMWHJBArKCpgydIqVnjJxadnWfLZ8U8wtg7vYzUmmySyRhVnFju2UbDlITv88JCfo8eBBqSo9te3bbTx78bP0yq3zsqIxMW/G8h3kZqZxZe8O0Q7FxCFLZGHmnv6Mfxbon4a2rmJV6anVB1bz6IWPMqjjoJDu35hI2fJNER9tsQHQ5vhZIgsj9XgoXLiUVnlCyvmjGt8g2P1a6SnTjMxcvoP0FBfjzrcB0Ob4WCILo+IF7+Ap9pAz7CJICt19NVPXTbXSU6ZZyC8u553P93JtXxsAbY6fJbIwcs+eTlK6l1Y//VXI9vnWV28xdd1UrjrjKis9ZeLe7BW7/AOg7ZZ7cwIskYVJ5TcHKP5iNznfzUVyzwjJPhftXMRjKx9jSN4QKz1l4l5ZpZdZn+xiaLeTOSPXBkCb42eJLEwKZz4LCtnjxodkfyv3r6wuPfXUkKdIdtkQQBPf3lu7l4IjFdw82Hpj5sRYIgsDVcX990W0aO8j7ZITv4a1qWAT93x4D6dnnW6lp0yzoKrMWL6DsztkMaCLDYA2J8YSWRiUfLSIyv+Uk33peZB8Yhewdx/ezR1L7iAzNZOpl0610lOmWageAG0zQJsQsEQWBoWvveifBXr8f53QfqpKT3nVy0vff8lKT5lmY/ryHZycmcaVvW0GaHPiLJGFmLewkMOrtpLVIxtX3vEX6T+m9FS2lZ4yzcPmA0Us3XKIGwZ2IjXZvoLMibM7BkLs8Ot/Qr2QM+bHx72PwNJTk4dOttJTplmpHgB93mnRDsU0E5bIQsw97z3S2nhJ/+HxlaQKLD31xOAnrPSUaVYOFZUzb+1eRvfLo7UNgDYhYv36ECr7bAVl+46QM6Q3ktqiydsHlp66r999DO8yPAxRGhM91QOgbQZoE0KWyEKoehbom35xXNsHlp66vsf1IY7OmOgqq/Qye8UuLj37ZLrYAGgTQpbIQsRXWkrhsi/I7JpB0lkXNHl7Kz1lmrt3P3cGQA+yG5dMaFkiC5GiOdPwVUDONSObvK2VnjLNXdUA6O4dsrigS5toh2OaGUtkIeJ+ey4pmT5a/OhnTdrOSk+ZRLB0az5bDxZzy2AbAG1CzxJZCFRs2UDJdjc5F3ZFMrKC3s5KT5lEMX3Zdk7OTGN4LxsAbULPElkIuF95yj8L9Ph7gt7m68NfW+kpkxA2Hyhi2dZ8GwBtwsY+VSdIPR7c/1hNq06ppPQeGtQ2+aX53L74dnzqs9JTptmbsXw7GSlJXHe+DYA24WEXZE5Q8buv4S3xkTNiOARx7j+w9NT0y6Zb6SnTrB0qKufdz/cxpv+p5LSwAdAmPCyRnSD3m6+TlO6j1XW/bHRdKz1lEs2sFbuo9Pm48cJO0Q7FNGNhPbUoIleIyGYR2SYi99fx/EUi8pmIeERkVEB7HxH5REQ2iMgXIjIm4LlXRWSHiKx1fvqE8z00pHLvLoo3fkPOeacjWbkNruv1ebl/6f2sPrCa3w36nZWeMs1e1QDood3a2QBoE1Zh65GJSBLwIvB9YA+wWkTmq+rGgNW+BsYDtbszJcD1qrpVRE4BPhWRharqdp6/T1X/Gq7Yg1U4/UlQIeeGOxpcT1V5bOVjLPl6iZWeMglj3ud7+c+RCm6xGaBNmIXz1OJ5wDZV3Q4gInOAq4DqRKaqO53nfIEbquqWgOV9InIQyAXcxAhVxb1gKS1OcZE6sOFB0FPXTeXtLW9zU8+brPSUSQhVA6B7nJLF+Z1tALQJr3CeWuwI7A54vMdpaxIROQ9IBf4d0PyYc8rxORFJq2e720RkjYisOXToUFNftlEli/5GpdtDzrCLG7zJo6r01NVnXs3P+jZtsLQx8eqjLYfYZgOgTYTE9O33ItIBmAXcqKpVvbaJQDegP9AGqHMaZlV9WVX7qWq/3NyGr18dD/esl3Gl+Mi8cWK96yzcubC69NRDAx6yX2iTMGYs30G7rDR+eI4NgDbhF85Ethc4NeBxntMWFBHJAt4HHlDVFVXtqrpf/cqBP+M/hRlR3oJDFH3+NVl92uNqm1fnOiv3r2Tison0ObmPlZ4yCeWrA4dtALSJqHB+ylYDZ4lIZxFJBcYC84PZ0Fl/HvB67Zs6nF4a4u/eXA18GdKog1A480nUK+Rcd2Odz28s2Fhdemry9yZb6SmTUGYs20FGSpLNAG0iJmyJTFU9wF3AQmATMFdVN4jIoyIyAkBE+ovIHuBHwEsissHZfDRwETC+jtvs3xCR9cB6oC0wKVzvoT7u/15MWlvIuOzYGze+Pvw1E5ZMICs1i2mXTrPSUyahHCwq4721+/hRvzwbAG0iJqznu1T1A+CDWm0PBiyvxn/KsfZ2s4HZ9ezzeyEOs0lKP1lE+TfltBt7Abhq/h0QWHpq2ven0a5luyhFaUx0zP6kagC03XJvIscu3DRR4czJSJKSfUvN8d1FFUXcsfgOKz1lElZZpZdZK3Zx6dnt6Ny2ZbTDMQnEElkT+IoPU7hyK5nd25CU953q9qrSU/92/5sXhr5gpadMQnrns718W1LJzYOsN2YiyxJZExTNeh5fhZAzemx1W2DpqScGP8GFHS+MYoTGRIfPp8z8eAc9O9oAaBN5dm9sE7jfnU9KltJi5ATASk8ZU+Wjrc4A6EFdbLykiTjrkQWpYv1KSnYdIXdEHyQ5BYAp66ZY6SmTEFSV/OIK9heWss9dxj536dHlwlK2HSymfVY6PzinQ7RDNQnIElmQ3NOf8c8Cfct9AMz5ag7T1k2z0lOmWSgqq2R/YRl73aXsd5exv7C0enlfYSn7C8uo8NQoiUpasotTcjI4JSedy3u0Z2z/U20AtIkKS2RB0PIy3MvW0+rMTFK6nsvCnQt5fOXjXJx3sZWeMjGv3OPlm8Jyf2IqLGWfu5R9hWXsdx/tURWVeWps4xJon5VOh5wMzumYzRU92tMhO91JXP6f1i1S7LNvYoIlsiAUvz0FbwnkXHN1jdJTTw550kpPmajy+ZT84qok5T/lV+PUX2EZh4rKj9muTctUOmSnc9pJLbigSxs6VCUoJ1mdnJlGcpL1rkx8sG/hILjfnktyhrL78mHc/c8JVnrKRISqcrjU45zaK2Wvu6oX5U9Q+9ylfHO4jEqv1tiuRWpSde+pW/ssOuQ4Pals/2nADtkZZKQmReldGRN6lsgaUbl9A8Vb3KRc0oUJy+4lOy3bSk+ZkCir9LLfOcVXo0fl/LvfXcqRCm+NbZJdQrusdDrmZHDu6a2re1EdsjOqr1dlZ9gpP5NYLJE1ovCVP4AKj/QqsdJTJmhen3Kw6OipvsC7/fYV+m+iKDhSccx2bVulckpOBmfktmTwWW2dXlSGv1eVnUFuZhpJLktSxgSyRNYA9Xr5zz/WsP00F5szS5gxdIaVnkogXp9S6fXh8Sker49Kr+Lx+fB4/e0lFd6AXpRzh5/TszpwuAyvr+Ypv1ZpydWn9s7pmFN9PaoqSbXPTic9xU75GdNUlsgaUDj/FbyHlQVDXDx38XOck3tOtEOKeT6fUul82Xu8SoXXV+PL31OVHJykUOmsd3QbH5VO4ghsrz+hBO6n7v1XBr2vqnb/uqqNv98qqUku2men0yE7nfM7t6lxXapqOSs9JXwH3pgEZomsHqrKktem0CkNVre5hmXTjwCLAAi8/FC1GHhNIvDEz9FmOaat7vVAnGfqep3ar1Xn9tX7b+w164i5kdf01pmIjiYfXxO+/E9EkktIdgkpSS6Sk4Rkl4uUJCE5SUhx1W5zkewSWqYmH9Oe4nK2SXL59+Vy2p31/M8du6+UJBfpKS46OImqbcs0XHbKz5iosERWDxGBrmfyVftChvUYU92uAX+ma3UbAW0Bz2vN9Wque+x6Nbep5/lGXpM6X1MbiCNwn8e+t5r7VJJcR7/8jyYCV40E0lBC8D9XO5HU3j5wmzr25RJLGsaYapbIGjDqyXeiHYIxxphG2IhHY4wxcc0SmTHGmLhmicwYY0xcs0RmjDEmrlkiM8YYE9cskRljjIlrlsiMMcbENUtkxhhj4ppoUwrKxSkROQTsOs7N2wL5IQwn3tnxOMqORU12PGpqDsfjdFXNjXYQjUmIRHYiRGSNqvaLdhyxwo7HUXYsarLjUZMdj8ixU4vGGGPimiUyY4wxcc0SWeNejnYAMcaOx1F2LGqy41GTHY8IsWtkxhhj4pr1yIwxxsQ1S2TGGGPimiUyh4hcISKbRWSbiNxfx/NpIvKW8/xKEekU+SgjI4hj8XMR2SgiX4jIP0Tk9GjEGSmNHY+A9a4VERWRZn3LdTDHQ0RGO5+RDSLyl0jHGClB/K6cJiIfisjnzu/LD6IRZ7Onqgn/AyQB/wa6AKnAOqB7rXXuBKY5y2OBt6IddxSPxSVAC2d5QnM9FsEeD2e9TGApsALoF+24o/z5OAv4HGjtPD452nFH8Vi8DExwlrsDO6Mdd3P8sR6Z33nANlXdrqoVwBzgqlrrXAW85iz/FRgqIhLBGCOl0WOhqh+qaonzcAWQF+EYIymYzwbA74A/AGWRDC4KgjketwIvquq3AKp6MMIxRkowx0KBLGc5G9gXwfgShiUyv47A7oDHe5y2OtdRVQ9QCJwUkegiK5hjEehmYEFYI4quRo+HiPQFTlXV9yMZWJQE8/noCnQVkY9FZIWIXBGx6CIrmGPxMPATEdkDfAD8/8iElliSox2AiV8i8hOgHzAk2rFEi4i4gGeB8VEOJZYk4z+9eDH+3vpSETlHVd1RjSo6fgy8qqrPiMgAYJaI9FRVX7QDa06sR+a3Fzg14HGe01bnOiKSjP80QUFEoousYI4FInIp8AAwQlXLIxRbNDR2PDKBnsC/RGQncAEwvxnf8BHM52MPMF9VK1V1B7AFf2JrboI5FjcDcwFU9ROYVeOiAAADYUlEQVQgHX8xYRNClsj8VgNniUhnEUnFfzPH/FrrzAducJZHAf9U5wpuM9PosRCR7wIv4U9izfX6R5UGj4eqFqpqW1XtpKqd8F8zHKGqa6ITbtgF87vyLv7eGCLSFv+pxu2RDDJCgjkWXwNDAUTkbPyJ7FBEo0wAlsiovuZ1F7AQ2ATMVdUNIvKoiIxwVpsBnCQi24CfA/Xehh3PgjwWTwGtgLdFZK2I1P7lbTaCPB4JI8jjsRAoEJGNwIfAfara7M5eBHksfgHcKiLrgDeB8c30D+CoshJVxhhj4pr1yIwxxsQ1S2TGGGPimiUyY4wxcc0SmTHGmLhmicwYY0xcs0RmzHEQkQecyu5fOEMQzg/ja/2v828nERkXrtcxJl5ZiSpjmsgpNTQc6Kuq5c6g39QT3GeyMy7pGKo60FnsBIwDmu20KMYcD+uRGdN0HYD8qtJcqpqvqvtEZKeIPCki60VklYicCSAiVzpz2H0uIktEpJ3T/rCIzBKRj/HX4OvhbLfW6emd5axX7Lzu74HBzvP3ishSEelTFZSILBeR3pE8EMbEAktkxjTdIuBUEdkiIlNEJLBocqGqngO8ADzvtC0HLlDV7+Kf6uNXAet3By5V1R8DdwB/VNU++Isx76n1uvcDy1S1j6o+h7/azHgAEekKpKvqulC+UWPigSUyY5pIVYuBc4Hb8NfNe0tExjtPvxnw7wBnOQ9YKCLrgfuAHgG7m6+qpc7yJ8CvReS/gNMD2uvzNjBcRFKAm4BXj/tNGRPHLJEZcxxU1auq/1LVh/DX27u26qnA1Zx/JwMvOD212/EXjq1yJGCffwFGAKXAByLyvUZiKAEW45/McTTwxvG/I2PilyUyY5pIRL5Tdf3K0QfY5SyPCfj3E2c5m6PTe9xAPUSkC7BdVf8EvAf0qrVKEf5pYwJNB/4ErK6akdmYRGOJzJimawW8JiIbReQL/Ne5Hnaea+203QPc67Q9jH+mgE+B/Ab2Oxr4UkTW4p/j7PVaz38BeEVknYjcC6CqnwKHgT+f8LsyJk5Z9XtjQsSZWLOfqjaUrEL9mqcA/wK62azDJlFZj8yYOCUi1wMrgQcsiZlEZj0yY4wxcc16ZMYYY+KaJTJjjDFxzRKZMcaYuGaJzBhjTFyzRGaMMSau/R/8Br5C446NZwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('Housing Dataset with 10 layer NN and different pruning techniques')\n",
        "plt.plot(x_coord, results_base_l1, label = 'L1 without finetuning')\n",
        "plt.plot(x_coord, results_finetune_l1, label = 'L1 finetuned')\n",
        "plt.plot(x_coord, results_base_l2, label = 'Random without finetuning')\n",
        "plt.plot(x_coord, results_finetune_l2, label = 'Random finetuned')\n",
        "plt.ylabel('Cross Entropy loss')\n",
        "plt.xlabel('Sparsity')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ2hfRUPKiUG"
      },
      "source": [
        "# Archive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndoBCQk1KiUH",
        "outputId": "3c845d12-bb51-4f06-e0e1-4fd94b4bdf31"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of Net(\n",
              "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
              "  (fc4): Linear(in_features=16, out_features=16, bias=True)\n",
              "  (fc5): Linear(in_features=16, out_features=8, bias=True)\n",
              "  (fc6): Linear(in_features=8, out_features=8, bias=True)\n",
              "  (fc7): Linear(in_features=8, out_features=8, bias=True)\n",
              "  (fc8): Linear(in_features=8, out_features=4, bias=True)\n",
              "  (fc9): Linear(in_features=4, out_features=4, bias=True)\n",
              "  (fc10): Linear(in_features=4, out_features=100, bias=True)\n",
              ")>"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ],
      "source": [
        "net.parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4-8sIAIKiUH"
      },
      "outputs": [],
      "source": [
        "parameters_to_prune = (\n",
        "    (net.fc1, 'weight'),\n",
        "    (net.fc2, 'weight'),\n",
        "    (net.fc3, 'weight'),\n",
        ")\n",
        "\n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune,\n",
        "    pruning_method=prune.L1Unstructured,\n",
        "    amount=0.2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl9Ub-gPKiUH",
        "outputId": "b79e7f09-f70f-4d08-883c-fbd4da206f13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sparsity in fc1.weight: 91.41%\n",
            "Sparsity in fc2.weight: 93.36%\n",
            "Sparsity in fc3.weight: 91.99%\n",
            "Global sparsity: 92.05%\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(net.fc1.weight == 0))\n",
        "        / float(net.fc1.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in fc2.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(net.fc2.weight == 0))\n",
        "        / float(net.fc2.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in fc3.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(net.fc3.weight == 0))\n",
        "        / float(net.fc3.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Global sparsity: {:.2f}%\".format(\n",
        "        100. * float(\n",
        "            + torch.sum(net.fc1.weight == 0)\n",
        "            + torch.sum(net.fc2.weight == 0)\n",
        "            + torch.sum(net.fc3.weight == 0)\n",
        "        )\n",
        "        / float(\n",
        "            + net.fc1.weight.nelement()\n",
        "            + net.fc2.weight.nelement()\n",
        "            + net.fc3.weight.nelement()\n",
        "        )\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "CvIuDFFSKiUI",
        "outputId": "4b83fad1-3be2-4dc9-c6e6-0f37b25e6417"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-149-cb0eca626ff8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# print(outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;31m# for name, param in net.named_parameters():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#   if param.requires_grad and name == 'fc2.weight':\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
          ]
        }
      ],
      "source": [
        "# Testing after pruning\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32,\n",
        "                        shuffle=True, num_workers=0)\n",
        "\n",
        "running_loss = 0.0\n",
        "for i, data in enumerate(test_loader, 0):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    inputs, labels = data['features'].to(device), data['price'].to(device)\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    # print(inputs)\n",
        "    # forward + backward + optimize\n",
        "    outputs = net(inputs)\n",
        "    # print(outputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    # for name, param in net.named_parameters():\n",
        "    #   if param.requires_grad and name == 'fc2.weight':\n",
        "    #     print(name, param.data)\n",
        "    # print(loss)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "\n",
        "running_loss = running_loss / 2613\n",
        "print(running_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYuMe0abKiUI"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5GXSfVDKiUI"
      },
      "outputs": [],
      "source": [
        "for epoch in range(50):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data['features'].to(device), data['price'].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # print(inputs)\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        # print(outputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # for name, param in net.named_parameters():\n",
        "        #   if param.requires_grad and name == 'fc2.weight':\n",
        "        #     print(name, param.data)\n",
        "        # print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:    # print every 200 mini-batches\n",
        "            print('[%d, %5d] loss: %.6f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4AI0k1qKiUI"
      },
      "outputs": [],
      "source": [
        "# Testing after finte-tuning with pruning\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32,\n",
        "                        shuffle=True, num_workers=0)\n",
        "\n",
        "running_loss = 0.0\n",
        "for i, data in enumerate(test_loader, 0):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    inputs, labels = data['features'].to(device), data['price'].to(device)\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    # print(inputs)\n",
        "    # forward + backward + optimize\n",
        "    outputs = net(inputs)\n",
        "    # print(outputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    # for name, param in net.named_parameters():\n",
        "    #   if param.requires_grad and name == 'fc2.weight':\n",
        "    #     print(name, param.data)\n",
        "    # print(loss)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "\n",
        "running_loss = running_loss / 2613\n",
        "print(running_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83ySPYZLiO1j"
      },
      "source": [
        "## Plants classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj_J9BS9wpII"
      },
      "source": [
        "# Experiment 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZmgh3lIwy4t"
      },
      "source": [
        "**Dataset - One hundred plants texture**\n",
        "\n",
        "**Architecture - 3 layer Neural Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0GPY8im17JO"
      },
      "source": [
        "### Preliminary training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7p29zFzIw9DA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlauX8Me5egY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhS0mG2dwC2z"
      },
      "outputs": [],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0xhaC_swEGs"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_zHdrIm3ulh"
      },
      "outputs": [],
      "source": [
        "class HouseSalesDataset(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, csv_file, root_dir, transform=None, train = True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.housing_data = pd.read_csv(csv_file)\n",
        "        self.x = self.housing_data.drop('price', axis = 1)\n",
        "        df = self.housing_data.drop('date', axis = 1)\n",
        "        self.x = (df-df.mean())/df.std()\n",
        "        self.y = self.housing_data[['price']]/1000\n",
        "\n",
        "        if train:\n",
        "          self.x = self.x.iloc[:18000, :]\n",
        "          self.y = self.y.iloc[:18000, :]\n",
        "\n",
        "        else:\n",
        "          self.x = self.x.iloc[18000:, :]\n",
        "          self.y = self.y.iloc[18000:, :]  \n",
        "\n",
        "\n",
        "        print('Length = ', len(self.y))\n",
        "        self.x = torch.Tensor(self.x.values).to(device)\n",
        "        self.y = torch.Tensor(self.y.values).to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # if self.transform:\n",
        "        #     sample = self.transform(sample)\n",
        "\n",
        "        features = self.x[idx]\n",
        "        price = self.y[idx]\n",
        "        return {'features': features, 'price': price}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtU0HSzLDdgH"
      },
      "outputs": [],
      "source": [
        "train_dataset = HouseSalesDataset(csv_file='kc_house_data.csv',\n",
        "                                    root_dir='/', train = True)\n",
        "\n",
        "test_dataset = HouseSalesDataset(csv_file='kc_house_data.csv',\n",
        "                                    root_dir='/', train = False)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32,\n",
        "                        shuffle=True, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZFpI9pnxLZw"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(20, 16)\n",
        "        self.fc2 = nn.Linear(16, 4)\n",
        "        self.fc3 = nn.Linear(4, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = Net().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bClq3wEdsnI"
      },
      "outputs": [],
      "source": [
        "# for i_batch, sample_batched in enumerate(dataloader):\n",
        "#     print(i_batch, sample_batched['features'].size(),\n",
        "#           sample_batched['price'].size())\n",
        "#     outputs = net(sample_batched['features'])\n",
        "#     print(outputs)\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDB3zWTix0kk"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.MSELoss()\n",
        "# optimizer = optim.SGD(net.parameters(), lr=10)\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oK2BOIYezpg1"
      },
      "outputs": [],
      "source": [
        "# Testing before training\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32,\n",
        "                        shuffle=True, num_workers=0)\n",
        "\n",
        "running_loss = 0.0\n",
        "for i, data in enumerate(test_loader, 0):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    inputs, labels = data['features'].to(device), data['price'].to(device)\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    # print(inputs)\n",
        "    # forward + backward + optimize\n",
        "    outputs = net(inputs)\n",
        "    # print(outputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    # for name, param in net.named_parameters():\n",
        "    #   if param.requires_grad and name == 'fc2.weight':\n",
        "    #     print(name, param.data)\n",
        "    # print(loss)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "\n",
        "running_loss = running_loss / 2613\n",
        "print(running_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mG-qVOwBx1Of"
      },
      "outputs": [],
      "source": [
        "for epoch in range(50):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data['features'].to(device), data['price'].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # print(inputs)\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        # print(outputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # for name, param in net.named_parameters():\n",
        "        #   if param.requires_grad and name == 'fc2.weight':\n",
        "        #     print(name, param.data)\n",
        "        # print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:    # print every 200 mini-batches\n",
        "            print('[%d, %5d] loss: %.6f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6dVOovf1uGK"
      },
      "outputs": [],
      "source": [
        "# Testing after training\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32,\n",
        "                        shuffle=True, num_workers=0)\n",
        "\n",
        "running_loss = 0.0\n",
        "for i, data in enumerate(test_loader, 0):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    inputs, labels = data['features'].to(device), data['price'].to(device)\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    # print(inputs)\n",
        "    # forward + backward + optimize\n",
        "    outputs = net(inputs)\n",
        "    # print(outputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    # for name, param in net.named_parameters():\n",
        "    #   if param.requires_grad and name == 'fc2.weight':\n",
        "    #     print(name, param.data)\n",
        "    # print(loss)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "\n",
        "running_loss = running_loss / 2613\n",
        "print(running_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INLwBAB2yAui"
      },
      "outputs": [],
      "source": [
        "# PATH = './cifar_net.pth'\n",
        "# torch.save(net.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAu-V_4jyIjm"
      },
      "outputs": [],
      "source": [
        "# dataiter = iter(testloader)\n",
        "# images, labels = dataiter.next()\n",
        "\n",
        "# # print images\n",
        "# imshow(torchvision.utils.make_grid(images))\n",
        "# print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhC2GF5ByMBq"
      },
      "outputs": [],
      "source": [
        "net = Net()\n",
        "net.load_state_dict(torch.load(PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndKMGUlk10Lb"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJ2qR7eiyOVA"
      },
      "outputs": [],
      "source": [
        "# outputs = net(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOScOg_Y1_6S"
      },
      "source": [
        "### Global pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzCyqXbp2Itl"
      },
      "outputs": [],
      "source": [
        "net.parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmsBmesT2Ch8"
      },
      "outputs": [],
      "source": [
        "parameters_to_prune = (\n",
        "    (net.fc1, 'weight'),\n",
        "    (net.fc2, 'weight'),\n",
        "    (net.fc3, 'weight'),\n",
        ")\n",
        "\n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune,\n",
        "    pruning_method=prune.L1Unstructured,\n",
        "    amount=0.2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpWOfgvp2Ch_"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(net.fc1.weight == 0))\n",
        "        / float(net.fc1.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in fc2.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(net.fc2.weight == 0))\n",
        "        / float(net.fc2.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in fc3.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(net.fc3.weight == 0))\n",
        "        / float(net.fc3.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Global sparsity: {:.2f}%\".format(\n",
        "        100. * float(\n",
        "            + torch.sum(net.fc1.weight == 0)\n",
        "            + torch.sum(net.fc2.weight == 0)\n",
        "            + torch.sum(net.fc3.weight == 0)\n",
        "        )\n",
        "        / float(\n",
        "            + net.fc1.weight.nelement()\n",
        "            + net.fc2.weight.nelement()\n",
        "            + net.fc3.weight.nelement()\n",
        "        )\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x37dQbGH2fTf"
      },
      "outputs": [],
      "source": [
        "# Testing after pruning\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32,\n",
        "                        shuffle=True, num_workers=0)\n",
        "\n",
        "running_loss = 0.0\n",
        "for i, data in enumerate(test_loader, 0):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    inputs, labels = data['features'].to(device), data['price'].to(device)\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    # print(inputs)\n",
        "    # forward + backward + optimize\n",
        "    outputs = net(inputs)\n",
        "    # print(outputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    # for name, param in net.named_parameters():\n",
        "    #   if param.requires_grad and name == 'fc2.weight':\n",
        "    #     print(name, param.data)\n",
        "    # print(loss)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "\n",
        "running_loss = running_loss / 2613\n",
        "print(running_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sjw9tUxz3egg"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeNmaL0l2s_f"
      },
      "outputs": [],
      "source": [
        "for epoch in range(50):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data['features'].to(device), data['price'].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # print(inputs)\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        # print(outputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # for name, param in net.named_parameters():\n",
        "        #   if param.requires_grad and name == 'fc2.weight':\n",
        "        #     print(name, param.data)\n",
        "        # print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:    # print every 200 mini-batches\n",
        "            print('[%d, %5d] loss: %.6f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0e6AaML3Dr2"
      },
      "outputs": [],
      "source": [
        "# Testing after finte-tuning with pruning\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32,\n",
        "                        shuffle=True, num_workers=0)\n",
        "\n",
        "running_loss = 0.0\n",
        "for i, data in enumerate(test_loader, 0):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    inputs, labels = data['features'].to(device), data['price'].to(device)\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    # print(inputs)\n",
        "    # forward + backward + optimize\n",
        "    outputs = net(inputs)\n",
        "    # print(outputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    # for name, param in net.named_parameters():\n",
        "    #   if param.requires_grad and name == 'fc2.weight':\n",
        "    #     print(name, param.data)\n",
        "    # print(loss)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "\n",
        "running_loss = running_loss / 2613\n",
        "print(running_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRaMyEVzvoBQ"
      },
      "outputs": [],
      "source": [
        "module = net.fc1\n",
        "print(list(module.named_parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgvIrZguvsHB"
      },
      "outputs": [],
      "source": [
        "print(list(module.named_buffers()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oH2TsKSovvCL"
      },
      "outputs": [],
      "source": [
        "prune.random_unstructured(module, name=\"weight\", amount=0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ni68XwmBvyX4"
      },
      "outputs": [],
      "source": [
        "print(list(module.named_parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLjS5bxQv5O9"
      },
      "outputs": [],
      "source": [
        "print(list(module.named_buffers()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUWJiC8JwKk5"
      },
      "outputs": [],
      "source": [
        "print(module.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5FWQq_twKk6"
      },
      "outputs": [],
      "source": [
        "print(module._forward_pre_hooks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DW05XgsKwKk6"
      },
      "outputs": [],
      "source": [
        "# prune.l1_unstructured(module, name=\"bias\", amount=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQSrIwNewKk6"
      },
      "outputs": [],
      "source": [
        "# print(list(module.named_parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKE7_4lFwKk7"
      },
      "outputs": [],
      "source": [
        "# print(list(module.named_buffers()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wX9nN8DowKk7"
      },
      "outputs": [],
      "source": [
        "# print(module.bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bo2KYtfwKk7"
      },
      "outputs": [],
      "source": [
        "# print(module._forward_pre_hooks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cey-RehrwdjI"
      },
      "outputs": [],
      "source": [
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data['features'], data['price']\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # print(inputs)\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        # print(outputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:    # print every 200 mini-batches\n",
        "            print('[%d, %5d] loss: %.6f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pp6X2fNvCfB"
      },
      "source": [
        "## From the blog"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3sEyvXjvsks"
      },
      "source": [
        "### Local pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70cx-Wlcux3H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpHfP5rKvFLq"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 3x3 square conv kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5x5 image dimension\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, int(x.nelement() / x.shape[0]))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = LeNet().to(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFglxGfJvItu"
      },
      "outputs": [],
      "source": [
        "module = model.fc1\n",
        "print(list(module.named_parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4guflj9pvKlO"
      },
      "outputs": [],
      "source": [
        "print(list(module.named_buffers()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEqOQihkvOIA"
      },
      "outputs": [],
      "source": [
        "prune.random_unstructured(module, name=\"weight\", amount=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVV7QFzLvQ5Q"
      },
      "outputs": [],
      "source": [
        "print(list(module.named_parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvgHtZOzvSx7"
      },
      "outputs": [],
      "source": [
        "print(list(module.named_buffers()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tH4-6Su0vXzs"
      },
      "outputs": [],
      "source": [
        "print(module.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peFlVtovvarn"
      },
      "outputs": [],
      "source": [
        "print(module._forward_pre_hooks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArWdi4u_vcw3"
      },
      "outputs": [],
      "source": [
        "prune.l1_unstructured(module, name=\"bias\", amount=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MAvudMbveea"
      },
      "outputs": [],
      "source": [
        "print(list(module.named_parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7MNQUgAvgHk"
      },
      "outputs": [],
      "source": [
        "print(list(module.named_buffers()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfxcpRQ7viVc"
      },
      "outputs": [],
      "source": [
        "print(module.bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYUSGwucvk6d"
      },
      "outputs": [],
      "source": [
        "print(module._forward_pre_hooks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqsT1zllvvZQ"
      },
      "source": [
        "### Global pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5qY-3i5vzqx"
      },
      "outputs": [],
      "source": [
        "model = LeNet()\n",
        "\n",
        "parameters_to_prune = (\n",
        "    (model.conv1, 'weight'),\n",
        "    (model.conv2, 'weight'),\n",
        "    (model.fc1, 'weight'),\n",
        "    (model.fc2, 'weight'),\n",
        "    (model.fc3, 'weight'),\n",
        ")\n",
        "\n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune,\n",
        "    pruning_method=prune.L1Unstructured,\n",
        "    amount=0.2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFte1oQhv1FT"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    \"Sparsity in conv1.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.conv1.weight == 0))\n",
        "        / float(model.conv1.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in conv2.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.conv2.weight == 0))\n",
        "        / float(model.conv2.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.fc1.weight == 0))\n",
        "        / float(model.fc1.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in fc2.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.fc2.weight == 0))\n",
        "        / float(model.fc2.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in fc3.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.fc3.weight == 0))\n",
        "        / float(model.fc3.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Global sparsity: {:.2f}%\".format(\n",
        "        100. * float(\n",
        "            torch.sum(model.conv1.weight == 0)\n",
        "            + torch.sum(model.conv2.weight == 0)\n",
        "            + torch.sum(model.fc1.weight == 0)\n",
        "            + torch.sum(model.fc2.weight == 0)\n",
        "            + torch.sum(model.fc3.weight == 0)\n",
        "        )\n",
        "        / float(\n",
        "            model.conv1.weight.nelement()\n",
        "            + model.conv2.weight.nelement()\n",
        "            + model.fc1.weight.nelement()\n",
        "            + model.fc2.weight.nelement()\n",
        "            + model.fc3.weight.nelement()\n",
        "        )\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbG2AlTEv8Nm"
      },
      "source": [
        "### Custom pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B192Oibnv-qq"
      },
      "outputs": [],
      "source": [
        "class FooBarPruningMethod(prune.BasePruningMethod):\n",
        "    \"\"\"Prune every other entry in a tensor\n",
        "    \"\"\"\n",
        "    PRUNING_TYPE = 'unstructured'\n",
        "\n",
        "    def compute_mask(self, t, default_mask):\n",
        "        mask = default_mask.clone()\n",
        "        mask.view(-1)[::2] = 0\n",
        "        return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPZi4OHrwhGd"
      },
      "outputs": [],
      "source": [
        "def foobar_unstructured(module, name):\n",
        "    \"\"\"Prunes tensor corresponding to parameter called `name` in `module`\n",
        "    by removing every other entry in the tensors.\n",
        "    Modifies module in place (and also return the modified module)\n",
        "    by:\n",
        "    1) adding a named buffer called `name+'_mask'` corresponding to the\n",
        "    binary mask applied to the parameter `name` by the pruning method.\n",
        "    The parameter `name` is replaced by its pruned version, while the\n",
        "    original (unpruned) parameter is stored in a new parameter named\n",
        "    `name+'_orig'`.\n",
        "\n",
        "    Args:\n",
        "        module (nn.Module): module containing the tensor to prune\n",
        "        name (string): parameter name within `module` on which pruning\n",
        "                will act.\n",
        "\n",
        "    Returns:\n",
        "        module (nn.Module): modified (i.e. pruned) version of the input\n",
        "            module\n",
        "\n",
        "    Examples:\n",
        "        >>> m = nn.Linear(3, 4)\n",
        "        >>> foobar_unstructured(m, name='bias')\n",
        "    \"\"\"\n",
        "    FooBarPruningMethod.apply(module, name)\n",
        "    return module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1Nrk6qYwir6"
      },
      "outputs": [],
      "source": [
        "model = LeNet()\n",
        "foobar_unstructured(model.fc3, name='bias')\n",
        "\n",
        "print(model.fc3.bias_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sb3qU-zbwko_"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "kj_J9BS9wpII",
        "rbG2AlTEv8Nm"
      ],
      "name": "Model pruning - 1D data - plant classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}