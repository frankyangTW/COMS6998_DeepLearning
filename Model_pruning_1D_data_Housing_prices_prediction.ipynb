{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mEyU01VKZJi"
      },
      "source": [
        "# Latest experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HleQjExKiT7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0eYdAbxKiT9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3s-ewmMKiT9",
        "outputId": "b6736d80-9705-4663-d627-926ded1a252e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAfg57FWKiT-",
        "outputId": "6b7de563-e2e4-4d42-bdb0-83feb4e49dc7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBc0_Q1kiMh3"
      },
      "source": [
        "## Housing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4oIwf3BKiT-"
      },
      "outputs": [],
      "source": [
        "class HouseSalesDataset(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, csv_file, root_dir, transform=None, train = True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.housing_data = pd.read_csv(csv_file)\n",
        "        self.x = self.housing_data.drop('price', axis = 1)\n",
        "        df = self.housing_data.drop('date', axis = 1)\n",
        "        self.x = (df-df.mean())/df.std()\n",
        "        self.y = self.housing_data[['price']]/1000\n",
        "\n",
        "        if train:\n",
        "          self.x = self.x.iloc[:18000, :]\n",
        "          self.y = self.y.iloc[:18000, :]\n",
        "\n",
        "        else:\n",
        "          self.x = self.x.iloc[18000:, :]\n",
        "          self.y = self.y.iloc[18000:, :]  \n",
        "\n",
        "\n",
        "        print('Length = ', len(self.y))\n",
        "        self.x = torch.Tensor(self.x.values).to(device)\n",
        "        self.y = torch.Tensor(self.y.values).to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # if self.transform:\n",
        "        #     sample = self.transform(sample)\n",
        "\n",
        "        features = self.x[idx]\n",
        "        price = self.y[idx]\n",
        "        return {'features': features, 'price': price}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHE3Dd6lKiT_",
        "outputId": "b1d6e057-da54-4aec-bb08-ab79f6699ce8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length =  18000\n",
            "Length =  3613\n"
          ]
        }
      ],
      "source": [
        "train_dataset = HouseSalesDataset(csv_file='kc_house_data.csv',\n",
        "                                    root_dir='/', train = True)\n",
        "\n",
        "test_dataset = HouseSalesDataset(csv_file='kc_house_data.csv',\n",
        "                                    root_dir='/', train = False)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32,\n",
        "                        shuffle=True, num_workers=0)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32,\n",
        "                        shuffle=True, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg9SZre4KiUE"
      },
      "outputs": [],
      "source": [
        "def test(net, test_loader, loss):\n",
        "  running_loss = 0.0\n",
        "  for i, data in enumerate(test_loader, 0):\n",
        "      # get the inputs; data is a list of [inputs, labels]\n",
        "      inputs, labels = data['features'].to(device), data['price'].to(device)\n",
        "      # print(inputs)\n",
        "      # forward\n",
        "      outputs = net(inputs)\n",
        "      # print(outputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      running_loss += loss.item()\n",
        "\n",
        "  running_loss = running_loss / 2613\n",
        "  print(\"Avg Loss during Testing - \", running_loss)\n",
        "  return running_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3lJMthqKiUF"
      },
      "outputs": [],
      "source": [
        "def train(net, train_loader, num_epoch, optimizer, criterion, running_loss_batch = 200):\n",
        "  for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(train_loader, 0):\n",
        "          # get the inputs; data is a list of [inputs, labels]\n",
        "          inputs, labels = data['features'].to(device), data['price'].to(device)\n",
        "\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "          # print(inputs)\n",
        "          # forward + backward + optimize\n",
        "          outputs = net(inputs)\n",
        "          # print(outputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "          # for name, param in net.named_parameters():\n",
        "          #   if param.requires_grad and name == 'fc2.weight':\n",
        "          #     print(name, param.data)\n",
        "          # print(loss)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          # print statistics\n",
        "          running_loss += loss.item()\n",
        "          if i % running_loss_batch == (running_loss_batch-1):    # print every 150 mini-batches\n",
        "              print('[%d, %5d] loss: %.6f' %\n",
        "                    (epoch + 1, i + 1, running_loss / running_loss_batch))\n",
        "              running_loss = 0.0\n",
        "\n",
        "  print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_dcfdypiUlA"
      },
      "source": [
        "### 3 layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACO-sU_9N6Wy"
      },
      "outputs": [],
      "source": [
        "def print_sparsity(net):\n",
        "  print(\n",
        "      \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc1.weight == 0))\n",
        "          / float(net.fc1.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Sparsity in fc2.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc2.weight == 0))\n",
        "          / float(net.fc2.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Sparsity in fc3.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc3.weight == 0))\n",
        "          / float(net.fc3.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Global sparsity: {:.2f}%\".format(\n",
        "          100. * float(\n",
        "              + torch.sum(net.fc1.weight == 0)\n",
        "              + torch.sum(net.fc2.weight == 0)\n",
        "              + torch.sum(net.fc3.weight == 0)\n",
        "          )\n",
        "          / float(\n",
        "              + net.fc1.weight.nelement()\n",
        "              + net.fc2.weight.nelement()\n",
        "              + net.fc3.weight.nelement()\n",
        "          )\n",
        "      )\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfMqGD0nKiT_"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(20, 16)\n",
        "        self.fc2 = nn.Linear(16, 4)\n",
        "        self.fc3 = nn.Linear(4, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = Net().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6r1P5wPKiUE"
      },
      "outputs": [],
      "source": [
        "# # criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.MSELoss()\n",
        "# # optimizer = optim.SGD(net.parameters(), lr=10)\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmmQhR_kLTnD",
        "outputId": "11944207-2a6b-4a2f-ab2c-9ae1bbb62f00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Avg Loss during Testing -  20565.146377487563\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "20565.146377487563"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Testing before training\n",
        "test(net, test_loader, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_7FLch8MyLz",
        "outputId": "a0d18991-4562-4d8f-f3cd-141cf66d23e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sparsity in fc1.weight: 0.00%\n",
            "Sparsity in fc2.weight: 0.00%\n",
            "Sparsity in fc3.weight: 0.00%\n",
            "Global sparsity: 0.00%\n",
            "[1,   560] loss: 300429.697391\n",
            "[2,   560] loss: 171622.900991\n",
            "[3,   560] loss: 121545.484583\n",
            "[4,   560] loss: 99901.555323\n",
            "[5,   560] loss: 87849.517773\n",
            "[6,   560] loss: 80611.654949\n",
            "[7,   560] loss: 76567.878448\n",
            "[8,   560] loss: 74023.100490\n",
            "[9,   560] loss: 68949.733404\n",
            "[10,   560] loss: 71020.153735\n",
            "[11,   560] loss: 70246.509391\n",
            "[12,   560] loss: 69164.522939\n",
            "[13,   560] loss: 68535.724574\n",
            "[14,   560] loss: 70629.258571\n",
            "[15,   560] loss: 69766.748472\n",
            "[16,   560] loss: 68854.450584\n",
            "[17,   560] loss: 68397.126484\n",
            "[18,   560] loss: 68261.658672\n",
            "[19,   560] loss: 68744.722768\n",
            "[20,   560] loss: 70210.063644\n",
            "[21,   560] loss: 69265.856726\n",
            "[22,   560] loss: 68737.477787\n",
            "[23,   560] loss: 66958.531906\n",
            "[24,   560] loss: 68181.123846\n",
            "[25,   560] loss: 68130.528374\n",
            "[26,   560] loss: 67675.490859\n",
            "[27,   560] loss: 68674.469001\n",
            "[28,   560] loss: 70911.522883\n",
            "[29,   560] loss: 69917.627860\n",
            "[30,   560] loss: 67944.541837\n",
            "[31,   560] loss: 68529.287355\n",
            "[32,   560] loss: 69097.528530\n",
            "[33,   560] loss: 70770.856487\n",
            "[34,   560] loss: 68879.388766\n",
            "[35,   560] loss: 69422.898322\n",
            "[36,   560] loss: 68845.697327\n",
            "[37,   560] loss: 68857.706982\n",
            "[38,   560] loss: 69181.504042\n",
            "[39,   560] loss: 68745.894524\n",
            "[40,   560] loss: 68232.392833\n",
            "[41,   560] loss: 67990.761279\n",
            "[42,   560] loss: 67572.698723\n",
            "[43,   560] loss: 68152.918534\n",
            "[44,   560] loss: 68556.954365\n",
            "[45,   560] loss: 68856.218213\n",
            "[46,   560] loss: 69107.030490\n",
            "[47,   560] loss: 68859.595527\n",
            "[48,   560] loss: 68782.558831\n",
            "[49,   560] loss: 69513.077647\n",
            "[50,   560] loss: 69063.922995\n",
            "Finished Training\n",
            "Avg Loss during Testing -  3084.206892969049\n",
            "Sparsity in fc1.weight: 13.75%\n",
            "Sparsity in fc2.weight: 53.12%\n",
            "Sparsity in fc3.weight: 0.00%\n",
            "Global sparsity: 20.10%\n",
            "Avg Loss during Testing -  3071.4166165865386\n",
            "[1,   560] loss: 67949.454681\n",
            "[2,   560] loss: 67170.505266\n",
            "[3,   560] loss: 67465.378066\n",
            "[4,   560] loss: 67040.965353\n",
            "[5,   560] loss: 67607.281691\n",
            "[6,   560] loss: 67585.210833\n",
            "[7,   560] loss: 68767.211373\n",
            "[8,   560] loss: 68305.922859\n",
            "[9,   560] loss: 67068.973532\n",
            "[10,   560] loss: 68086.013832\n",
            "[11,   560] loss: 67401.175795\n",
            "[12,   560] loss: 68015.132621\n",
            "[13,   560] loss: 67427.259317\n",
            "[14,   560] loss: 67452.636945\n",
            "[15,   560] loss: 67062.353892\n",
            "[16,   560] loss: 67595.278721\n",
            "[17,   560] loss: 67142.202888\n",
            "[18,   560] loss: 67274.323957\n",
            "[19,   560] loss: 67967.629782\n",
            "[20,   560] loss: 69069.133524\n",
            "[21,   560] loss: 68051.250129\n",
            "[22,   560] loss: 67227.182957\n",
            "[23,   560] loss: 67395.264579\n",
            "[24,   560] loss: 66965.230936\n",
            "[25,   560] loss: 66591.722614\n",
            "[26,   560] loss: 65912.985100\n",
            "[27,   560] loss: 67138.255702\n",
            "[28,   560] loss: 68190.121352\n",
            "[29,   560] loss: 68185.679206\n",
            "[30,   560] loss: 69675.495163\n",
            "[31,   560] loss: 71073.037019\n",
            "[32,   560] loss: 72186.450359\n",
            "[33,   560] loss: 71727.057342\n",
            "[34,   560] loss: 69762.631365\n",
            "[35,   560] loss: 67323.345396\n",
            "[36,   560] loss: 68233.117212\n",
            "[37,   560] loss: 67828.389303\n",
            "[38,   560] loss: 66024.856461\n",
            "[39,   560] loss: 67641.414226\n",
            "[40,   560] loss: 67617.793157\n",
            "[41,   560] loss: 67331.217862\n",
            "[42,   560] loss: 65915.558995\n",
            "[43,   560] loss: 68160.978366\n",
            "[44,   560] loss: 67096.016996\n",
            "[45,   560] loss: 67412.588839\n",
            "[46,   560] loss: 65960.482861\n",
            "[47,   560] loss: 66785.638079\n",
            "[48,   560] loss: 65960.972705\n",
            "[49,   560] loss: 66465.977023\n",
            "[50,   560] loss: 66283.574044\n",
            "Finished Training\n",
            "Avg Loss during Testing -  2794.745364972015\n",
            "L1 values -  [3084.206892969049, 3071.4166165865386]\n",
            "L1 values -  [3084.206892969049, 2794.745364972015]\n",
            "Sparsity in fc1.weight: 20.62%\n",
            "Sparsity in fc2.weight: 15.62%\n",
            "Sparsity in fc3.weight: 50.00%\n",
            "Global sparsity: 20.10%\n",
            "Avg Loss during Testing -  6333.156765750574\n",
            "[1,   560] loss: 116022.596125\n",
            "[2,   560] loss: 88059.507807\n",
            "[3,   560] loss: 79049.898743\n",
            "[4,   560] loss: 74540.665348\n",
            "[5,   560] loss: 72760.373104\n",
            "[6,   560] loss: 71380.972593\n",
            "[7,   560] loss: 69390.739952\n",
            "[8,   560] loss: 69637.894866\n",
            "[9,   560] loss: 69492.720755\n",
            "[10,   560] loss: 68197.358611\n",
            "[11,   560] loss: 67734.015873\n",
            "[12,   560] loss: 68337.232799\n",
            "[13,   560] loss: 68886.481601\n",
            "[14,   560] loss: 68776.030214\n",
            "[15,   560] loss: 65936.197658\n",
            "[16,   560] loss: 68274.583264\n",
            "[17,   560] loss: 67514.166221\n",
            "[18,   560] loss: 68548.456707\n",
            "[19,   560] loss: 68577.818110\n",
            "[20,   560] loss: 68163.404762\n",
            "[21,   560] loss: 67547.973141\n",
            "[22,   560] loss: 67323.197384\n",
            "[23,   560] loss: 67559.510446\n",
            "[24,   560] loss: 68093.976420\n",
            "[25,   560] loss: 66886.443413\n",
            "[26,   560] loss: 67140.338370\n",
            "[27,   560] loss: 67848.970956\n",
            "[28,   560] loss: 67247.419842\n",
            "[29,   560] loss: 67143.424360\n",
            "[30,   560] loss: 67011.234127\n",
            "[31,   560] loss: 67203.260801\n",
            "[32,   560] loss: 67602.510535\n",
            "[33,   560] loss: 67300.458186\n",
            "[34,   560] loss: 66993.189952\n",
            "[35,   560] loss: 67522.326032\n",
            "[36,   560] loss: 66600.669446\n",
            "[37,   560] loss: 66673.685577\n",
            "[38,   560] loss: 67790.929782\n",
            "[39,   560] loss: 66619.848706\n",
            "[40,   560] loss: 65976.518028\n",
            "[41,   560] loss: 65934.062404\n",
            "[42,   560] loss: 65721.980685\n",
            "[43,   560] loss: 67071.883772\n",
            "[44,   560] loss: 66333.820189\n",
            "[45,   560] loss: 66668.894646\n",
            "[46,   560] loss: 66450.804260\n",
            "[47,   560] loss: 66215.282556\n",
            "[48,   560] loss: 67412.141950\n",
            "[49,   560] loss: 66015.546031\n",
            "[50,   560] loss: 66707.753934\n",
            "Finished Training\n",
            "Avg Loss during Testing -  3312.844593140069\n",
            "L2 values -  [3084.206892969049, 6333.156765750574]\n",
            "L2 values -  [3084.206892969049, 3312.844593140069]\n",
            "Sparsity in fc1.weight: 31.88%\n",
            "Sparsity in fc2.weight: 82.81%\n",
            "Sparsity in fc3.weight: 0.00%\n",
            "Global sparsity: 39.95%\n",
            "Avg Loss during Testing -  2921.015459062859\n",
            "[1,   560] loss: 67385.226683\n",
            "[2,   560] loss: 66866.777199\n",
            "[3,   560] loss: 67022.067400\n",
            "[4,   560] loss: 66887.814962\n",
            "[5,   560] loss: 66650.785531\n",
            "[6,   560] loss: 66625.553376\n",
            "[7,   560] loss: 66593.325945\n",
            "[8,   560] loss: 67155.951967\n",
            "[9,   560] loss: 69517.606342\n",
            "[10,   560] loss: 67025.534818\n",
            "[11,   560] loss: 67613.976074\n",
            "[12,   560] loss: 64234.880127\n",
            "[13,   560] loss: 65567.787821\n",
            "[14,   560] loss: 66177.933151\n",
            "[15,   560] loss: 66536.125949\n",
            "[16,   560] loss: 67359.807593\n",
            "[17,   560] loss: 67715.169981\n",
            "[18,   560] loss: 67994.865695\n",
            "[19,   560] loss: 67645.294277\n",
            "[20,   560] loss: 68377.619866\n",
            "[21,   560] loss: 67767.260393\n",
            "[22,   560] loss: 66558.256986\n",
            "[23,   560] loss: 66606.183105\n",
            "[24,   560] loss: 65972.728428\n",
            "[25,   560] loss: 66125.233838\n",
            "[26,   560] loss: 66426.432800\n",
            "[27,   560] loss: 66021.403271\n",
            "[28,   560] loss: 67145.198633\n",
            "[29,   560] loss: 66455.566420\n",
            "[30,   560] loss: 66774.733943\n",
            "[31,   560] loss: 66882.972304\n",
            "[32,   560] loss: 66213.959333\n",
            "[33,   560] loss: 65516.035557\n",
            "[34,   560] loss: 64967.458845\n",
            "[35,   560] loss: 65922.183235\n",
            "[36,   560] loss: 65927.921017\n",
            "[37,   560] loss: 65783.634781\n",
            "[38,   560] loss: 65722.383868\n",
            "[39,   560] loss: 65736.306376\n",
            "[40,   560] loss: 65605.287699\n",
            "[41,   560] loss: 65000.805378\n",
            "[42,   560] loss: 65985.937437\n",
            "[43,   560] loss: 65298.720302\n",
            "[44,   560] loss: 65762.571392\n",
            "[45,   560] loss: 65949.321952\n",
            "[46,   560] loss: 65593.350335\n",
            "[47,   560] loss: 65458.081519\n",
            "[48,   560] loss: 64981.513299\n",
            "[49,   560] loss: 66083.719734\n",
            "[50,   560] loss: 65875.052588\n",
            "Finished Training\n",
            "Avg Loss during Testing -  2877.615210456133\n",
            "L1 values -  [3084.206892969049, 3071.4166165865386, 2921.015459062859]\n",
            "L1 values -  [3084.206892969049, 2794.745364972015, 2877.615210456133]\n",
            "Sparsity in fc1.weight: 41.25%\n",
            "Sparsity in fc2.weight: 32.81%\n",
            "Sparsity in fc3.weight: 50.00%\n",
            "Global sparsity: 39.95%\n",
            "Avg Loss during Testing -  6335.233910077019\n",
            "[1,   560] loss: 135246.584877\n",
            "[2,   560] loss: 132681.271069\n",
            "[3,   560] loss: 132816.690217\n",
            "[4,   560] loss: 132922.439000\n",
            "[5,   560] loss: 132308.052009\n",
            "[6,   560] loss: 132664.293499\n",
            "[7,   560] loss: 132795.846470\n",
            "[8,   560] loss: 132481.344249\n",
            "[9,   560] loss: 132958.094280\n",
            "[10,   560] loss: 132848.467146\n",
            "[11,   560] loss: 130429.894144\n",
            "[12,   560] loss: 132728.467965\n",
            "[13,   560] loss: 132851.297531\n",
            "[14,   560] loss: 132930.271666\n",
            "[15,   560] loss: 132519.048546\n",
            "[16,   560] loss: 132756.144245\n",
            "[17,   560] loss: 132911.319521\n",
            "[18,   560] loss: 132888.708018\n",
            "[19,   560] loss: 132919.343324\n",
            "[20,   560] loss: 132735.681892\n",
            "[21,   560] loss: 132875.082481\n",
            "[22,   560] loss: 132729.997799\n",
            "[23,   560] loss: 132790.228976\n",
            "[24,   560] loss: 132520.691528\n",
            "[25,   560] loss: 132764.841947\n",
            "[26,   560] loss: 132574.277264\n",
            "[27,   560] loss: 132591.214722\n",
            "[28,   560] loss: 132970.030619\n",
            "[29,   560] loss: 132898.101723\n",
            "[30,   560] loss: 132568.895710\n",
            "[31,   560] loss: 132610.792773\n",
            "[32,   560] loss: 132855.885456\n",
            "[33,   560] loss: 132909.425879\n",
            "[34,   560] loss: 132792.113449\n",
            "[35,   560] loss: 132471.913658\n",
            "[36,   560] loss: 132672.874135\n",
            "[37,   560] loss: 132753.692728\n",
            "[38,   560] loss: 132795.727836\n",
            "[39,   560] loss: 132729.522353\n",
            "[40,   560] loss: 133037.012340\n",
            "[41,   560] loss: 132835.669580\n",
            "[42,   560] loss: 132581.182663\n",
            "[43,   560] loss: 132762.829712\n",
            "[44,   560] loss: 132535.237762\n",
            "[45,   560] loss: 130425.664223\n",
            "[46,   560] loss: 132765.811572\n",
            "[47,   560] loss: 132512.062936\n",
            "[48,   560] loss: 132738.142024\n",
            "[49,   560] loss: 132810.959190\n",
            "[50,   560] loss: 132356.294629\n",
            "Finished Training\n",
            "Avg Loss during Testing -  6294.461409897627\n",
            "L2 values -  [3084.206892969049, 6333.156765750574, 6335.233910077019]\n",
            "L2 values -  [3084.206892969049, 3312.844593140069, 6294.461409897627]\n",
            "Sparsity in fc1.weight: 54.38%\n",
            "Sparsity in fc2.weight: 92.19%\n",
            "Sparsity in fc3.weight: 0.00%\n",
            "Global sparsity: 60.05%\n",
            "Avg Loss during Testing -  3162.526452024732\n",
            "[1,   560] loss: 60935.568162\n",
            "[2,   560] loss: 56915.292864\n",
            "[3,   560] loss: 56338.522402\n",
            "[4,   560] loss: 55900.627529\n",
            "[5,   560] loss: 55978.965712\n",
            "[6,   560] loss: 54097.257345\n",
            "[7,   560] loss: 50229.226817\n",
            "[8,   560] loss: 50499.954112\n",
            "[9,   560] loss: 49712.491856\n",
            "[10,   560] loss: 48443.817219\n",
            "[11,   560] loss: 48292.658500\n",
            "[12,   560] loss: 48612.026484\n",
            "[13,   560] loss: 47777.576857\n",
            "[14,   560] loss: 47377.486040\n",
            "[15,   560] loss: 46844.835726\n",
            "[16,   560] loss: 46062.314345\n",
            "[17,   560] loss: 46499.092128\n",
            "[18,   560] loss: 46185.081796\n",
            "[19,   560] loss: 46553.300480\n",
            "[20,   560] loss: 45664.347220\n",
            "[21,   560] loss: 44556.437981\n",
            "[22,   560] loss: 44571.035404\n",
            "[23,   560] loss: 44606.803008\n",
            "[24,   560] loss: 44358.911094\n",
            "[25,   560] loss: 44028.054007\n",
            "[26,   560] loss: 44073.569683\n",
            "[27,   560] loss: 43206.945475\n",
            "[28,   560] loss: 43406.186871\n",
            "[29,   560] loss: 43470.187744\n",
            "[30,   560] loss: 43416.979018\n",
            "[31,   560] loss: 43589.132978\n",
            "[32,   560] loss: 44577.025403\n",
            "[33,   560] loss: 44071.729539\n",
            "[34,   560] loss: 43045.287017\n",
            "[35,   560] loss: 43725.878414\n",
            "[36,   560] loss: 43012.953927\n",
            "[37,   560] loss: 44454.338953\n",
            "[38,   560] loss: 43787.095706\n",
            "[39,   560] loss: 43538.846308\n",
            "[40,   560] loss: 44135.183057\n",
            "[41,   560] loss: 43085.538201\n",
            "[42,   560] loss: 43032.247855\n",
            "[43,   560] loss: 43079.793511\n",
            "[44,   560] loss: 41679.842646\n",
            "[45,   560] loss: 42884.510910\n",
            "[46,   560] loss: 43618.770642\n",
            "[47,   560] loss: 42877.627621\n",
            "[48,   560] loss: 43945.009080\n",
            "[49,   560] loss: 43061.352963\n",
            "[50,   560] loss: 42637.923288\n",
            "Finished Training\n",
            "Avg Loss during Testing -  1598.687015642939\n",
            "L1 values -  [3084.206892969049, 3071.4166165865386, 2921.015459062859, 3162.526452024732]\n",
            "L1 values -  [3084.206892969049, 2794.745364972015, 2877.615210456133, 1598.687015642939]\n",
            "Sparsity in fc1.weight: 60.31%\n",
            "Sparsity in fc2.weight: 56.25%\n",
            "Sparsity in fc3.weight: 100.00%\n",
            "Global sparsity: 60.05%\n",
            "Avg Loss during Testing -  12067.471976057213\n",
            "[1,   560] loss: 221654.922419\n",
            "[2,   560] loss: 194476.471924\n",
            "[3,   560] loss: 173303.699100\n",
            "[4,   560] loss: 158543.412026\n",
            "[5,   560] loss: 147296.616082\n",
            "[6,   560] loss: 140699.686771\n",
            "[7,   560] loss: 136500.436412\n",
            "[8,   560] loss: 134145.570121\n",
            "[9,   560] loss: 133291.390733\n",
            "[10,   560] loss: 133104.244775\n",
            "[11,   560] loss: 132923.740039\n",
            "[12,   560] loss: 132933.577731\n",
            "[13,   560] loss: 132948.187395\n",
            "[14,   560] loss: 132894.399400\n",
            "[15,   560] loss: 132626.535704\n",
            "[16,   560] loss: 132749.639038\n",
            "[17,   560] loss: 132520.788201\n",
            "[18,   560] loss: 132687.514174\n",
            "[19,   560] loss: 132585.915705\n",
            "[20,   560] loss: 132983.884609\n",
            "[21,   560] loss: 132306.542697\n",
            "[22,   560] loss: 132270.192097\n",
            "[23,   560] loss: 132711.413124\n",
            "[24,   560] loss: 132772.887981\n",
            "[25,   560] loss: 132764.137395\n",
            "[26,   560] loss: 132762.633583\n",
            "[27,   560] loss: 132400.939858\n",
            "[28,   560] loss: 133002.577769\n",
            "[29,   560] loss: 130659.678505\n",
            "[30,   560] loss: 132756.109072\n",
            "[31,   560] loss: 132926.777316\n",
            "[32,   560] loss: 131994.540112\n",
            "[33,   560] loss: 132944.958740\n",
            "[34,   560] loss: 132829.432234\n",
            "[35,   560] loss: 132840.178418\n",
            "[36,   560] loss: 132410.531651\n",
            "[37,   560] loss: 132729.793876\n",
            "[38,   560] loss: 132727.399097\n",
            "[39,   560] loss: 132859.569329\n",
            "[40,   560] loss: 132821.953770\n",
            "[41,   560] loss: 132467.539094\n",
            "[42,   560] loss: 132774.482851\n",
            "[43,   560] loss: 132841.518101\n",
            "[44,   560] loss: 132790.346722\n",
            "[45,   560] loss: 132883.623957\n",
            "[46,   560] loss: 132793.343098\n",
            "[47,   560] loss: 132845.710655\n",
            "[48,   560] loss: 132747.873239\n",
            "[49,   560] loss: 132938.954572\n",
            "[50,   560] loss: 132752.960226\n",
            "Finished Training\n",
            "Avg Loss during Testing -  6305.679976021335\n",
            "L2 values -  [3084.206892969049, 6333.156765750574, 6335.233910077019, 12067.471976057213]\n",
            "L2 values -  [3084.206892969049, 3312.844593140069, 6294.461409897627, 6305.679976021335]\n",
            "Sparsity in fc1.weight: 76.88%\n",
            "Sparsity in fc2.weight: 100.00%\n",
            "Sparsity in fc3.weight: 0.00%\n",
            "Global sparsity: 79.90%\n",
            "Avg Loss during Testing -  7082.999798184558\n",
            "[1,   560] loss: 134784.446899\n",
            "[2,   560] loss: 131927.983744\n",
            "[3,   560] loss: 132930.488006\n",
            "[4,   560] loss: 132946.183866\n",
            "[5,   560] loss: 132820.109849\n",
            "[6,   560] loss: 132647.613452\n",
            "[7,   560] loss: 130138.140015\n",
            "[8,   560] loss: 132673.922681\n",
            "[9,   560] loss: 132485.653418\n",
            "[10,   560] loss: 132472.080636\n",
            "[11,   560] loss: 132584.764286\n",
            "[12,   560] loss: 132882.164868\n",
            "[13,   560] loss: 132634.756494\n",
            "[14,   560] loss: 131599.457614\n",
            "[15,   560] loss: 132875.658824\n",
            "[16,   560] loss: 132568.564387\n",
            "[17,   560] loss: 132922.221394\n",
            "[18,   560] loss: 132849.594210\n",
            "[19,   560] loss: 132682.430312\n",
            "[20,   560] loss: 132702.843715\n",
            "[21,   560] loss: 132587.633772\n",
            "[22,   560] loss: 132878.510631\n",
            "[23,   560] loss: 132834.694835\n",
            "[24,   560] loss: 132741.295585\n",
            "[25,   560] loss: 132185.622684\n",
            "[26,   560] loss: 132222.027061\n",
            "[27,   560] loss: 132549.259455\n",
            "[28,   560] loss: 133035.172642\n",
            "[29,   560] loss: 132855.663267\n",
            "[30,   560] loss: 132740.456805\n",
            "[31,   560] loss: 132445.384950\n",
            "[32,   560] loss: 132863.115294\n",
            "[33,   560] loss: 132698.853156\n",
            "[34,   560] loss: 132978.495040\n",
            "[35,   560] loss: 132593.211579\n",
            "[36,   560] loss: 132934.898012\n",
            "[37,   560] loss: 132879.050408\n",
            "[38,   560] loss: 132581.361565\n",
            "[39,   560] loss: 132469.373403\n",
            "[40,   560] loss: 132720.689045\n",
            "[41,   560] loss: 131826.308078\n",
            "[42,   560] loss: 132872.383517\n",
            "[43,   560] loss: 132544.068952\n",
            "[44,   560] loss: 132838.624620\n",
            "[45,   560] loss: 132527.289708\n",
            "[46,   560] loss: 132511.114694\n",
            "[47,   560] loss: 132485.810714\n",
            "[48,   560] loss: 132513.341745\n",
            "[49,   560] loss: 132553.824128\n",
            "[50,   560] loss: 132688.807928\n",
            "Finished Training\n",
            "Avg Loss during Testing -  6322.3426646814005\n",
            "L1 values -  [3084.206892969049, 3071.4166165865386, 2921.015459062859, 3162.526452024732, 7082.999798184558]\n",
            "L1 values -  [3084.206892969049, 2794.745364972015, 2877.615210456133, 1598.687015642939, 6322.3426646814005]\n",
            "Sparsity in fc1.weight: 80.94%\n",
            "Sparsity in fc2.weight: 76.56%\n",
            "Sparsity in fc3.weight: 50.00%\n",
            "Global sparsity: 79.90%\n",
            "Avg Loss during Testing -  19641.623457233065\n",
            "[1,   560] loss: 323714.534612\n",
            "[2,   560] loss: 220403.849275\n",
            "[3,   560] loss: 166494.162524\n",
            "[4,   560] loss: 143001.608283\n",
            "[5,   560] loss: 135089.934141\n",
            "[6,   560] loss: 133035.207997\n",
            "[7,   560] loss: 132643.103864\n",
            "[8,   560] loss: 132544.217139\n",
            "[9,   560] loss: 132841.470843\n",
            "[10,   560] loss: 132931.692132\n",
            "[11,   560] loss: 132425.171523\n",
            "[12,   560] loss: 132709.944817\n",
            "[13,   560] loss: 132479.865831\n",
            "[14,   560] loss: 132574.594270\n",
            "[15,   560] loss: 132607.751845\n",
            "[16,   560] loss: 132872.223343\n",
            "[17,   560] loss: 132960.126147\n",
            "[18,   560] loss: 132827.718066\n",
            "[19,   560] loss: 132937.316462\n",
            "[20,   560] loss: 132674.224568\n",
            "[21,   560] loss: 132603.720654\n",
            "[22,   560] loss: 132424.249627\n",
            "[23,   560] loss: 132941.800150\n",
            "[24,   560] loss: 132948.594741\n",
            "[25,   560] loss: 132563.035052\n",
            "[26,   560] loss: 132610.450014\n",
            "[27,   560] loss: 132294.745292\n",
            "[28,   560] loss: 132852.343809\n",
            "[29,   560] loss: 132560.794807\n",
            "[30,   560] loss: 132768.416068\n",
            "[31,   560] loss: 132160.871526\n",
            "[32,   560] loss: 132909.187992\n",
            "[33,   560] loss: 132788.389118\n",
            "[34,   560] loss: 133027.800084\n",
            "[35,   560] loss: 132026.257345\n",
            "[36,   560] loss: 132330.745016\n",
            "[37,   560] loss: 131867.035372\n",
            "[38,   560] loss: 132632.262092\n",
            "[39,   560] loss: 132954.507764\n",
            "[40,   560] loss: 132776.355580\n",
            "[41,   560] loss: 132948.205587\n",
            "[42,   560] loss: 132885.116466\n",
            "[43,   560] loss: 132563.991061\n",
            "[44,   560] loss: 132539.769458\n",
            "[45,   560] loss: 131976.352403\n",
            "[46,   560] loss: 132315.132328\n",
            "[47,   560] loss: 132804.371784\n",
            "[48,   560] loss: 132854.425579\n",
            "[49,   560] loss: 132969.360223\n",
            "[50,   560] loss: 132859.932621\n",
            "Finished Training\n",
            "Avg Loss during Testing -  6326.821536847015\n",
            "L2 values -  [3084.206892969049, 6333.156765750574, 6335.233910077019, 12067.471976057213, 19641.623457233065]\n",
            "L2 values -  [3084.206892969049, 3312.844593140069, 6294.461409897627, 6305.679976021335, 6326.821536847015]\n",
            "Sparsity in fc1.weight: 89.06%\n",
            "Sparsity in fc2.weight: 100.00%\n",
            "Sparsity in fc3.weight: 0.00%\n",
            "Global sparsity: 89.95%\n",
            "Avg Loss during Testing -  7085.032605900785\n",
            "[1,   560] loss: 134326.420225\n",
            "[2,   560] loss: 131683.276723\n",
            "[3,   560] loss: 132816.547562\n",
            "[4,   560] loss: 133009.709706\n",
            "[5,   560] loss: 132744.706376\n",
            "[6,   560] loss: 132963.710146\n",
            "[7,   560] loss: 131599.303523\n",
            "[8,   560] loss: 132898.371858\n",
            "[9,   560] loss: 132296.953369\n",
            "[10,   560] loss: 132572.790273\n",
            "[11,   560] loss: 132881.030123\n",
            "[12,   560] loss: 132844.041239\n",
            "[13,   560] loss: 132743.807917\n",
            "[14,   560] loss: 133045.320592\n",
            "[15,   560] loss: 132641.175921\n",
            "[16,   560] loss: 130754.452183\n",
            "[17,   560] loss: 132132.401039\n",
            "[18,   560] loss: 132823.928076\n",
            "[19,   560] loss: 132819.260931\n",
            "[20,   560] loss: 132842.414638\n",
            "[21,   560] loss: 130908.989432\n",
            "[22,   560] loss: 131993.462284\n",
            "[23,   560] loss: 132585.456770\n",
            "[24,   560] loss: 132633.406794\n",
            "[25,   560] loss: 132691.477260\n",
            "[26,   560] loss: 132819.245884\n",
            "[27,   560] loss: 132943.259298\n",
            "[28,   560] loss: 132744.780831\n",
            "[29,   560] loss: 132132.014851\n",
            "[30,   560] loss: 132737.383915\n",
            "[31,   560] loss: 132641.227933\n",
            "[32,   560] loss: 132243.557342\n",
            "[33,   560] loss: 132419.102309\n",
            "[34,   560] loss: 132391.594985\n",
            "[35,   560] loss: 132109.503700\n",
            "[36,   560] loss: 132934.368052\n",
            "[37,   560] loss: 131994.846599\n",
            "[38,   560] loss: 132903.644573\n",
            "[39,   560] loss: 132116.632718\n",
            "[40,   560] loss: 132844.102068\n",
            "[41,   560] loss: 132719.814938\n",
            "[42,   560] loss: 132755.384504\n",
            "[43,   560] loss: 132927.491835\n",
            "[44,   560] loss: 132344.969116\n",
            "[45,   560] loss: 132432.714369\n",
            "[46,   560] loss: 133038.027190\n",
            "[47,   560] loss: 132832.368338\n",
            "[48,   560] loss: 132876.704600\n",
            "[49,   560] loss: 132564.614010\n",
            "[50,   560] loss: 132435.149519\n",
            "Finished Training\n",
            "Avg Loss during Testing -  6317.369332723402\n",
            "L1 values -  [3084.206892969049, 3071.4166165865386, 2921.015459062859, 3162.526452024732, 7082.999798184558, 7085.032605900785]\n",
            "L1 values -  [3084.206892969049, 2794.745364972015, 2877.615210456133, 1598.687015642939, 6322.3426646814005, 6317.369332723402]\n",
            "Sparsity in fc1.weight: 90.31%\n",
            "Sparsity in fc2.weight: 87.50%\n",
            "Sparsity in fc3.weight: 100.00%\n",
            "Global sparsity: 89.95%\n",
            "Avg Loss during Testing -  12070.307178052048\n",
            "[1,   560] loss: 221077.416190\n",
            "[2,   560] loss: 193898.917843\n",
            "[3,   560] loss: 173229.905692\n",
            "[4,   560] loss: 157707.650834\n",
            "[5,   560] loss: 147673.587221\n",
            "[6,   560] loss: 140915.472248\n",
            "[7,   560] loss: 136346.334633\n",
            "[8,   560] loss: 134522.901587\n",
            "[9,   560] loss: 133414.084773\n",
            "[10,   560] loss: 133003.184312\n",
            "[11,   560] loss: 132926.433583\n",
            "[12,   560] loss: 132769.296090\n",
            "[13,   560] loss: 132728.177563\n",
            "[14,   560] loss: 132916.980110\n",
            "[15,   560] loss: 132114.186087\n",
            "[16,   560] loss: 132920.010753\n",
            "[17,   560] loss: 132776.394026\n",
            "[18,   560] loss: 132869.081313\n",
            "[19,   560] loss: 132611.331533\n",
            "[20,   560] loss: 132094.157244\n",
            "[21,   560] loss: 132846.932220\n",
            "[22,   560] loss: 132737.544807\n",
            "[23,   560] loss: 132429.572754\n",
            "[24,   560] loss: 132794.419577\n",
            "[25,   560] loss: 132625.135041\n",
            "[26,   560] loss: 132775.499330\n",
            "[27,   560] loss: 132694.246150\n",
            "[28,   560] loss: 132090.031037\n",
            "[29,   560] loss: 132718.089530\n",
            "[30,   560] loss: 132717.865475\n",
            "[31,   560] loss: 132356.046690\n",
            "[32,   560] loss: 132866.968890\n",
            "[33,   560] loss: 132714.175377\n",
            "[34,   560] loss: 132901.628603\n",
            "[35,   560] loss: 132341.545323\n",
            "[36,   560] loss: 131072.623884\n",
            "[37,   560] loss: 132636.711419\n",
            "[38,   560] loss: 132949.055165\n",
            "[39,   560] loss: 131926.681180\n",
            "[40,   560] loss: 132882.716828\n",
            "[41,   560] loss: 132735.334138\n",
            "[42,   560] loss: 132805.660107\n",
            "[43,   560] loss: 132558.624498\n",
            "[44,   560] loss: 131907.543534\n",
            "[45,   560] loss: 132945.355179\n",
            "[46,   560] loss: 132770.547046\n",
            "[47,   560] loss: 132521.560854\n",
            "[48,   560] loss: 131784.633639\n",
            "[49,   560] loss: 132823.058960\n",
            "[50,   560] loss: 132923.189924\n",
            "Finished Training\n",
            "Avg Loss during Testing -  6308.038433134807\n",
            "L2 values -  [3084.206892969049, 6333.156765750574, 6335.233910077019, 12067.471976057213, 19641.623457233065, 12070.307178052048]\n",
            "L2 values -  [3084.206892969049, 3312.844593140069, 6294.461409897627, 6305.679976021335, 6326.821536847015, 6308.038433134807]\n"
          ]
        }
      ],
      "source": [
        "# Full cycle\n",
        "\n",
        "parameters_to_prune = (\n",
        "    (net.fc1, 'weight'),\n",
        "    (net.fc2, 'weight'),\n",
        "    (net.fc3, 'weight'),\n",
        ")\n",
        "\n",
        "batch_loss = 560\n",
        "epoch = 50\n",
        "\n",
        "results_base_l1 = []\n",
        "results_finetune_l1 = []\n",
        "\n",
        "results_base_l2 = []\n",
        "results_finetune_l2 = []\n",
        "\n",
        "\n",
        "# Network\n",
        "net = Net().to(device)\n",
        "print_sparsity(net)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.1)\n",
        "\n",
        "train(net, train_loader, epoch, optimizer, criterion, batch_loss)\n",
        "loss_mse = test(net, test_loader, criterion)\n",
        "\n",
        "results_base_l1.append(loss_mse)\n",
        "results_finetune_l1.append(loss_mse)\n",
        "results_base_l2.append(loss_mse)\n",
        "results_finetune_l2.append(loss_mse)\n",
        "\n",
        "PATH = './cifar_net.pth'\n",
        "torch.save(net.state_dict(), PATH)\n",
        "\n",
        "prune_values = [0.2, 0.4, 0.6, 0.8, 0.9]\n",
        "pruning_methods = [0,1]\n",
        "\n",
        "\n",
        "for prune_value in prune_values:\n",
        "  for pruning_method in pruning_methods:\n",
        "  # Each prune value experiment is independent\n",
        "    net = Net().to(device)\n",
        "    net.load_state_dict(torch.load(PATH))\n",
        "    \n",
        "    parameters_to_prune = (\n",
        "      (net.fc1, 'weight'),\n",
        "      (net.fc2, 'weight'),\n",
        "      (net.fc3, 'weight'),\n",
        "    )\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.1)\n",
        "    \n",
        "    if pruning_method == 0:\n",
        "      prune.global_unstructured(\n",
        "          parameters_to_prune,\n",
        "          pruning_method=prune.L1Unstructured,\n",
        "          amount=prune_value,\n",
        "      )\n",
        "    else:\n",
        "      prune.global_unstructured(\n",
        "          parameters_to_prune,\n",
        "          pruning_method=prune.RandomUnstructured,\n",
        "          amount=prune_value,\n",
        "      )\n",
        "\n",
        "    print_sparsity(net)\n",
        "    \n",
        "    base_loss = test(net, test_loader, criterion)\n",
        "\n",
        "    train(net, train_loader, epoch, optimizer, criterion, batch_loss)\n",
        "    finetune_loss = test(net, test_loader, criterion)\n",
        "    \n",
        "    if pruning_method == 0:\n",
        "      results_base_l1.append(base_loss)\n",
        "      results_finetune_l1.append(finetune_loss)\n",
        "\n",
        "      print('L1 values - ', results_base_l1)\n",
        "      print('L1 values - ', results_finetune_l1)\n",
        "    else:\n",
        "      results_base_l2.append(base_loss)\n",
        "      results_finetune_l2.append(finetune_loss)\n",
        "\n",
        "      print('L2 values - ', results_base_l2)\n",
        "      print('L2 values - ', results_finetune_l2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cFnPtoKpUea6",
        "outputId": "ae497341-6a52-42c3-cebf-476b6920b1b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[3084.206892969049,\n",
              " 3071.4166165865386,\n",
              " 2921.015459062859,\n",
              " 3162.526452024732,\n",
              " 7082.999798184558,\n",
              " 7085.032605900785]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_base_l1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oI_HhRKGUhz7",
        "outputId": "4560d4eb-b7ee-4866-c463-87ade90b9a64"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[3084.206892969049,\n",
              " 2794.745364972015,\n",
              " 2877.615210456133,\n",
              " 1598.687015642939,\n",
              " 6322.3426646814005,\n",
              " 6317.369332723402]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_finetune_l1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "S3cUedcYlVkn",
        "outputId": "af79387e-fdac-46c8-8f8d-c1ba7cfab43c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[3084.206892969049,\n",
              " 6333.156765750574,\n",
              " 6335.233910077019,\n",
              " 12067.471976057213,\n",
              " 19641.623457233065,\n",
              " 12070.307178052048]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_base_l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-ANhmuuulWTZ",
        "outputId": "ddc86bcd-9e57-4f3c-a1d3-07268ec8ecdc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[3084.206892969049,\n",
              " 3312.844593140069,\n",
              " 6294.461409897627,\n",
              " 6305.679976021335,\n",
              " 6326.821536847015,\n",
              " 6308.038433134807]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_finetune_l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAE75a6IYIpt"
      },
      "outputs": [],
      "source": [
        "x_coord = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "mLvJ6Ue-Uk31",
        "outputId": "32149d3f-6510-45ba-eff5-6d65e6ac223e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('Housing Dataset with 3 layer NN and different pruning techniques')\n",
        "plt.plot(x_coord, results_base_l1, label = 'L1 without finetuning')\n",
        "plt.plot(x_coord, results_finetune_l1, label = 'L1 finetuned')\n",
        "plt.plot(x_coord, results_base_l2, label = 'Random without finetuning')\n",
        "plt.plot(x_coord, results_finetune_l2, label = 'Random finetuned')\n",
        "plt.ylabel('MSE loss')\n",
        "plt.xlabel('Sparsity')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68gqkcHomsJE"
      },
      "source": [
        "### 5 layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pj6NeLkdmsJF"
      },
      "outputs": [],
      "source": [
        "def print_sparsity(net):\n",
        "  print(\n",
        "      \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc1.weight == 0))\n",
        "          / float(net.fc1.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Sparsity in fc2.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc2.weight == 0))\n",
        "          / float(net.fc2.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Sparsity in fc3.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc3.weight == 0))\n",
        "          / float(net.fc3.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Sparsity in fc4.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc4.weight == 0))\n",
        "          / float(net.fc4.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Sparsity in fc5.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc5.weight == 0))\n",
        "          / float(net.fc5.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Global sparsity: {:.2f}%\".format(\n",
        "          100. * float(\n",
        "              + torch.sum(net.fc1.weight == 0)\n",
        "              + torch.sum(net.fc2.weight == 0)\n",
        "              + torch.sum(net.fc3.weight == 0)\n",
        "              + torch.sum(net.fc4.weight == 0)\n",
        "              + torch.sum(net.fc5.weight == 0)\n",
        "          )\n",
        "          / float(\n",
        "              + net.fc1.weight.nelement()\n",
        "              + net.fc2.weight.nelement()\n",
        "              + net.fc3.weight.nelement()\n",
        "              + net.fc4.weight.nelement()\n",
        "              + net.fc5.weight.nelement()\n",
        "          )\n",
        "      )\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W83uStXimsJF"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(20, 16)\n",
        "        self.fc2 = nn.Linear(16, 16)\n",
        "        self.fc3 = nn.Linear(16, 8)\n",
        "        self.fc4 = nn.Linear(8, 4)\n",
        "        self.fc5 = nn.Linear(4, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x)) \n",
        "        x = torch.tanh(self.fc3(x))\n",
        "        x = torch.tanh(self.fc4(x))\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "net = Net().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Tum6muImsJG"
      },
      "outputs": [],
      "source": [
        "# # criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.MSELoss()\n",
        "# # optimizer = optim.SGD(net.parameters(), lr=10)\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0Cu78o5msJG",
        "outputId": "5da82874-6415-4ef2-ae73-80c5aa196d76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Avg Loss during Testing -  20563.5038090796\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "20563.5038090796"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Testing before training\n",
        "test(net, test_loader, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7BA2K2XmsJH",
        "outputId": "a2154960-40cd-4868-f93e-6791659f65d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sparsity in fc1.weight: 0.00%\n",
            "Sparsity in fc2.weight: 0.00%\n",
            "Sparsity in fc3.weight: 0.00%\n",
            "Sparsity in fc4.weight: 0.00%\n",
            "Sparsity in fc5.weight: 0.00%\n",
            "Global sparsity: 0.00%\n",
            "[1,   560] loss: 300980.459431\n",
            "[2,   560] loss: 174882.238494\n",
            "[3,   560] loss: 139353.537413\n",
            "[4,   560] loss: 132792.285931\n",
            "[5,   560] loss: 129940.992815\n",
            "[6,   560] loss: 101811.947642\n",
            "[7,   560] loss: 94892.837228\n",
            "[8,   560] loss: 89747.465548\n",
            "[9,   560] loss: 88538.374803\n",
            "[10,   560] loss: 85692.721995\n",
            "[11,   560] loss: 82387.190761\n",
            "[12,   560] loss: 81546.869918\n",
            "[13,   560] loss: 80957.576357\n",
            "[14,   560] loss: 76214.250963\n",
            "[15,   560] loss: 73841.818269\n",
            "[16,   560] loss: 73040.120537\n",
            "[17,   560] loss: 80155.522780\n",
            "[18,   560] loss: 80305.074789\n",
            "[19,   560] loss: 80466.552543\n",
            "[20,   560] loss: 81626.449226\n",
            "[21,   560] loss: 78184.990941\n",
            "[22,   560] loss: 70973.553913\n",
            "[23,   560] loss: 75925.043251\n",
            "[24,   560] loss: 81914.758015\n",
            "[25,   560] loss: 82915.583566\n",
            "[26,   560] loss: 82367.430047\n",
            "[27,   560] loss: 80961.396615\n",
            "[28,   560] loss: 81375.806648\n",
            "[29,   560] loss: 84907.110505\n",
            "[30,   560] loss: 87274.229276\n",
            "[31,   560] loss: 87841.342097\n",
            "[32,   560] loss: 87174.566026\n",
            "[33,   560] loss: 87726.275056\n",
            "[34,   560] loss: 87040.899411\n",
            "[35,   560] loss: 87006.978868\n",
            "[36,   560] loss: 91126.557038\n",
            "[37,   560] loss: 97474.167226\n",
            "[38,   560] loss: 101752.496181\n",
            "[39,   560] loss: 98518.903927\n",
            "[40,   560] loss: 98141.959966\n",
            "[41,   560] loss: 98412.985863\n",
            "[42,   560] loss: 94782.410261\n",
            "[43,   560] loss: 91541.988794\n",
            "[44,   560] loss: 89477.392644\n",
            "[45,   560] loss: 94989.099119\n",
            "[46,   560] loss: 91960.708208\n",
            "[47,   560] loss: 94557.532711\n",
            "[48,   560] loss: 92547.266996\n",
            "[49,   560] loss: 91761.780922\n",
            "[50,   560] loss: 92495.713710\n",
            "Finished Training\n",
            "Avg Loss during Testing -  4367.997509447952\n",
            "Sparsity in fc1.weight: 21.25%\n",
            "Sparsity in fc2.weight: 17.58%\n",
            "Sparsity in fc3.weight: 17.97%\n",
            "Sparsity in fc4.weight: 37.50%\n",
            "Sparsity in fc5.weight: 0.00%\n",
            "Global sparsity: 20.00%\n",
            "Avg Loss during Testing -  4538.163779210917\n",
            "[1,   560] loss: 95490.870386\n",
            "[2,   560] loss: 99705.934738\n",
            "[3,   560] loss: 99313.956058\n",
            "[4,   560] loss: 91563.431252\n",
            "[5,   560] loss: 86203.181013\n",
            "[6,   560] loss: 88137.717770\n",
            "[7,   560] loss: 93892.287350\n",
            "[8,   560] loss: 94857.654318\n",
            "[9,   560] loss: 95233.389771\n",
            "[10,   560] loss: 97854.079271\n",
            "[11,   560] loss: 101819.081097\n",
            "[12,   560] loss: 103122.458517\n",
            "[13,   560] loss: 101515.185993\n",
            "[14,   560] loss: 98273.774892\n",
            "[15,   560] loss: 101033.185146\n",
            "[16,   560] loss: 100966.352354\n",
            "[17,   560] loss: 100731.201282\n",
            "[18,   560] loss: 101324.725971\n",
            "[19,   560] loss: 101745.996840\n",
            "[20,   560] loss: 102144.618736\n",
            "[21,   560] loss: 102380.897667\n",
            "[22,   560] loss: 101469.950174\n",
            "[23,   560] loss: 102269.078373\n",
            "[24,   560] loss: 102316.554914\n",
            "[25,   560] loss: 102009.074201\n",
            "[26,   560] loss: 102099.204951\n",
            "[27,   560] loss: 104857.575352\n",
            "[28,   560] loss: 105612.815555\n",
            "[29,   560] loss: 106882.638661\n",
            "[30,   560] loss: 106910.299161\n",
            "[31,   560] loss: 105792.632812\n",
            "[32,   560] loss: 105343.993712\n",
            "[33,   560] loss: 105158.972838\n",
            "[34,   560] loss: 105633.292780\n",
            "[35,   560] loss: 105593.404025\n",
            "[36,   560] loss: 106544.991263\n",
            "[37,   560] loss: 113379.465262\n",
            "[38,   560] loss: 112018.193188\n",
            "[39,   560] loss: 111965.863871\n",
            "[40,   560] loss: 111819.961230\n",
            "[41,   560] loss: 112026.579182\n",
            "[42,   560] loss: 112020.489634\n",
            "[43,   560] loss: 111750.877225\n",
            "[44,   560] loss: 112061.152389\n",
            "[45,   560] loss: 111425.406996\n",
            "[46,   560] loss: 111727.726311\n",
            "[47,   560] loss: 112067.620079\n",
            "[48,   560] loss: 111823.872395\n",
            "[49,   560] loss: 111986.305127\n",
            "[50,   560] loss: 112057.228627\n",
            "Finished Training\n",
            "Avg Loss during Testing -  5731.2301682692305\n",
            "L1 values -  [4367.997509447952, 4538.163779210917]\n",
            "L1 values -  [4367.997509447952, 5731.2301682692305]\n",
            "Sparsity in fc1.weight: 20.00%\n",
            "Sparsity in fc2.weight: 19.53%\n",
            "Sparsity in fc3.weight: 19.53%\n",
            "Sparsity in fc4.weight: 25.00%\n",
            "Sparsity in fc5.weight: 25.00%\n",
            "Global sparsity: 20.00%\n",
            "Avg Loss during Testing -  7823.647266970436\n",
            "[1,   560] loss: 116288.401294\n",
            "[2,   560] loss: 92528.470016\n",
            "[3,   560] loss: 92038.095004\n",
            "[4,   560] loss: 88096.084333\n",
            "[5,   560] loss: 81194.068129\n",
            "[6,   560] loss: 81333.076719\n",
            "[7,   560] loss: 81200.808589\n",
            "[8,   560] loss: 82689.668928\n",
            "[9,   560] loss: 82065.963642\n",
            "[10,   560] loss: 82359.615765\n",
            "[11,   560] loss: 81265.962844\n",
            "[12,   560] loss: 81726.624203\n",
            "[13,   560] loss: 81285.507209\n",
            "[14,   560] loss: 79875.057124\n",
            "[15,   560] loss: 78971.895142\n",
            "[16,   560] loss: 79138.667275\n",
            "[17,   560] loss: 79650.827314\n",
            "[18,   560] loss: 79823.461778\n",
            "[19,   560] loss: 77304.392732\n",
            "[20,   560] loss: 77798.480334\n",
            "[21,   560] loss: 75269.345592\n",
            "[22,   560] loss: 77675.788450\n",
            "[23,   560] loss: 75363.842320\n",
            "[24,   560] loss: 68869.956022\n",
            "[25,   560] loss: 69049.165325\n",
            "[26,   560] loss: 70704.591303\n",
            "[27,   560] loss: 69771.971150\n",
            "[28,   560] loss: 69356.432445\n",
            "[29,   560] loss: 69632.086536\n",
            "[30,   560] loss: 69682.282030\n",
            "[31,   560] loss: 70906.677682\n",
            "[32,   560] loss: 71499.797021\n",
            "[33,   560] loss: 82253.394578\n",
            "[34,   560] loss: 73604.627548\n",
            "[35,   560] loss: 71345.943408\n",
            "[36,   560] loss: 71271.169618\n",
            "[37,   560] loss: 80742.907495\n",
            "[38,   560] loss: 83808.905650\n",
            "[39,   560] loss: 89127.747189\n",
            "[40,   560] loss: 85325.861304\n",
            "[41,   560] loss: 86480.678069\n",
            "[42,   560] loss: 96779.692058\n",
            "[43,   560] loss: 126375.138703\n",
            "[44,   560] loss: 120389.951838\n",
            "[45,   560] loss: 113567.042048\n",
            "[46,   560] loss: 109037.259794\n",
            "[47,   560] loss: 102827.020808\n",
            "[48,   560] loss: 101398.191971\n",
            "[49,   560] loss: 100843.921408\n",
            "[50,   560] loss: 101053.579492\n",
            "Finished Training\n",
            "Avg Loss during Testing -  4704.056339396766\n",
            "L2 values -  [4367.997509447952, 7823.647266970436]\n",
            "L2 values -  [4367.997509447952, 4704.056339396766]\n",
            "Sparsity in fc1.weight: 40.00%\n",
            "Sparsity in fc2.weight: 39.45%\n",
            "Sparsity in fc3.weight: 35.16%\n",
            "Sparsity in fc4.weight: 68.75%\n",
            "Sparsity in fc5.weight: 0.00%\n",
            "Global sparsity: 40.00%\n",
            "Avg Loss during Testing -  4616.783823520618\n",
            "[1,   560] loss: 94549.509910\n",
            "[2,   560] loss: 96302.573769\n",
            "[3,   560] loss: 98906.896395\n",
            "[4,   560] loss: 100007.972185\n",
            "[5,   560] loss: 99131.153184\n",
            "[6,   560] loss: 103152.142889\n",
            "[7,   560] loss: 103255.172831\n",
            "[8,   560] loss: 103285.054060\n",
            "[9,   560] loss: 103238.432272\n",
            "[10,   560] loss: 102907.368311\n",
            "[11,   560] loss: 103100.771781\n",
            "[12,   560] loss: 103132.229243\n",
            "[13,   560] loss: 103032.753160\n",
            "[14,   560] loss: 103124.484947\n",
            "[15,   560] loss: 103190.075122\n",
            "[16,   560] loss: 104026.219423\n",
            "[17,   560] loss: 104695.879768\n",
            "[18,   560] loss: 104753.825457\n",
            "[19,   560] loss: 106106.636269\n",
            "[20,   560] loss: 104203.340723\n",
            "[21,   560] loss: 103301.776203\n",
            "[22,   560] loss: 107964.360645\n",
            "[23,   560] loss: 111145.255929\n",
            "[24,   560] loss: 110933.041863\n",
            "[25,   560] loss: 111096.697004\n",
            "[26,   560] loss: 110953.462584\n",
            "[27,   560] loss: 111010.402427\n",
            "[28,   560] loss: 110412.551099\n",
            "[29,   560] loss: 111029.554597\n",
            "[30,   560] loss: 111121.593813\n",
            "[31,   560] loss: 111042.389087\n",
            "[32,   560] loss: 110359.100837\n",
            "[33,   560] loss: 108581.035505\n",
            "[34,   560] loss: 111025.675834\n",
            "[35,   560] loss: 110564.691225\n",
            "[36,   560] loss: 111088.529199\n",
            "[37,   560] loss: 111094.743052\n",
            "[38,   560] loss: 111177.049100\n",
            "[39,   560] loss: 110640.323525\n",
            "[40,   560] loss: 110522.146369\n",
            "[41,   560] loss: 110903.221446\n",
            "[42,   560] loss: 111084.906798\n",
            "[43,   560] loss: 110813.044639\n",
            "[44,   560] loss: 110735.060522\n",
            "[45,   560] loss: 107888.014847\n",
            "[46,   560] loss: 111076.975865\n",
            "[47,   560] loss: 111141.040206\n",
            "[48,   560] loss: 111140.619601\n",
            "[49,   560] loss: 110801.675045\n",
            "[50,   560] loss: 111164.456881\n",
            "Finished Training\n",
            "Avg Loss during Testing -  5417.997559528081\n",
            "L1 values -  [4367.997509447952, 4538.163779210917, 4616.783823520618]\n",
            "L1 values -  [4367.997509447952, 5731.2301682692305, 5417.997559528081]\n",
            "Sparsity in fc1.weight: 38.75%\n",
            "Sparsity in fc2.weight: 41.80%\n",
            "Sparsity in fc3.weight: 40.62%\n",
            "Sparsity in fc4.weight: 40.62%\n",
            "Sparsity in fc5.weight: 0.00%\n",
            "Global sparsity: 40.00%\n",
            "Avg Loss during Testing -  8909.276233017605\n",
            "[1,   560] loss: 125408.821770\n",
            "[2,   560] loss: 98698.254987\n",
            "[3,   560] loss: 89883.258573\n",
            "[4,   560] loss: 78964.582772\n",
            "[5,   560] loss: 74300.293122\n",
            "[6,   560] loss: 72179.273800\n",
            "[7,   560] loss: 74188.640601\n",
            "[8,   560] loss: 68962.488492\n",
            "[9,   560] loss: 69586.669978\n",
            "[10,   560] loss: 70240.892453\n",
            "[11,   560] loss: 66688.684569\n",
            "[12,   560] loss: 64155.698914\n",
            "[13,   560] loss: 63084.268127\n",
            "[14,   560] loss: 63239.381887\n",
            "[15,   560] loss: 63316.331161\n",
            "[16,   560] loss: 62570.767965\n",
            "[17,   560] loss: 59910.223289\n",
            "[18,   560] loss: 66255.249276\n",
            "[19,   560] loss: 69952.404850\n",
            "[20,   560] loss: 59908.517679\n",
            "[21,   560] loss: 59836.314326\n",
            "[22,   560] loss: 62097.073492\n",
            "[23,   560] loss: 68056.885210\n",
            "[24,   560] loss: 65854.658412\n",
            "[25,   560] loss: 66535.814830\n",
            "[26,   560] loss: 67154.464436\n",
            "[27,   560] loss: 69354.158003\n",
            "[28,   560] loss: 69571.816479\n",
            "[29,   560] loss: 64791.758407\n",
            "[30,   560] loss: 63707.224200\n",
            "[31,   560] loss: 63943.952370\n",
            "[32,   560] loss: 64168.623737\n",
            "[33,   560] loss: 61244.662347\n",
            "[34,   560] loss: 63228.790595\n",
            "[35,   560] loss: 65636.295150\n",
            "[36,   560] loss: 61481.607493\n",
            "[37,   560] loss: 73142.851627\n",
            "[38,   560] loss: 72352.566560\n",
            "[39,   560] loss: 65894.598115\n",
            "[40,   560] loss: 74545.054210\n",
            "[41,   560] loss: 70490.117416\n",
            "[42,   560] loss: 70305.671186\n",
            "[43,   560] loss: 69845.760093\n",
            "[44,   560] loss: 68865.890998\n",
            "[45,   560] loss: 69363.087967\n",
            "[46,   560] loss: 66418.539429\n",
            "[47,   560] loss: 65792.017637\n",
            "[48,   560] loss: 65769.425324\n",
            "[49,   560] loss: 65832.575424\n",
            "[50,   560] loss: 65750.163658\n",
            "Finished Training\n",
            "Avg Loss during Testing -  2872.6591793137677\n",
            "L2 values -  [4367.997509447952, 7823.647266970436, 8909.276233017605]\n",
            "L2 values -  [4367.997509447952, 4704.056339396766, 2872.6591793137677]\n",
            "Sparsity in fc1.weight: 60.00%\n",
            "Sparsity in fc2.weight: 58.59%\n",
            "Sparsity in fc3.weight: 58.59%\n",
            "Sparsity in fc4.weight: 84.38%\n",
            "Sparsity in fc5.weight: 0.00%\n",
            "Global sparsity: 60.00%\n",
            "Avg Loss during Testing -  4861.873330164083\n",
            "[1,   560] loss: 106071.766370\n",
            "[2,   560] loss: 106660.586000\n",
            "[3,   560] loss: 108861.772960\n",
            "[4,   560] loss: 109527.217512\n",
            "[5,   560] loss: 109834.910034\n",
            "[6,   560] loss: 109513.704914\n",
            "[7,   560] loss: 109980.342240\n",
            "[8,   560] loss: 110067.688923\n",
            "[9,   560] loss: 110006.282265\n",
            "[10,   560] loss: 110002.966518\n",
            "[11,   560] loss: 110037.615205\n",
            "[12,   560] loss: 109682.989101\n",
            "[13,   560] loss: 109829.017292\n",
            "[14,   560] loss: 109984.150171\n",
            "[15,   560] loss: 110071.633105\n",
            "[16,   560] loss: 109714.829869\n",
            "[17,   560] loss: 109971.098769\n",
            "[18,   560] loss: 109973.868474\n",
            "[19,   560] loss: 109905.576712\n",
            "[20,   560] loss: 109506.562559\n",
            "[21,   560] loss: 108897.081588\n",
            "[22,   560] loss: 109900.063857\n",
            "[23,   560] loss: 109951.618412\n",
            "[24,   560] loss: 109774.463979\n",
            "[25,   560] loss: 109710.168279\n",
            "[26,   560] loss: 109983.908674\n",
            "[27,   560] loss: 109820.146603\n",
            "[28,   560] loss: 109957.019622\n",
            "[29,   560] loss: 109960.555804\n",
            "[30,   560] loss: 109980.761666\n",
            "[31,   560] loss: 109583.293910\n",
            "[32,   560] loss: 109362.180636\n",
            "[33,   560] loss: 109716.591528\n",
            "[34,   560] loss: 109910.513243\n",
            "[35,   560] loss: 110041.414680\n",
            "[36,   560] loss: 109842.955720\n",
            "[37,   560] loss: 109573.015622\n",
            "[38,   560] loss: 110030.411011\n",
            "[39,   560] loss: 109867.513180\n",
            "[40,   560] loss: 109477.958932\n",
            "[41,   560] loss: 109243.397060\n",
            "[42,   560] loss: 109912.580845\n",
            "[43,   560] loss: 110031.795769\n",
            "[44,   560] loss: 110064.580769\n",
            "[45,   560] loss: 109922.401754\n",
            "[46,   560] loss: 109952.533381\n",
            "[47,   560] loss: 109345.717550\n",
            "[48,   560] loss: 109936.189073\n",
            "[49,   560] loss: 109918.116657\n",
            "[50,   560] loss: 110003.638724\n",
            "Finished Training\n",
            "Avg Loss during Testing -  5366.928613767102\n",
            "L1 values -  [4367.997509447952, 4538.163779210917, 4616.783823520618, 4861.873330164083]\n",
            "L1 values -  [4367.997509447952, 5731.2301682692305, 5417.997559528081, 5366.928613767102]\n",
            "Sparsity in fc1.weight: 57.81%\n",
            "Sparsity in fc2.weight: 62.11%\n",
            "Sparsity in fc3.weight: 60.94%\n",
            "Sparsity in fc4.weight: 62.50%\n",
            "Sparsity in fc5.weight: 50.00%\n",
            "Global sparsity: 60.00%\n",
            "Avg Loss during Testing -  15094.460811925946\n",
            "[1,   560] loss: 162657.814575\n",
            "[2,   560] loss: 137605.636506\n",
            "[3,   560] loss: 127955.376287\n",
            "[4,   560] loss: 117953.370965\n",
            "[5,   560] loss: 114764.480218\n",
            "[6,   560] loss: 114874.496038\n",
            "[7,   560] loss: 114404.715339\n",
            "[8,   560] loss: 114262.786311\n",
            "[9,   560] loss: 117124.970246\n",
            "[10,   560] loss: 120525.687486\n",
            "[11,   560] loss: 114453.490695\n",
            "[12,   560] loss: 111754.215908\n",
            "[13,   560] loss: 111461.536565\n",
            "[14,   560] loss: 112296.468380\n",
            "[15,   560] loss: 109747.006306\n",
            "[16,   560] loss: 107564.081686\n",
            "[17,   560] loss: 107757.073619\n",
            "[18,   560] loss: 101734.386640\n",
            "[19,   560] loss: 100640.611564\n",
            "[20,   560] loss: 98568.021235\n",
            "[21,   560] loss: 97815.134780\n",
            "[22,   560] loss: 99516.504485\n",
            "[23,   560] loss: 99087.302222\n",
            "[24,   560] loss: 99991.737702\n",
            "[25,   560] loss: 99458.547318\n",
            "[26,   560] loss: 99574.948776\n",
            "[27,   560] loss: 100024.961987\n",
            "[28,   560] loss: 93487.886644\n",
            "[29,   560] loss: 91611.089992\n",
            "[30,   560] loss: 90961.031529\n",
            "[31,   560] loss: 90677.965949\n",
            "[32,   560] loss: 90374.548633\n",
            "[33,   560] loss: 91308.914826\n",
            "[34,   560] loss: 91312.925950\n",
            "[35,   560] loss: 110144.945825\n",
            "[36,   560] loss: 97163.793300\n",
            "[37,   560] loss: 93905.594713\n",
            "[38,   560] loss: 93182.194777\n",
            "[39,   560] loss: 93316.527443\n",
            "[40,   560] loss: 93298.031161\n",
            "[41,   560] loss: 93233.965649\n",
            "[42,   560] loss: 92932.753045\n",
            "[43,   560] loss: 93331.257654\n",
            "[44,   560] loss: 91984.107406\n",
            "[45,   560] loss: 93195.630082\n",
            "[46,   560] loss: 93344.759837\n",
            "[47,   560] loss: 93227.508364\n",
            "[48,   560] loss: 94431.246437\n",
            "[49,   560] loss: 93941.710118\n",
            "[50,   560] loss: 93819.850900\n",
            "Finished Training\n",
            "Avg Loss during Testing -  4321.284145677861\n",
            "L2 values -  [4367.997509447952, 7823.647266970436, 8909.276233017605, 15094.460811925946]\n",
            "L2 values -  [4367.997509447952, 4704.056339396766, 2872.6591793137677, 4321.284145677861]\n",
            "Sparsity in fc1.weight: 82.50%\n",
            "Sparsity in fc2.weight: 80.08%\n",
            "Sparsity in fc3.weight: 73.44%\n",
            "Sparsity in fc4.weight: 90.62%\n",
            "Sparsity in fc5.weight: 0.00%\n",
            "Global sparsity: 80.00%\n",
            "Avg Loss during Testing -  4772.666032816685\n",
            "[1,   560] loss: 99738.728435\n",
            "[2,   560] loss: 97878.881341\n",
            "[3,   560] loss: 98470.129684\n",
            "[4,   560] loss: 100123.397886\n",
            "[5,   560] loss: 98561.844552\n",
            "[6,   560] loss: 101331.689178\n",
            "[7,   560] loss: 101018.382568\n",
            "[8,   560] loss: 101277.988742\n",
            "[9,   560] loss: 101281.949449\n",
            "[10,   560] loss: 101145.886590\n",
            "[11,   560] loss: 100808.218879\n",
            "[12,   560] loss: 101020.085596\n",
            "[13,   560] loss: 101369.243827\n",
            "[14,   560] loss: 101369.271929\n",
            "[15,   560] loss: 101472.421258\n",
            "[16,   560] loss: 101296.782945\n",
            "[17,   560] loss: 101294.381794\n",
            "[18,   560] loss: 101374.180995\n",
            "[19,   560] loss: 101271.935829\n",
            "[20,   560] loss: 101182.920253\n",
            "[21,   560] loss: 101367.581581\n",
            "[22,   560] loss: 100632.604768\n",
            "[23,   560] loss: 101358.674513\n",
            "[24,   560] loss: 101092.992133\n",
            "[25,   560] loss: 100926.493691\n",
            "[26,   560] loss: 101392.526266\n",
            "[27,   560] loss: 101412.397967\n",
            "[28,   560] loss: 101006.631777\n",
            "[29,   560] loss: 101243.406747\n",
            "[30,   560] loss: 101425.869667\n",
            "[31,   560] loss: 101294.658726\n",
            "[32,   560] loss: 100870.724857\n",
            "[33,   560] loss: 101386.386879\n",
            "[34,   560] loss: 101166.355408\n",
            "[35,   560] loss: 101053.949949\n",
            "[36,   560] loss: 101163.462510\n",
            "[37,   560] loss: 101264.062102\n",
            "[38,   560] loss: 101250.993858\n",
            "[39,   560] loss: 100177.274477\n",
            "[40,   560] loss: 101266.482680\n",
            "[41,   560] loss: 101149.901109\n",
            "[42,   560] loss: 101149.929686\n",
            "[43,   560] loss: 101291.150143\n",
            "[44,   560] loss: 101299.579381\n",
            "[45,   560] loss: 101297.581292\n",
            "[46,   560] loss: 101213.734249\n",
            "[47,   560] loss: 101047.040400\n",
            "[48,   560] loss: 101262.151573\n",
            "[49,   560] loss: 101354.079822\n",
            "[50,   560] loss: 101437.130608\n",
            "Finished Training\n",
            "Avg Loss during Testing -  4845.17295882367\n",
            "L1 values -  [4367.997509447952, 4538.163779210917, 4616.783823520618, 4861.873330164083, 4772.666032816685]\n",
            "L1 values -  [4367.997509447952, 5731.2301682692305, 5417.997559528081, 5366.928613767102, 4845.17295882367]\n",
            "Sparsity in fc1.weight: 80.00%\n",
            "Sparsity in fc2.weight: 80.47%\n",
            "Sparsity in fc3.weight: 79.69%\n",
            "Sparsity in fc4.weight: 75.00%\n",
            "Sparsity in fc5.weight: 100.00%\n",
            "Global sparsity: 80.00%\n",
            "Avg Loss during Testing -  10264.367029037505\n",
            "[1,   560] loss: 189366.830340\n",
            "[2,   560] loss: 167585.248675\n",
            "[3,   560] loss: 153779.272855\n",
            "[4,   560] loss: 144243.355200\n",
            "[5,   560] loss: 138632.273301\n",
            "[6,   560] loss: 135220.452933\n",
            "[7,   560] loss: 133586.614432\n",
            "[8,   560] loss: 132974.397663\n",
            "[9,   560] loss: 133008.439830\n",
            "[10,   560] loss: 132717.334549\n",
            "[11,   560] loss: 132560.658419\n",
            "[12,   560] loss: 132798.861206\n",
            "[13,   560] loss: 132911.391141\n",
            "[14,   560] loss: 132991.264572\n",
            "[15,   560] loss: 132669.104447\n",
            "[16,   560] loss: 132655.589983\n",
            "[17,   560] loss: 133004.899930\n",
            "[18,   560] loss: 132629.975558\n",
            "[19,   560] loss: 130754.285118\n",
            "[20,   560] loss: 132708.147527\n",
            "[21,   560] loss: 132931.889478\n",
            "[22,   560] loss: 129439.455218\n",
            "[23,   560] loss: 132636.859309\n",
            "[24,   560] loss: 132783.774330\n",
            "[25,   560] loss: 132981.190726\n",
            "[26,   560] loss: 132667.920735\n",
            "[27,   560] loss: 132740.668551\n",
            "[28,   560] loss: 132794.201182\n",
            "[29,   560] loss: 132704.479304\n",
            "[30,   560] loss: 132599.800848\n",
            "[31,   560] loss: 132737.843422\n",
            "[32,   560] loss: 132913.508336\n",
            "[33,   560] loss: 132872.445961\n",
            "[34,   560] loss: 132862.580465\n",
            "[35,   560] loss: 132808.706037\n",
            "[36,   560] loss: 132682.418164\n",
            "[37,   560] loss: 131378.697210\n",
            "[38,   560] loss: 132906.438564\n",
            "[39,   560] loss: 132698.967386\n",
            "[40,   560] loss: 132771.283716\n",
            "[41,   560] loss: 132454.947011\n",
            "[42,   560] loss: 132447.336799\n",
            "[43,   560] loss: 132724.094294\n",
            "[44,   560] loss: 132764.687005\n",
            "[45,   560] loss: 132693.267379\n",
            "[46,   560] loss: 132678.832192\n",
            "[47,   560] loss: 132784.525187\n",
            "[48,   560] loss: 132841.608677\n",
            "[49,   560] loss: 132591.516581\n",
            "[50,   560] loss: 132845.279367\n",
            "Finished Training\n",
            "Avg Loss during Testing -  6308.893643262055\n",
            "L2 values -  [4367.997509447952, 7823.647266970436, 8909.276233017605, 15094.460811925946, 10264.367029037505]\n",
            "L2 values -  [4367.997509447952, 4704.056339396766, 2872.6591793137677, 4321.284145677861, 6308.893643262055]\n",
            "Sparsity in fc1.weight: 91.56%\n",
            "Sparsity in fc2.weight: 91.41%\n",
            "Sparsity in fc3.weight: 85.94%\n",
            "Sparsity in fc4.weight: 90.62%\n",
            "Sparsity in fc5.weight: 0.00%\n",
            "Global sparsity: 90.00%\n",
            "Avg Loss during Testing -  7115.1995461394945\n",
            "[1,   560] loss: 141640.745752\n",
            "[2,   560] loss: 133085.357903\n",
            "[3,   560] loss: 132625.222063\n",
            "[4,   560] loss: 132727.026531\n",
            "[5,   560] loss: 132606.996313\n",
            "[6,   560] loss: 132095.985142\n",
            "[7,   560] loss: 132513.726758\n",
            "[8,   560] loss: 132422.976814\n",
            "[9,   560] loss: 131480.530001\n",
            "[10,   560] loss: 132859.506414\n",
            "[11,   560] loss: 132625.902916\n",
            "[12,   560] loss: 130194.724313\n",
            "[13,   560] loss: 132594.174139\n",
            "[14,   560] loss: 132473.759581\n",
            "[15,   560] loss: 132969.038302\n",
            "[16,   560] loss: 132869.712605\n",
            "[17,   560] loss: 132871.028223\n",
            "[18,   560] loss: 132707.987409\n",
            "[19,   560] loss: 132571.242334\n",
            "[20,   560] loss: 132590.934469\n",
            "[21,   560] loss: 133019.279844\n",
            "[22,   560] loss: 131035.315137\n",
            "[23,   560] loss: 132628.565506\n",
            "[24,   560] loss: 132865.159787\n",
            "[25,   560] loss: 132737.608901\n",
            "[26,   560] loss: 132873.688257\n",
            "[27,   560] loss: 132545.770229\n",
            "[28,   560] loss: 132858.943345\n",
            "[29,   560] loss: 132866.195996\n",
            "[30,   560] loss: 132940.599215\n",
            "[31,   560] loss: 132389.318213\n",
            "[32,   560] loss: 132091.836182\n",
            "[33,   560] loss: 132968.389530\n",
            "[34,   560] loss: 132148.169409\n",
            "[35,   560] loss: 132449.231149\n",
            "[36,   560] loss: 132607.530671\n",
            "[37,   560] loss: 132784.136293\n",
            "[38,   560] loss: 132783.167773\n",
            "[39,   560] loss: 132685.157718\n",
            "[40,   560] loss: 132259.019904\n",
            "[41,   560] loss: 131900.847367\n",
            "[42,   560] loss: 132157.888351\n",
            "[43,   560] loss: 130375.769364\n",
            "[44,   560] loss: 132710.000446\n",
            "[45,   560] loss: 132146.906756\n",
            "[46,   560] loss: 131217.226894\n",
            "[47,   560] loss: 132862.660749\n",
            "[48,   560] loss: 132703.605615\n",
            "[49,   560] loss: 132775.933922\n",
            "[50,   560] loss: 132445.313766\n",
            "Finished Training\n",
            "Avg Loss during Testing -  6298.660993410352\n",
            "L1 values -  [4367.997509447952, 4538.163779210917, 4616.783823520618, 4861.873330164083, 4772.666032816685, 7115.1995461394945]\n",
            "L1 values -  [4367.997509447952, 5731.2301682692305, 5417.997559528081, 5366.928613767102, 4845.17295882367, 6298.660993410352]\n",
            "Sparsity in fc1.weight: 90.31%\n",
            "Sparsity in fc2.weight: 89.84%\n",
            "Sparsity in fc3.weight: 89.84%\n",
            "Sparsity in fc4.weight: 90.62%\n",
            "Sparsity in fc5.weight: 75.00%\n",
            "Global sparsity: 90.00%\n",
            "Avg Loss during Testing -  6557.644667288557\n",
            "[1,   560] loss: 135751.469716\n",
            "[2,   560] loss: 134731.626004\n",
            "[3,   560] loss: 135011.667930\n",
            "[4,   560] loss: 134090.069897\n",
            "[5,   560] loss: 134115.656403\n",
            "[6,   560] loss: 132312.928843\n",
            "[7,   560] loss: 134304.270351\n",
            "[8,   560] loss: 133521.836422\n",
            "[9,   560] loss: 133623.623587\n",
            "[10,   560] loss: 133473.398483\n",
            "[11,   560] loss: 132947.590032\n",
            "[12,   560] loss: 132875.949962\n",
            "[13,   560] loss: 132839.017962\n",
            "[14,   560] loss: 132870.410658\n",
            "[15,   560] loss: 132794.989969\n",
            "[16,   560] loss: 132744.056574\n",
            "[17,   560] loss: 132334.520173\n",
            "[18,   560] loss: 132587.558109\n",
            "[19,   560] loss: 132690.838954\n",
            "[20,   560] loss: 130389.935449\n",
            "[21,   560] loss: 132995.053680\n",
            "[22,   560] loss: 132639.185017\n",
            "[23,   560] loss: 132904.163581\n",
            "[24,   560] loss: 132488.929953\n",
            "[25,   560] loss: 132332.603087\n",
            "[26,   560] loss: 132887.741284\n",
            "[27,   560] loss: 132516.325049\n",
            "[28,   560] loss: 132235.584971\n",
            "[29,   560] loss: 131793.034780\n",
            "[30,   560] loss: 132727.844249\n",
            "[31,   560] loss: 132906.604402\n",
            "[32,   560] loss: 132835.329269\n",
            "[33,   560] loss: 133027.186959\n",
            "[34,   560] loss: 132666.446191\n",
            "[35,   560] loss: 132856.298723\n",
            "[36,   560] loss: 131665.166173\n",
            "[37,   560] loss: 132848.552699\n",
            "[38,   560] loss: 132765.945595\n",
            "[39,   560] loss: 132883.152822\n",
            "[40,   560] loss: 131633.849794\n",
            "[41,   560] loss: 132316.146830\n",
            "[42,   560] loss: 132531.493673\n",
            "[43,   560] loss: 132040.298710\n",
            "[44,   560] loss: 132887.931784\n",
            "[45,   560] loss: 132331.940112\n",
            "[46,   560] loss: 132734.476078\n",
            "[47,   560] loss: 132374.625024\n",
            "[48,   560] loss: 132741.721341\n",
            "[49,   560] loss: 132630.864380\n",
            "[50,   560] loss: 132280.577696\n",
            "Finished Training\n",
            "Avg Loss during Testing -  6317.372766575775\n",
            "L2 values -  [4367.997509447952, 7823.647266970436, 8909.276233017605, 15094.460811925946, 10264.367029037505, 6557.644667288557]\n",
            "L2 values -  [4367.997509447952, 4704.056339396766, 2872.6591793137677, 4321.284145677861, 6308.893643262055, 6317.372766575775]\n"
          ]
        }
      ],
      "source": [
        "# Full cycle\n",
        "\n",
        "parameters_to_prune = (\n",
        "    (net.fc1, 'weight'),\n",
        "    (net.fc2, 'weight'),\n",
        "    (net.fc3, 'weight'),\n",
        "    (net.fc4, 'weight'),\n",
        "    (net.fc5, 'weight'),\n",
        ")\n",
        "\n",
        "batch_loss = 560\n",
        "epoch = 50\n",
        "\n",
        "results_base_l1 = []\n",
        "results_finetune_l1 = []\n",
        "\n",
        "results_base_l2 = []\n",
        "results_finetune_l2 = []\n",
        "\n",
        "# Network\n",
        "net = Net().to(device)\n",
        "print_sparsity(net)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.1)\n",
        "\n",
        "train(net, train_loader, epoch, optimizer, criterion, batch_loss)\n",
        "loss_mse = test(net, test_loader, criterion)\n",
        "\n",
        "results_base_l1.append(loss_mse)\n",
        "results_finetune_l1.append(loss_mse)\n",
        "results_base_l2.append(loss_mse)\n",
        "results_finetune_l2.append(loss_mse)\n",
        "\n",
        "PATH = './cifar_net.pth'\n",
        "torch.save(net.state_dict(), PATH)\n",
        "\n",
        "prune_values = [0.2, 0.4, 0.6, 0.8, 0.9]\n",
        "pruning_methods = [0,1]\n",
        "\n",
        "\n",
        "for prune_value in prune_values:\n",
        "  for pruning_method in pruning_methods:\n",
        "  # Each prune value experiment is independent\n",
        "    net = Net().to(device)\n",
        "    net.load_state_dict(torch.load(PATH))\n",
        "    \n",
        "    parameters_to_prune = (\n",
        "    (net.fc1, 'weight'),\n",
        "    (net.fc2, 'weight'),\n",
        "    (net.fc3, 'weight'),\n",
        "    (net.fc4, 'weight'),\n",
        "    (net.fc5, 'weight'),\n",
        "    )\n",
        "\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.1)\n",
        "    \n",
        "    if pruning_method == 0:\n",
        "      prune.global_unstructured(\n",
        "          parameters_to_prune,\n",
        "          pruning_method=prune.L1Unstructured,\n",
        "          amount=prune_value,\n",
        "      )\n",
        "    else:\n",
        "      prune.global_unstructured(\n",
        "          parameters_to_prune,\n",
        "          pruning_method=prune.RandomUnstructured,\n",
        "          amount=prune_value,\n",
        "      )\n",
        "\n",
        "    print_sparsity(net)\n",
        "    \n",
        "    base_loss = test(net, test_loader, criterion)\n",
        "\n",
        "    train(net, train_loader, epoch, optimizer, criterion, batch_loss)\n",
        "    finetune_loss = test(net, test_loader, criterion)\n",
        "    \n",
        "    if pruning_method == 0:\n",
        "      results_base_l1.append(base_loss)\n",
        "      results_finetune_l1.append(finetune_loss)\n",
        "\n",
        "      print('L1 values - ', results_base_l1)\n",
        "      print('L1 values - ', results_finetune_l1)\n",
        "    else:\n",
        "      results_base_l2.append(base_loss)\n",
        "      results_finetune_l2.append(finetune_loss)\n",
        "\n",
        "      print('L2 values - ', results_base_l2)\n",
        "      print('L2 values - ', results_finetune_l2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qqf2WvBVmsJH",
        "outputId": "7a658066-f449-4aed-ae0f-e6dce8e685c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[4367.997509447952,\n",
              " 4538.163779210917,\n",
              " 4616.783823520618,\n",
              " 4861.873330164083,\n",
              " 4772.666032816685,\n",
              " 7115.1995461394945]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_base_l1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPPioan3msJH",
        "outputId": "e732b3ee-5295-4e9e-cfc6-ab5880b0278b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[4367.997509447952,\n",
              " 5731.2301682692305,\n",
              " 5417.997559528081,\n",
              " 5366.928613767102,\n",
              " 4845.17295882367,\n",
              " 6298.660993410352]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_finetune_l1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yk59mUmgmsJI",
        "outputId": "3b00cb0c-f47c-42f3-8510-31af34b9576d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[4367.997509447952,\n",
              " 7823.647266970436,\n",
              " 8909.276233017605,\n",
              " 15094.460811925946,\n",
              " 10264.367029037505,\n",
              " 6557.644667288557]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_base_l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxKo0dkkmsJI",
        "outputId": "15cdb443-96b0-429a-f580-3af1363dcafe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[4367.997509447952,\n",
              " 4704.056339396766,\n",
              " 2872.6591793137677,\n",
              " 4321.284145677861,\n",
              " 6308.893643262055,\n",
              " 6317.372766575775]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_finetune_l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AO_G4qPmsJI"
      },
      "outputs": [],
      "source": [
        "x_coord = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "OiqOqIO8msJI",
        "outputId": "6b17d79e-d0f8-4b41-e7eb-6a21a6632762"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('Housing Dataset with 5 layer NN and different pruning techniques')\n",
        "plt.plot(x_coord, results_base_l1, label = 'L1 without finetuning')\n",
        "plt.plot(x_coord, results_finetune_l1, label = 'L1 finetuned')\n",
        "plt.plot(x_coord, results_base_l2, label = 'Random without finetuning')\n",
        "plt.plot(x_coord, results_finetune_l2, label = 'Random finetuned')\n",
        "plt.ylabel('MSE loss')\n",
        "plt.xlabel('Sparsity')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyhuBVX_ottd"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBf7zmvBot2t"
      },
      "source": [
        "### 10 layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gc4t-rWqot2u"
      },
      "outputs": [],
      "source": [
        "def print_sparsity(net):\n",
        "  print(\n",
        "      \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc1.weight == 0))\n",
        "          / float(net.fc1.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Sparsity in fc2.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc2.weight == 0))\n",
        "          / float(net.fc2.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Sparsity in fc3.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc3.weight == 0))\n",
        "          / float(net.fc3.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Sparsity in fc4.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc4.weight == 0))\n",
        "          / float(net.fc4.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Sparsity in fc5.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc5.weight == 0))\n",
        "          / float(net.fc5.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "\n",
        "  print(\n",
        "      \"Sparsity in fc6.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc6.weight == 0))\n",
        "          / float(net.fc6.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Sparsity in fc7.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc7.weight == 0))\n",
        "          / float(net.fc7.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Sparsity in fc8.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc8.weight == 0))\n",
        "          / float(net.fc8.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Sparsity in fc9.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc9.weight == 0))\n",
        "          / float(net.fc9.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "  print(\n",
        "      \"Sparsity in fc10.weight: {:.2f}%\".format(\n",
        "          100. * float(torch.sum(net.fc10.weight == 0))\n",
        "          / float(net.fc10.weight.nelement())\n",
        "      )\n",
        "  )\n",
        "\n",
        "  print(\n",
        "      \"Global sparsity: {:.2f}%\".format(\n",
        "          100. * float(\n",
        "              + torch.sum(net.fc1.weight == 0)\n",
        "              + torch.sum(net.fc2.weight == 0)\n",
        "              + torch.sum(net.fc3.weight == 0)\n",
        "              + torch.sum(net.fc4.weight == 0)\n",
        "              + torch.sum(net.fc5.weight == 0)\n",
        "              + torch.sum(net.fc6.weight == 0)\n",
        "              + torch.sum(net.fc7.weight == 0)\n",
        "              + torch.sum(net.fc8.weight == 0)\n",
        "              + torch.sum(net.fc9.weight == 0)\n",
        "              + torch.sum(net.fc10.weight == 0)\n",
        "          )\n",
        "          / float(\n",
        "              + net.fc1.weight.nelement()\n",
        "              + net.fc2.weight.nelement()\n",
        "              + net.fc3.weight.nelement()\n",
        "              + net.fc4.weight.nelement()\n",
        "              + net.fc5.weight.nelement()\n",
        "              + net.fc6.weight.nelement()\n",
        "              + net.fc7.weight.nelement()\n",
        "              + net.fc8.weight.nelement()\n",
        "              + net.fc9.weight.nelement()\n",
        "              + net.fc10.weight.nelement()\n",
        "          )\n",
        "      )\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yAYT3Mgot2v"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(20, 20)\n",
        "        self.fc2 = nn.Linear(20, 16)\n",
        "        self.fc3 = nn.Linear(16, 16)\n",
        "        self.fc4 = nn.Linear(16, 16)\n",
        "        self.fc5 = nn.Linear(16, 8)\n",
        "        self.fc6 = nn.Linear(8, 8)\n",
        "        self.fc7 = nn.Linear(8, 4)\n",
        "        self.fc8 = nn.Linear(4, 4)\n",
        "        self.fc9 = nn.Linear(4, 2)\n",
        "        self.fc10 = nn.Linear(2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x)) \n",
        "        x = torch.tanh(self.fc3(x))\n",
        "        x = torch.tanh(self.fc4(x))\n",
        "        x = torch.tanh(self.fc5(x))\n",
        "        x = torch.tanh(self.fc6(x)) \n",
        "        x = torch.tanh(self.fc7(x))\n",
        "        x = torch.tanh(self.fc8(x))\n",
        "        x = torch.tanh(self.fc9(x))\n",
        "        x = self.fc10(x)\n",
        "        return x\n",
        "\n",
        "net = Net().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVgVvHOqot2v"
      },
      "outputs": [],
      "source": [
        "# # criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.MSELoss()\n",
        "# # optimizer = optim.SGD(net.parameters(), lr=10)\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKfrukPOot2v",
        "outputId": "b3485c7c-a551-4e5c-80d9-86ef4382bbc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Avg Loss during Testing -  20534.913569173365\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "20534.913569173365"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Testing before training\n",
        "test(net, test_loader, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xf9NFJEXot2w",
        "outputId": "c28892c2-9132-476d-ffbf-9e2efaf3c243"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sparsity in fc1.weight: 0.00%\n",
            "Sparsity in fc2.weight: 0.00%\n",
            "Sparsity in fc3.weight: 0.00%\n",
            "Sparsity in fc4.weight: 0.00%\n",
            "Sparsity in fc5.weight: 0.00%\n",
            "Sparsity in fc6.weight: 0.00%\n",
            "Sparsity in fc7.weight: 0.00%\n",
            "Sparsity in fc8.weight: 0.00%\n",
            "Sparsity in fc9.weight: 0.00%\n",
            "Sparsity in fc10.weight: 0.00%\n",
            "Global sparsity: 0.00%\n",
            "[1,   560] loss: 339013.993401\n",
            "[2,   560] loss: 230165.709033\n",
            "[3,   560] loss: 171416.415817\n",
            "[4,   560] loss: 145306.897642\n",
            "[5,   560] loss: 134444.265960\n",
            "[6,   560] loss: 133005.179708\n",
            "[7,   560] loss: 132446.382342\n",
            "[8,   560] loss: 132744.403749\n",
            "[9,   560] loss: 132834.166466\n",
            "[10,   560] loss: 132738.260969\n",
            "[11,   560] loss: 132625.727047\n",
            "[12,   560] loss: 132942.565116\n",
            "[13,   560] loss: 132718.372911\n",
            "[14,   560] loss: 131446.753735\n",
            "[15,   560] loss: 132494.361963\n",
            "[16,   560] loss: 132995.588316\n",
            "[17,   560] loss: 132460.824937\n",
            "[18,   560] loss: 132835.106306\n",
            "[19,   560] loss: 132794.888853\n",
            "[20,   560] loss: 132928.141068\n",
            "[21,   560] loss: 132503.659710\n",
            "[22,   560] loss: 132768.546181\n",
            "[23,   560] loss: 131551.823176\n",
            "[24,   560] loss: 132960.603927\n",
            "[25,   560] loss: 132662.819716\n",
            "[26,   560] loss: 132833.192875\n",
            "[27,   560] loss: 129675.459780\n",
            "[28,   560] loss: 132657.524976\n",
            "[29,   560] loss: 132859.833702\n",
            "[30,   560] loss: 132510.129492\n",
            "[31,   560] loss: 130629.864771\n",
            "[32,   560] loss: 132560.841619\n",
            "[33,   560] loss: 132839.790395\n",
            "[34,   560] loss: 132841.189927\n",
            "[35,   560] loss: 132358.385885\n",
            "[36,   560] loss: 132550.497667\n",
            "[37,   560] loss: 132793.476461\n",
            "[38,   560] loss: 132879.851632\n",
            "[39,   560] loss: 132549.647178\n",
            "[40,   560] loss: 131047.968705\n",
            "[41,   560] loss: 131989.912873\n",
            "[42,   560] loss: 132761.854021\n",
            "[43,   560] loss: 132815.274707\n",
            "[44,   560] loss: 132732.724791\n",
            "[45,   560] loss: 132753.473553\n",
            "[46,   560] loss: 132632.362064\n",
            "[47,   560] loss: 132967.386007\n",
            "[48,   560] loss: 132993.053700\n",
            "[49,   560] loss: 132626.636175\n",
            "[50,   560] loss: 132196.899159\n",
            "Finished Training\n",
            "Avg Loss during Testing -  6321.074424302765\n",
            "Sparsity in fc1.weight: 16.25%\n",
            "Sparsity in fc2.weight: 23.12%\n",
            "Sparsity in fc3.weight: 23.44%\n",
            "Sparsity in fc4.weight: 21.09%\n",
            "Sparsity in fc5.weight: 26.56%\n",
            "Sparsity in fc6.weight: 4.69%\n",
            "Sparsity in fc7.weight: 15.62%\n",
            "Sparsity in fc8.weight: 6.25%\n",
            "Sparsity in fc9.weight: 0.00%\n",
            "Sparsity in fc10.weight: 0.00%\n",
            "Global sparsity: 19.97%\n",
            "Avg Loss during Testing -  6324.8538893573\n",
            "[1,   560] loss: 132598.917885\n",
            "[2,   560] loss: 132694.793931\n",
            "[3,   560] loss: 132636.959553\n",
            "[4,   560] loss: 132881.670455\n",
            "[5,   560] loss: 132776.001528\n",
            "[6,   560] loss: 132505.072897\n",
            "[7,   560] loss: 132612.229754\n",
            "[8,   560] loss: 132919.529133\n",
            "[9,   560] loss: 132674.877204\n",
            "[10,   560] loss: 132762.977860\n",
            "[11,   560] loss: 132558.237626\n",
            "[12,   560] loss: 132641.870675\n",
            "[13,   560] loss: 132855.860498\n",
            "[14,   560] loss: 132927.508580\n",
            "[15,   560] loss: 132518.971223\n",
            "[16,   560] loss: 132302.091884\n",
            "[17,   560] loss: 132528.547412\n",
            "[18,   560] loss: 132545.940597\n",
            "[19,   560] loss: 132396.730776\n",
            "[20,   560] loss: 132640.859668\n",
            "[21,   560] loss: 132885.819964\n",
            "[22,   560] loss: 132773.415050\n",
            "[23,   560] loss: 132938.393248\n",
            "[24,   560] loss: 132977.436213\n",
            "[25,   560] loss: 132487.793073\n",
            "[26,   560] loss: 132310.581006\n",
            "[27,   560] loss: 132939.676744\n",
            "[28,   560] loss: 132490.607990\n",
            "[29,   560] loss: 132289.709919\n",
            "[30,   560] loss: 132944.708479\n",
            "[31,   560] loss: 132252.370382\n",
            "[32,   560] loss: 132405.407540\n",
            "[33,   560] loss: 132712.418147\n",
            "[34,   560] loss: 132117.493499\n",
            "[35,   560] loss: 132710.807024\n",
            "[36,   560] loss: 132726.820187\n",
            "[37,   560] loss: 132836.570410\n",
            "[38,   560] loss: 132862.954297\n",
            "[39,   560] loss: 132689.624627\n",
            "[40,   560] loss: 132243.635937\n",
            "[41,   560] loss: 132769.575481\n",
            "[42,   560] loss: 132553.337127\n",
            "[43,   560] loss: 132518.857910\n",
            "[44,   560] loss: 132644.480207\n",
            "[45,   560] loss: 132825.754021\n",
            "[46,   560] loss: 132936.997893\n",
            "[47,   560] loss: 132504.259783\n",
            "[48,   560] loss: 132935.305981\n",
            "[49,   560] loss: 132385.866682\n",
            "[50,   560] loss: 132931.877316\n",
            "Finished Training\n",
            "Avg Loss during Testing -  6299.219768046785\n",
            "L1 values -  [6321.074424302765, 6324.8538893573]\n",
            "L1 values -  [6321.074424302765, 6299.219768046785]\n",
            "Sparsity in fc1.weight: 20.50%\n",
            "Sparsity in fc2.weight: 19.38%\n",
            "Sparsity in fc3.weight: 18.75%\n",
            "Sparsity in fc4.weight: 19.92%\n",
            "Sparsity in fc5.weight: 21.09%\n",
            "Sparsity in fc6.weight: 21.88%\n",
            "Sparsity in fc7.weight: 28.12%\n",
            "Sparsity in fc8.weight: 6.25%\n",
            "Sparsity in fc9.weight: 25.00%\n",
            "Sparsity in fc10.weight: 0.00%\n",
            "Global sparsity: 19.97%\n",
            "Avg Loss during Testing -  6321.766012186663\n",
            "[1,   560] loss: 127080.045358\n",
            "[2,   560] loss: 104077.163281\n",
            "[3,   560] loss: 99958.560174\n",
            "[4,   560] loss: 112360.779307\n",
            "[5,   560] loss: 121469.682614\n",
            "[6,   560] loss: 130293.692432\n",
            "[7,   560] loss: 115713.989035\n",
            "[8,   560] loss: 137569.035104\n",
            "[9,   560] loss: 130188.968952\n",
            "[10,   560] loss: 129995.869256\n",
            "[11,   560] loss: 129904.482443\n",
            "[12,   560] loss: 129199.786171\n",
            "[13,   560] loss: 127402.870020\n",
            "[14,   560] loss: 127653.690831\n",
            "[15,   560] loss: 127615.808859\n",
            "[16,   560] loss: 127328.853170\n",
            "[17,   560] loss: 127467.499969\n",
            "[18,   560] loss: 127506.179792\n",
            "[19,   560] loss: 127498.899383\n",
            "[20,   560] loss: 127629.645365\n",
            "[21,   560] loss: 127172.393464\n",
            "[22,   560] loss: 127165.488801\n",
            "[23,   560] loss: 126041.614554\n",
            "[24,   560] loss: 125129.342927\n",
            "[25,   560] loss: 116178.334919\n",
            "[26,   560] loss: 116195.920846\n",
            "[27,   560] loss: 115129.708151\n",
            "[28,   560] loss: 120605.885083\n",
            "[29,   560] loss: 121309.514080\n",
            "[30,   560] loss: 114093.568255\n",
            "[31,   560] loss: 114399.506829\n",
            "[32,   560] loss: 114787.211353\n",
            "[33,   560] loss: 117758.346029\n",
            "[34,   560] loss: 123308.064328\n",
            "[35,   560] loss: 131678.586649\n",
            "[36,   560] loss: 121381.019831\n",
            "[37,   560] loss: 119637.377187\n",
            "[38,   560] loss: 121758.377466\n",
            "[39,   560] loss: 107369.153162\n",
            "[40,   560] loss: 102207.995131\n",
            "[41,   560] loss: 101162.742824\n",
            "[42,   560] loss: 99300.743663\n",
            "[43,   560] loss: 99507.457024\n",
            "[44,   560] loss: 99194.626547\n",
            "[45,   560] loss: 98532.458702\n",
            "[46,   560] loss: 98887.736196\n",
            "[47,   560] loss: 98708.906972\n",
            "[48,   560] loss: 98680.785927\n",
            "[49,   560] loss: 98531.643966\n",
            "[50,   560] loss: 98543.307373\n",
            "Finished Training\n",
            "Avg Loss during Testing -  4277.459931412648\n",
            "L2 values -  [6321.074424302765, 6321.766012186663]\n",
            "L2 values -  [6321.074424302765, 4277.459931412648]\n",
            "Sparsity in fc1.weight: 33.75%\n",
            "Sparsity in fc2.weight: 41.56%\n",
            "Sparsity in fc3.weight: 48.83%\n",
            "Sparsity in fc4.weight: 42.97%\n",
            "Sparsity in fc5.weight: 53.91%\n",
            "Sparsity in fc6.weight: 18.75%\n",
            "Sparsity in fc7.weight: 25.00%\n",
            "Sparsity in fc8.weight: 6.25%\n",
            "Sparsity in fc9.weight: 0.00%\n",
            "Sparsity in fc10.weight: 0.00%\n",
            "Global sparsity: 40.01%\n",
            "Avg Loss during Testing -  6323.009651262916\n",
            "[1,   560] loss: 132803.205155\n",
            "[2,   560] loss: 132809.630828\n",
            "[3,   560] loss: 132755.427748\n",
            "[4,   560] loss: 132635.413173\n",
            "[5,   560] loss: 132894.738379\n",
            "[6,   560] loss: 132617.432394\n",
            "[7,   560] loss: 132851.496104\n",
            "[8,   560] loss: 132733.075865\n",
            "[9,   560] loss: 132636.060411\n",
            "[10,   560] loss: 132816.543841\n",
            "[11,   560] loss: 132812.465220\n",
            "[12,   560] loss: 132434.126189\n",
            "[13,   560] loss: 132892.308060\n",
            "[14,   560] loss: 132402.906934\n",
            "[15,   560] loss: 132568.238665\n",
            "[16,   560] loss: 132572.242930\n",
            "[17,   560] loss: 132882.961987\n",
            "[18,   560] loss: 132987.203223\n",
            "[19,   560] loss: 132681.425188\n",
            "[20,   560] loss: 132653.257558\n",
            "[21,   560] loss: 132871.723005\n",
            "[22,   560] loss: 132767.899428\n",
            "[23,   560] loss: 132844.042376\n",
            "[24,   560] loss: 132718.039014\n",
            "[25,   560] loss: 132572.072719\n",
            "[26,   560] loss: 132779.004098\n",
            "[27,   560] loss: 132349.004405\n",
            "[28,   560] loss: 132739.696352\n",
            "[29,   560] loss: 132681.977595\n",
            "[30,   560] loss: 132756.407007\n",
            "[31,   560] loss: 132733.343841\n",
            "[32,   560] loss: 132460.983175\n",
            "[33,   560] loss: 132171.812409\n",
            "[34,   560] loss: 132889.195689\n",
            "[35,   560] loss: 132678.906100\n",
            "[36,   560] loss: 132122.245675\n",
            "[37,   560] loss: 132695.056655\n",
            "[38,   560] loss: 132814.865196\n",
            "[39,   560] loss: 132921.248207\n",
            "[40,   560] loss: 132633.395773\n",
            "[41,   560] loss: 132810.483970\n",
            "[42,   560] loss: 132946.023814\n",
            "[43,   560] loss: 132530.942707\n",
            "[44,   560] loss: 132903.752302\n",
            "[45,   560] loss: 132950.633625\n",
            "[46,   560] loss: 132938.072517\n",
            "[47,   560] loss: 131771.064607\n",
            "[48,   560] loss: 132876.529715\n",
            "[49,   560] loss: 132897.664122\n",
            "[50,   560] loss: 132739.124376\n",
            "Finished Training\n",
            "Avg Loss during Testing -  6310.706497560275\n",
            "L1 values -  [6321.074424302765, 6324.8538893573, 6323.009651262916]\n",
            "L1 values -  [6321.074424302765, 6299.219768046785, 6310.706497560275]\n",
            "Sparsity in fc1.weight: 41.00%\n",
            "Sparsity in fc2.weight: 40.00%\n",
            "Sparsity in fc3.weight: 40.62%\n",
            "Sparsity in fc4.weight: 37.89%\n",
            "Sparsity in fc5.weight: 38.28%\n",
            "Sparsity in fc6.weight: 48.44%\n",
            "Sparsity in fc7.weight: 34.38%\n",
            "Sparsity in fc8.weight: 37.50%\n",
            "Sparsity in fc9.weight: 37.50%\n",
            "Sparsity in fc10.weight: 0.00%\n",
            "Global sparsity: 40.01%\n",
            "Avg Loss during Testing -  6335.08767460773\n",
            "[1,   560] loss: 107665.667304\n",
            "[2,   560] loss: 99103.543802\n",
            "[3,   560] loss: 109633.820762\n",
            "[4,   560] loss: 118456.471739\n",
            "[5,   560] loss: 102166.069700\n",
            "[6,   560] loss: 96119.113231\n",
            "[7,   560] loss: 114316.519451\n",
            "[8,   560] loss: 112706.449885\n",
            "[9,   560] loss: 107257.305483\n",
            "[10,   560] loss: 89330.806135\n",
            "[11,   560] loss: 87001.387884\n",
            "[12,   560] loss: 87147.958435\n",
            "[13,   560] loss: 100567.544958\n",
            "[14,   560] loss: 113540.440834\n",
            "[15,   560] loss: 110701.912803\n",
            "[16,   560] loss: 109075.093617\n",
            "[17,   560] loss: 108182.059162\n",
            "[18,   560] loss: 106803.488110\n",
            "[19,   560] loss: 106355.901709\n",
            "[20,   560] loss: 103364.328275\n",
            "[21,   560] loss: 105079.813232\n",
            "[22,   560] loss: 104982.851779\n",
            "[23,   560] loss: 104647.111032\n",
            "[24,   560] loss: 104477.334354\n",
            "[25,   560] loss: 104298.129374\n",
            "[26,   560] loss: 104185.313811\n",
            "[27,   560] loss: 103927.636283\n",
            "[28,   560] loss: 103800.262193\n",
            "[29,   560] loss: 103068.896586\n",
            "[30,   560] loss: 103743.201385\n",
            "[31,   560] loss: 103765.301869\n",
            "[32,   560] loss: 103682.847886\n",
            "[33,   560] loss: 103363.681233\n",
            "[34,   560] loss: 103618.881710\n",
            "[35,   560] loss: 103333.989160\n",
            "[36,   560] loss: 103488.047949\n",
            "[37,   560] loss: 103331.277734\n",
            "[38,   560] loss: 103675.241438\n",
            "[39,   560] loss: 103527.877072\n",
            "[40,   560] loss: 103669.427951\n",
            "[41,   560] loss: 103150.906665\n",
            "[42,   560] loss: 103500.052107\n",
            "[43,   560] loss: 103534.974268\n",
            "[44,   560] loss: 103305.357268\n",
            "[45,   560] loss: 103600.950680\n",
            "[46,   560] loss: 103506.673671\n",
            "[47,   560] loss: 103547.907230\n",
            "[48,   560] loss: 103406.480615\n",
            "[49,   560] loss: 103511.480078\n",
            "[50,   560] loss: 103622.425331\n",
            "Finished Training\n",
            "Avg Loss during Testing -  5078.85098246747\n",
            "L2 values -  [6321.074424302765, 6321.766012186663, 6335.08767460773]\n",
            "L2 values -  [6321.074424302765, 4277.459931412648, 5078.85098246747]\n",
            "Sparsity in fc1.weight: 51.75%\n",
            "Sparsity in fc2.weight: 60.62%\n",
            "Sparsity in fc3.weight: 73.44%\n",
            "Sparsity in fc4.weight: 64.06%\n",
            "Sparsity in fc5.weight: 71.88%\n",
            "Sparsity in fc6.weight: 43.75%\n",
            "Sparsity in fc7.weight: 40.62%\n",
            "Sparsity in fc8.weight: 18.75%\n",
            "Sparsity in fc9.weight: 0.00%\n",
            "Sparsity in fc10.weight: 0.00%\n",
            "Global sparsity: 59.99%\n",
            "Avg Loss during Testing -  6321.356218456994\n",
            "[1,   560] loss: 132717.730406\n",
            "[2,   560] loss: 132777.023009\n",
            "[3,   560] loss: 132544.352023\n",
            "[4,   560] loss: 132391.299058\n",
            "[5,   560] loss: 132805.189526\n",
            "[6,   560] loss: 132970.080197\n",
            "[7,   560] loss: 132558.862528\n",
            "[8,   560] loss: 132772.403913\n",
            "[9,   560] loss: 132952.807363\n",
            "[10,   560] loss: 132500.552689\n",
            "[11,   560] loss: 132550.102665\n",
            "[12,   560] loss: 132884.174341\n",
            "[13,   560] loss: 132817.887960\n",
            "[14,   560] loss: 132578.222084\n",
            "[15,   560] loss: 132599.579300\n",
            "[16,   560] loss: 132884.726451\n",
            "[17,   560] loss: 132216.187692\n",
            "[18,   560] loss: 132960.058922\n",
            "[19,   560] loss: 130571.870916\n",
            "[20,   560] loss: 132144.091403\n",
            "[21,   560] loss: 133007.711290\n",
            "[22,   560] loss: 132881.948717\n",
            "[23,   560] loss: 132917.348887\n",
            "[24,   560] loss: 131557.595086\n",
            "[25,   560] loss: 132758.112821\n",
            "[26,   560] loss: 132362.365615\n",
            "[27,   560] loss: 132922.109187\n",
            "[28,   560] loss: 132785.859821\n",
            "[29,   560] loss: 132372.207467\n",
            "[30,   560] loss: 132825.520124\n",
            "[31,   560] loss: 132923.764708\n",
            "[32,   560] loss: 132912.335198\n",
            "[33,   560] loss: 132828.372674\n",
            "[34,   560] loss: 132816.369887\n",
            "[35,   560] loss: 132797.720156\n",
            "[36,   560] loss: 132666.138473\n",
            "[37,   560] loss: 132540.816699\n",
            "[38,   560] loss: 132876.675757\n",
            "[39,   560] loss: 132864.024854\n",
            "[40,   560] loss: 132328.419531\n",
            "[41,   560] loss: 132719.686844\n",
            "[42,   560] loss: 132815.106383\n",
            "[43,   560] loss: 132651.100328\n",
            "[44,   560] loss: 133070.752511\n",
            "[45,   560] loss: 132748.608025\n",
            "[46,   560] loss: 132490.241793\n",
            "[47,   560] loss: 132479.457167\n",
            "[48,   560] loss: 131960.032174\n",
            "[49,   560] loss: 132651.079520\n",
            "[50,   560] loss: 131914.790262\n",
            "Finished Training\n",
            "Avg Loss during Testing -  6317.747283713643\n",
            "L1 values -  [6321.074424302765, 6324.8538893573, 6323.009651262916, 6321.356218456994]\n",
            "L1 values -  [6321.074424302765, 6299.219768046785, 6310.706497560275, 6317.747283713643]\n",
            "Sparsity in fc1.weight: 59.75%\n",
            "Sparsity in fc2.weight: 59.69%\n",
            "Sparsity in fc3.weight: 59.38%\n",
            "Sparsity in fc4.weight: 60.94%\n",
            "Sparsity in fc5.weight: 60.94%\n",
            "Sparsity in fc6.weight: 65.62%\n",
            "Sparsity in fc7.weight: 46.88%\n",
            "Sparsity in fc8.weight: 56.25%\n",
            "Sparsity in fc9.weight: 62.50%\n",
            "Sparsity in fc10.weight: 100.00%\n",
            "Global sparsity: 59.99%\n",
            "Avg Loss during Testing -  13137.98205785974\n",
            "[1,   560] loss: 241602.259528\n",
            "[2,   560] loss: 210953.039355\n",
            "[3,   560] loss: 186073.615492\n",
            "[4,   560] loss: 167904.127933\n",
            "[5,   560] loss: 154431.677605\n",
            "[6,   560] loss: 145250.082349\n",
            "[7,   560] loss: 139162.536998\n",
            "[8,   560] loss: 135687.951050\n",
            "[9,   560] loss: 133716.579538\n",
            "[10,   560] loss: 133310.624030\n",
            "[11,   560] loss: 133035.834853\n",
            "[12,   560] loss: 132686.202277\n",
            "[13,   560] loss: 132307.933235\n",
            "[14,   560] loss: 132835.862524\n",
            "[15,   560] loss: 132948.165022\n",
            "[16,   560] loss: 130595.318021\n",
            "[17,   560] loss: 132302.181250\n",
            "[18,   560] loss: 132897.840622\n",
            "[19,   560] loss: 132751.457206\n",
            "[20,   560] loss: 132703.750314\n",
            "[21,   560] loss: 132832.831030\n",
            "[22,   560] loss: 132574.661879\n",
            "[23,   560] loss: 132587.512901\n",
            "[24,   560] loss: 132950.630110\n",
            "[25,   560] loss: 132674.052630\n",
            "[26,   560] loss: 132551.657422\n",
            "[27,   560] loss: 132819.746258\n",
            "[28,   560] loss: 132895.550133\n",
            "[29,   560] loss: 132868.244817\n",
            "[30,   560] loss: 132822.500823\n",
            "[31,   560] loss: 132747.149731\n",
            "[32,   560] loss: 132526.852940\n",
            "[33,   560] loss: 132654.300596\n",
            "[34,   560] loss: 132639.090702\n",
            "[35,   560] loss: 132861.167794\n",
            "[36,   560] loss: 132750.436244\n",
            "[37,   560] loss: 132023.978948\n",
            "[38,   560] loss: 132732.159103\n",
            "[39,   560] loss: 132982.257969\n",
            "[40,   560] loss: 132494.332708\n",
            "[41,   560] loss: 132878.582457\n",
            "[42,   560] loss: 132944.575956\n",
            "[43,   560] loss: 132653.700471\n",
            "[44,   560] loss: 132865.638780\n",
            "[45,   560] loss: 132588.300736\n",
            "[46,   560] loss: 132745.304611\n",
            "[47,   560] loss: 132622.887479\n",
            "[48,   560] loss: 132228.527658\n",
            "[49,   560] loss: 132913.387095\n",
            "[50,   560] loss: 132435.714718\n",
            "Finished Training\n",
            "Avg Loss during Testing -  6304.020419237945\n",
            "L2 values -  [6321.074424302765, 6321.766012186663, 6335.08767460773, 13137.98205785974]\n",
            "L2 values -  [6321.074424302765, 4277.459931412648, 5078.85098246747, 6304.020419237945]\n",
            "Sparsity in fc1.weight: 75.25%\n",
            "Sparsity in fc2.weight: 79.06%\n",
            "Sparsity in fc3.weight: 90.62%\n",
            "Sparsity in fc4.weight: 85.55%\n",
            "Sparsity in fc5.weight: 88.28%\n",
            "Sparsity in fc6.weight: 70.31%\n",
            "Sparsity in fc7.weight: 59.38%\n",
            "Sparsity in fc8.weight: 25.00%\n",
            "Sparsity in fc9.weight: 0.00%\n",
            "Sparsity in fc10.weight: 0.00%\n",
            "Global sparsity: 80.03%\n",
            "Avg Loss during Testing -  6335.608402548555\n",
            "[1,   560] loss: 132731.314955\n",
            "[2,   560] loss: 132518.721655\n",
            "[3,   560] loss: 132848.114317\n",
            "[4,   560] loss: 133009.964087\n",
            "[5,   560] loss: 132785.647015\n",
            "[6,   560] loss: 132853.824023\n",
            "[7,   560] loss: 132827.221240\n",
            "[8,   560] loss: 132771.119074\n",
            "[9,   560] loss: 132828.214495\n",
            "[10,   560] loss: 132811.104632\n",
            "[11,   560] loss: 132556.123197\n",
            "[12,   560] loss: 132680.562960\n",
            "[13,   560] loss: 132783.386872\n",
            "[14,   560] loss: 132640.560083\n",
            "[15,   560] loss: 132788.274198\n",
            "[16,   560] loss: 132878.306267\n",
            "[17,   560] loss: 132282.694611\n",
            "[18,   560] loss: 132486.218021\n",
            "[19,   560] loss: 132960.505347\n",
            "[20,   560] loss: 132321.160617\n",
            "[21,   560] loss: 132646.073437\n",
            "[22,   560] loss: 132274.927947\n",
            "[23,   560] loss: 132812.385474\n",
            "[24,   560] loss: 132388.742254\n",
            "[25,   560] loss: 132503.743059\n",
            "[26,   560] loss: 132725.250701\n",
            "[27,   560] loss: 132669.539118\n",
            "[28,   560] loss: 132552.301618\n",
            "[29,   560] loss: 132993.947144\n",
            "[30,   560] loss: 132579.604109\n",
            "[31,   560] loss: 132622.898553\n",
            "[32,   560] loss: 132744.864450\n",
            "[33,   560] loss: 131606.486332\n",
            "[34,   560] loss: 132652.397158\n",
            "[35,   560] loss: 132677.111625\n",
            "[36,   560] loss: 132524.689833\n",
            "[37,   560] loss: 132889.457007\n",
            "[38,   560] loss: 132899.424048\n",
            "[39,   560] loss: 132662.137528\n",
            "[40,   560] loss: 132652.904307\n",
            "[41,   560] loss: 132726.623898\n",
            "[42,   560] loss: 132484.285934\n",
            "[43,   560] loss: 132618.822241\n",
            "[44,   560] loss: 132780.051175\n",
            "[45,   560] loss: 132724.178777\n",
            "[46,   560] loss: 132818.701315\n",
            "[47,   560] loss: 132758.951364\n",
            "[48,   560] loss: 132964.794095\n",
            "[49,   560] loss: 132852.445316\n",
            "[50,   560] loss: 132784.098106\n",
            "Finished Training\n",
            "Avg Loss during Testing -  6310.503429367585\n",
            "L1 values -  [6321.074424302765, 6324.8538893573, 6323.009651262916, 6321.356218456994, 6335.608402548555]\n",
            "L1 values -  [6321.074424302765, 6299.219768046785, 6310.706497560275, 6317.747283713643, 6310.503429367585]\n",
            "Sparsity in fc1.weight: 79.50%\n",
            "Sparsity in fc2.weight: 79.38%\n",
            "Sparsity in fc3.weight: 80.08%\n",
            "Sparsity in fc4.weight: 84.77%\n",
            "Sparsity in fc5.weight: 75.00%\n",
            "Sparsity in fc6.weight: 71.88%\n",
            "Sparsity in fc7.weight: 90.62%\n",
            "Sparsity in fc8.weight: 75.00%\n",
            "Sparsity in fc9.weight: 100.00%\n",
            "Sparsity in fc10.weight: 50.00%\n",
            "Global sparsity: 80.03%\n",
            "Avg Loss during Testing -  8495.670463786835\n",
            "[1,   560] loss: 150946.230451\n",
            "[2,   560] loss: 136359.175310\n",
            "[3,   560] loss: 132220.205472\n",
            "[4,   560] loss: 132372.301761\n",
            "[5,   560] loss: 132736.215880\n",
            "[6,   560] loss: 132555.656034\n",
            "[7,   560] loss: 132688.207847\n",
            "[8,   560] loss: 132827.896770\n",
            "[9,   560] loss: 132731.046589\n",
            "[10,   560] loss: 132523.061217\n",
            "[11,   560] loss: 132704.696122\n",
            "[12,   560] loss: 132782.559361\n",
            "[13,   560] loss: 132839.253177\n",
            "[14,   560] loss: 132768.435174\n",
            "[15,   560] loss: 132726.038853\n",
            "[16,   560] loss: 132788.644315\n",
            "[17,   560] loss: 132386.588797\n",
            "[18,   560] loss: 132945.073472\n",
            "[19,   560] loss: 132904.098029\n",
            "[20,   560] loss: 132817.455246\n",
            "[21,   560] loss: 132650.505894\n",
            "[22,   560] loss: 132739.226821\n",
            "[23,   560] loss: 132134.011217\n",
            "[24,   560] loss: 132940.270487\n",
            "[25,   560] loss: 132700.009448\n",
            "[26,   560] loss: 132651.473532\n",
            "[27,   560] loss: 132773.210833\n",
            "[28,   560] loss: 132749.761893\n",
            "[29,   560] loss: 132787.499568\n",
            "[30,   560] loss: 132633.540932\n",
            "[31,   560] loss: 132995.354632\n",
            "[32,   560] loss: 132686.135718\n",
            "[33,   560] loss: 132880.378610\n",
            "[34,   560] loss: 131782.050799\n",
            "[35,   560] loss: 132493.823029\n",
            "[36,   560] loss: 132907.509518\n",
            "[37,   560] loss: 132864.777302\n",
            "[38,   560] loss: 132233.122147\n",
            "[39,   560] loss: 132911.073601\n",
            "[40,   560] loss: 132853.876367\n",
            "[41,   560] loss: 132630.543377\n",
            "[42,   560] loss: 132624.862301\n",
            "[43,   560] loss: 132902.537835\n",
            "[44,   560] loss: 130424.559086\n",
            "[45,   560] loss: 132546.051116\n",
            "[46,   560] loss: 132877.212939\n",
            "[47,   560] loss: 132913.716406\n",
            "[48,   560] loss: 132660.552748\n",
            "[49,   560] loss: 132792.671404\n",
            "[50,   560] loss: 132764.182732\n",
            "Finished Training\n",
            "Avg Loss during Testing -  6304.394404181018\n",
            "L2 values -  [6321.074424302765, 6321.766012186663, 6335.08767460773, 13137.98205785974, 8495.670463786835]\n",
            "L2 values -  [6321.074424302765, 4277.459931412648, 5078.85098246747, 6304.020419237945, 6304.394404181018]\n",
            "Sparsity in fc1.weight: 88.75%\n",
            "Sparsity in fc2.weight: 89.38%\n",
            "Sparsity in fc3.weight: 95.70%\n",
            "Sparsity in fc4.weight: 93.36%\n",
            "Sparsity in fc5.weight: 95.31%\n",
            "Sparsity in fc6.weight: 92.19%\n",
            "Sparsity in fc7.weight: 75.00%\n",
            "Sparsity in fc8.weight: 25.00%\n",
            "Sparsity in fc9.weight: 0.00%\n",
            "Sparsity in fc10.weight: 0.00%\n",
            "Global sparsity: 90.01%\n",
            "Avg Loss during Testing -  6323.960129490767\n",
            "[1,   560] loss: 132977.183576\n",
            "[2,   560] loss: 132736.512284\n",
            "[3,   560] loss: 132704.576252\n",
            "[4,   560] loss: 132503.020037\n",
            "[5,   560] loss: 132038.749944\n",
            "[6,   560] loss: 132724.660938\n",
            "[7,   560] loss: 132824.588208\n",
            "[8,   560] loss: 132713.337179\n",
            "[9,   560] loss: 132860.745201\n",
            "[10,   560] loss: 132824.105730\n",
            "[11,   560] loss: 132845.876277\n",
            "[12,   560] loss: 132769.958315\n",
            "[13,   560] loss: 132928.656948\n",
            "[14,   560] loss: 132870.384295\n",
            "[15,   560] loss: 132514.599428\n",
            "[16,   560] loss: 132886.379224\n",
            "[17,   560] loss: 132818.544967\n",
            "[18,   560] loss: 132468.795138\n",
            "[19,   560] loss: 132866.957854\n",
            "[20,   560] loss: 131245.705082\n",
            "[21,   560] loss: 132583.243021\n",
            "[22,   560] loss: 132835.034075\n",
            "[23,   560] loss: 132901.845975\n",
            "[24,   560] loss: 132528.855713\n",
            "[25,   560] loss: 132813.762497\n",
            "[26,   560] loss: 132913.041047\n",
            "[27,   560] loss: 132938.985278\n",
            "[28,   560] loss: 131993.266678\n",
            "[29,   560] loss: 132685.986105\n",
            "[30,   560] loss: 132824.303931\n",
            "[31,   560] loss: 132809.455999\n",
            "[32,   560] loss: 132899.182272\n",
            "[33,   560] loss: 132670.545410\n",
            "[34,   560] loss: 132839.155891\n",
            "[35,   560] loss: 132883.888034\n",
            "[36,   560] loss: 132240.408918\n",
            "[37,   560] loss: 132851.563299\n",
            "[38,   560] loss: 132757.595951\n",
            "[39,   560] loss: 132896.532889\n",
            "[40,   560] loss: 132851.643443\n",
            "[41,   560] loss: 131983.349125\n",
            "[42,   560] loss: 132034.614076\n",
            "[43,   560] loss: 132239.268851\n",
            "[44,   560] loss: 132868.237347\n",
            "[45,   560] loss: 132723.432265\n",
            "[46,   560] loss: 132884.540639\n",
            "[47,   560] loss: 132856.596369\n",
            "[48,   560] loss: 132667.756550\n",
            "[49,   560] loss: 132582.064178\n",
            "[50,   560] loss: 132522.240095\n",
            "Finished Training\n",
            "Avg Loss during Testing -  6318.053090915614\n",
            "L1 values -  [6321.074424302765, 6324.8538893573, 6323.009651262916, 6321.356218456994, 6335.608402548555, 6323.960129490767]\n",
            "L1 values -  [6321.074424302765, 6299.219768046785, 6310.706497560275, 6317.747283713643, 6310.503429367585, 6318.053090915614]\n",
            "Sparsity in fc1.weight: 90.00%\n",
            "Sparsity in fc2.weight: 86.56%\n",
            "Sparsity in fc3.weight: 89.45%\n",
            "Sparsity in fc4.weight: 92.19%\n",
            "Sparsity in fc5.weight: 91.41%\n",
            "Sparsity in fc6.weight: 93.75%\n",
            "Sparsity in fc7.weight: 96.88%\n",
            "Sparsity in fc8.weight: 87.50%\n",
            "Sparsity in fc9.weight: 100.00%\n",
            "Sparsity in fc10.weight: 100.00%\n",
            "Global sparsity: 90.01%\n",
            "Avg Loss during Testing -  13149.729417814773\n",
            "[1,   560] loss: 241907.623689\n",
            "[2,   560] loss: 211085.854604\n",
            "[3,   560] loss: 185428.165165\n",
            "[4,   560] loss: 168359.874432\n",
            "[5,   560] loss: 153805.488665\n",
            "[6,   560] loss: 145319.103843\n",
            "[7,   560] loss: 139531.667871\n",
            "[8,   560] loss: 135925.920006\n",
            "[9,   560] loss: 134098.657642\n",
            "[10,   560] loss: 132561.162626\n",
            "[11,   560] loss: 132913.309138\n",
            "[12,   560] loss: 132684.552250\n",
            "[13,   560] loss: 132597.539784\n",
            "[14,   560] loss: 132965.580371\n",
            "[15,   560] loss: 132875.803013\n",
            "[16,   560] loss: 132463.877679\n",
            "[17,   560] loss: 132689.154876\n",
            "[18,   560] loss: 132968.926716\n",
            "[19,   560] loss: 132121.217278\n",
            "[20,   560] loss: 132736.271397\n",
            "[21,   560] loss: 132854.714568\n",
            "[22,   560] loss: 132580.495072\n",
            "[23,   560] loss: 132939.029851\n",
            "[24,   560] loss: 132267.337737\n",
            "[25,   560] loss: 132615.225684\n",
            "[26,   560] loss: 132133.710400\n",
            "[27,   560] loss: 132573.171763\n",
            "[28,   560] loss: 132924.201235\n",
            "[29,   560] loss: 132965.217951\n",
            "[30,   560] loss: 132699.312967\n",
            "[31,   560] loss: 132236.289753\n",
            "[32,   560] loss: 132668.586548\n",
            "[33,   560] loss: 132605.789746\n",
            "[34,   560] loss: 132615.330451\n",
            "[35,   560] loss: 132796.994388\n",
            "[36,   560] loss: 132880.417432\n",
            "[37,   560] loss: 132755.679370\n",
            "[38,   560] loss: 131923.317477\n",
            "[39,   560] loss: 132534.388609\n",
            "[40,   560] loss: 132697.555026\n",
            "[41,   560] loss: 132584.561590\n",
            "[42,   560] loss: 132906.350272\n",
            "[43,   560] loss: 132775.817889\n",
            "[44,   560] loss: 132630.139708\n",
            "[45,   560] loss: 132847.524972\n",
            "[46,   560] loss: 132465.819053\n",
            "[47,   560] loss: 132977.738920\n",
            "[48,   560] loss: 132793.414111\n",
            "[49,   560] loss: 132782.425774\n",
            "[50,   560] loss: 132680.694099\n",
            "Finished Training\n",
            "Avg Loss during Testing -  6310.201035088978\n",
            "L2 values -  [6321.074424302765, 6321.766012186663, 6335.08767460773, 13137.98205785974, 8495.670463786835, 13149.729417814773]\n",
            "L2 values -  [6321.074424302765, 4277.459931412648, 5078.85098246747, 6304.020419237945, 6304.394404181018, 6310.201035088978]\n"
          ]
        }
      ],
      "source": [
        "# Full cycle\n",
        "\n",
        "parameters_to_prune = (\n",
        "    (net.fc1, 'weight'),\n",
        "    (net.fc2, 'weight'),\n",
        "    (net.fc3, 'weight'),\n",
        "    (net.fc4, 'weight'),\n",
        "    (net.fc5, 'weight'),\n",
        "    (net.fc6, 'weight'),\n",
        "    (net.fc7, 'weight'),\n",
        "    (net.fc8, 'weight'),\n",
        "    (net.fc9, 'weight'),\n",
        "    (net.fc10, 'weight'),\n",
        "    )\n",
        "\n",
        "batch_loss = 560\n",
        "epoch = 50\n",
        "\n",
        "results_base_l1 = []\n",
        "results_finetune_l1 = []\n",
        "\n",
        "results_base_l2 = []\n",
        "results_finetune_l2 = []\n",
        "\n",
        "# Network\n",
        "net = Net().to(device)\n",
        "print_sparsity(net)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.1)\n",
        "\n",
        "train(net, train_loader, epoch, optimizer, criterion, batch_loss)\n",
        "loss_mse = test(net, test_loader, criterion)\n",
        "\n",
        "results_base_l1.append(loss_mse)\n",
        "results_finetune_l1.append(loss_mse)\n",
        "results_base_l2.append(loss_mse)\n",
        "results_finetune_l2.append(loss_mse)\n",
        "\n",
        "PATH = './cifar_net.pth'\n",
        "torch.save(net.state_dict(), PATH)\n",
        "\n",
        "prune_values = [0.2, 0.4, 0.6, 0.8, 0.9]\n",
        "pruning_methods = [0,1]\n",
        "\n",
        "\n",
        "for prune_value in prune_values:\n",
        "  for pruning_method in pruning_methods:\n",
        "  # Each prune value experiment is independent\n",
        "    net = Net().to(device)\n",
        "    net.load_state_dict(torch.load(PATH))\n",
        "    \n",
        "    parameters_to_prune = (\n",
        "    (net.fc1, 'weight'),\n",
        "    (net.fc2, 'weight'),\n",
        "    (net.fc3, 'weight'),\n",
        "    (net.fc4, 'weight'),\n",
        "    (net.fc5, 'weight'),\n",
        "    (net.fc6, 'weight'),\n",
        "    (net.fc7, 'weight'),\n",
        "    (net.fc8, 'weight'),\n",
        "    (net.fc9, 'weight'),\n",
        "    (net.fc10, 'weight'),\n",
        "    )\n",
        "    \n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.1)\n",
        "    \n",
        "    if pruning_method == 0:\n",
        "      prune.global_unstructured(\n",
        "          parameters_to_prune,\n",
        "          pruning_method=prune.L1Unstructured,\n",
        "          amount=prune_value,\n",
        "      )\n",
        "    else:\n",
        "      prune.global_unstructured(\n",
        "          parameters_to_prune,\n",
        "          pruning_method=prune.RandomUnstructured,\n",
        "          amount=prune_value,\n",
        "      )\n",
        "\n",
        "    print_sparsity(net)\n",
        "    \n",
        "    base_loss = test(net, test_loader, criterion)\n",
        "\n",
        "    train(net, train_loader, epoch, optimizer, criterion, batch_loss)\n",
        "    finetune_loss = test(net, test_loader, criterion)\n",
        "    \n",
        "    if pruning_method == 0:\n",
        "      results_base_l1.append(base_loss)\n",
        "      results_finetune_l1.append(finetune_loss)\n",
        "\n",
        "      print('L1 values - ', results_base_l1)\n",
        "      print('L1 values - ', results_finetune_l1)\n",
        "    else:\n",
        "      results_base_l2.append(base_loss)\n",
        "      results_finetune_l2.append(finetune_loss)\n",
        "\n",
        "      print('L2 values - ', results_base_l2)\n",
        "      print('L2 values - ', results_finetune_l2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "OX9TdJXfot2w",
        "outputId": "afe08ba8-56f8-4929-8404-8eccc7ec21f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[6321.074424302765,\n",
              " 6324.8538893573,\n",
              " 6323.009651262916,\n",
              " 6321.356218456994,\n",
              " 6335.608402548555,\n",
              " 6323.960129490767]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_base_l1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXz5mcZUot2w",
        "outputId": "6cf8ba16-fa47-4824-8949-2e52299c0613"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[6321.074424302765,\n",
              " 6299.219768046785,\n",
              " 6310.706497560275,\n",
              " 6317.747283713643,\n",
              " 6310.503429367585,\n",
              " 6318.053090915614]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_finetune_l1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEhS8001ot2x",
        "outputId": "1caa2ed4-7470-4fbb-c3dc-8c350ce5557c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[6321.074424302765,\n",
              " 6321.766012186663,\n",
              " 6335.08767460773,\n",
              " 13137.98205785974,\n",
              " 8495.670463786835,\n",
              " 13149.729417814773]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_base_l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "0874MtSHot2x",
        "outputId": "82a3cbaf-e3c9-4aa1-f7bf-cec8f356b60c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[6321.074424302765,\n",
              " 4277.459931412648,\n",
              " 5078.85098246747,\n",
              " 6304.020419237945,\n",
              " 6304.394404181018,\n",
              " 6310.201035088978]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_finetune_l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ShFTcOGGot2x"
      },
      "outputs": [],
      "source": [
        "x_coord = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "w-iMHwYwot2x",
        "outputId": "e860d66a-c0a5-455b-dbf4-0a67b4d55f5f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('Housing Dataset with 10 layer NN and different pruning techniques')\n",
        "plt.plot(x_coord, results_base_l1, label = 'L1 without finetuning')\n",
        "plt.plot(x_coord, results_finetune_l1, label = 'L1 finetuned')\n",
        "plt.plot(x_coord, results_base_l2, label = 'Random without finetuning')\n",
        "plt.plot(x_coord, results_finetune_l2, label = 'Random finetuned')\n",
        "plt.ylabel('MSE loss')\n",
        "plt.xlabel('Sparsity')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ2hfRUPKiUG"
      },
      "source": [
        "# Archive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndoBCQk1KiUH",
        "outputId": "934fc4c1-d6e9-432a-d3c4-2b0847b33874"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method Module.parameters of Net(\n",
              "  (fc1): Linear(in_features=20, out_features=20, bias=True)\n",
              "  (fc2): Linear(in_features=20, out_features=16, bias=True)\n",
              "  (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
              "  (fc4): Linear(in_features=16, out_features=16, bias=True)\n",
              "  (fc5): Linear(in_features=16, out_features=8, bias=True)\n",
              "  (fc6): Linear(in_features=8, out_features=8, bias=True)\n",
              "  (fc7): Linear(in_features=8, out_features=4, bias=True)\n",
              "  (fc8): Linear(in_features=4, out_features=4, bias=True)\n",
              "  (fc9): Linear(in_features=4, out_features=2, bias=True)\n",
              "  (fc10): Linear(in_features=2, out_features=1, bias=True)\n",
              ")>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net.parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "S4-8sIAIKiUH"
      },
      "outputs": [],
      "source": [
        "parameters_to_prune = (\n",
        "    (net.fc1, 'weight'),\n",
        "    (net.fc2, 'weight'),\n",
        "    (net.fc3, 'weight'),\n",
        ")\n",
        "\n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune,\n",
        "    pruning_method=prune.L1Unstructured,\n",
        "    amount=0.2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl9Ub-gPKiUH",
        "outputId": "164d7c13-b861-4b08-efff-f5adcd34e2ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sparsity in fc1.weight: 91.75%\n",
            "Sparsity in fc2.weight: 89.06%\n",
            "Sparsity in fc3.weight: 92.19%\n",
            "Global sparsity: 90.98%\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(net.fc1.weight == 0))\n",
        "        / float(net.fc1.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in fc2.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(net.fc2.weight == 0))\n",
        "        / float(net.fc2.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in fc3.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(net.fc3.weight == 0))\n",
        "        / float(net.fc3.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Global sparsity: {:.2f}%\".format(\n",
        "        100. * float(\n",
        "            + torch.sum(net.fc1.weight == 0)\n",
        "            + torch.sum(net.fc2.weight == 0)\n",
        "            + torch.sum(net.fc3.weight == 0)\n",
        "        )\n",
        "        / float(\n",
        "            + net.fc1.weight.nelement()\n",
        "            + net.fc2.weight.nelement()\n",
        "            + net.fc3.weight.nelement()\n",
        "        )\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvIuDFFSKiUI",
        "outputId": "be5b8cb5-cbde-415b-db22-dfd40ffacf3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6299.001349921068\n"
          ]
        }
      ],
      "source": [
        "# Testing after pruning\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32,\n",
        "                        shuffle=True, num_workers=0)\n",
        "\n",
        "running_loss = 0.0\n",
        "for i, data in enumerate(test_loader, 0):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    inputs, labels = data['features'].to(device), data['price'].to(device)\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    # print(inputs)\n",
        "    # forward + backward + optimize\n",
        "    outputs = net(inputs)\n",
        "    # print(outputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    # for name, param in net.named_parameters():\n",
        "    #   if param.requires_grad and name == 'fc2.weight':\n",
        "    #     print(name, param.data)\n",
        "    # print(loss)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "\n",
        "running_loss = running_loss / 2613\n",
        "print(running_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hYuMe0abKiUI"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5GXSfVDKiUI",
        "outputId": "84a11ef4-5b84-49e5-aa96-bfed4793e9fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,   200] loss: 122181.137539\n",
            "[1,   400] loss: 146072.629141\n",
            "[2,   200] loss: 137330.837764\n",
            "[2,   400] loss: 131440.652686\n",
            "[3,   200] loss: 136169.098115\n",
            "[3,   400] loss: 129041.408721\n",
            "[4,   200] loss: 138213.130977\n",
            "[4,   400] loss: 127328.475977\n",
            "[5,   200] loss: 130457.387090\n",
            "[5,   400] loss: 127137.012061\n",
            "[6,   200] loss: 125147.003721\n",
            "[6,   400] loss: 143463.541582\n",
            "[7,   200] loss: 131718.300742\n",
            "[7,   400] loss: 139262.185137\n",
            "[8,   200] loss: 123157.821914\n",
            "[8,   400] loss: 157254.663535\n",
            "[9,   200] loss: 126901.994570\n",
            "[9,   400] loss: 141942.693594\n",
            "[10,   200] loss: 123002.345010\n",
            "[10,   400] loss: 150233.661172\n",
            "[11,   200] loss: 124628.028857\n",
            "[11,   400] loss: 132072.065371\n",
            "[12,   200] loss: 141475.281016\n",
            "[12,   400] loss: 129519.830937\n",
            "[13,   200] loss: 125851.403252\n",
            "[13,   400] loss: 139541.558438\n",
            "[14,   200] loss: 143665.514805\n",
            "[14,   400] loss: 128431.276602\n",
            "[15,   200] loss: 134079.954600\n",
            "[15,   400] loss: 131773.914492\n",
            "[16,   200] loss: 142269.049697\n",
            "[16,   400] loss: 133414.586035\n",
            "[17,   200] loss: 138283.644941\n",
            "[17,   400] loss: 131299.523711\n",
            "[18,   200] loss: 124855.285264\n",
            "[18,   400] loss: 140532.927930\n",
            "[19,   200] loss: 142980.839932\n",
            "[19,   400] loss: 129751.729707\n",
            "[20,   200] loss: 119281.121504\n",
            "[20,   400] loss: 136371.047412\n",
            "[21,   200] loss: 144408.274756\n",
            "[21,   400] loss: 122312.323320\n",
            "[22,   200] loss: 141315.811914\n",
            "[22,   400] loss: 136311.564434\n",
            "[23,   200] loss: 127735.802656\n",
            "[23,   400] loss: 133896.440771\n",
            "[24,   200] loss: 133027.427734\n",
            "[24,   400] loss: 126968.946943\n",
            "[25,   200] loss: 142115.678330\n",
            "[25,   400] loss: 127522.076064\n",
            "[26,   200] loss: 147932.946641\n",
            "[26,   400] loss: 124105.889521\n",
            "[27,   200] loss: 132350.987119\n",
            "[27,   400] loss: 127095.023496\n",
            "[28,   200] loss: 124719.365195\n",
            "[28,   400] loss: 137853.421621\n",
            "[29,   200] loss: 127717.051787\n",
            "[29,   400] loss: 130286.414512\n",
            "[30,   200] loss: 124704.037900\n",
            "[30,   400] loss: 153740.540742\n",
            "[31,   200] loss: 120762.926553\n",
            "[31,   400] loss: 125896.119688\n",
            "[32,   200] loss: 131933.134492\n",
            "[32,   400] loss: 126832.004414\n",
            "[33,   200] loss: 145112.984004\n",
            "[33,   400] loss: 117670.914590\n",
            "[34,   200] loss: 133518.156348\n",
            "[34,   400] loss: 124729.918740\n",
            "[35,   200] loss: 125076.841279\n",
            "[35,   400] loss: 137008.428320\n",
            "[36,   200] loss: 141073.229687\n",
            "[36,   400] loss: 141374.834082\n",
            "[37,   200] loss: 123155.081602\n",
            "[37,   400] loss: 132914.704687\n",
            "[38,   200] loss: 137027.054463\n",
            "[38,   400] loss: 133794.707383\n",
            "[39,   200] loss: 145591.160967\n",
            "[39,   400] loss: 129066.342188\n",
            "[40,   200] loss: 138864.678711\n",
            "[40,   400] loss: 127303.923809\n",
            "[41,   200] loss: 144577.973740\n",
            "[41,   400] loss: 124100.621367\n",
            "[42,   200] loss: 129023.554248\n",
            "[42,   400] loss: 131504.133633\n",
            "[43,   200] loss: 126210.100713\n",
            "[43,   400] loss: 141215.292539\n",
            "[44,   200] loss: 132759.388018\n",
            "[44,   400] loss: 135182.389033\n",
            "[45,   200] loss: 130275.949990\n",
            "[45,   400] loss: 140559.182471\n",
            "[46,   200] loss: 134509.973916\n",
            "[46,   400] loss: 131018.174873\n",
            "[47,   200] loss: 128985.091445\n",
            "[47,   400] loss: 151863.764668\n",
            "[48,   200] loss: 127277.646660\n",
            "[48,   400] loss: 133441.166045\n",
            "[49,   200] loss: 139026.259375\n",
            "[49,   400] loss: 133213.078525\n",
            "[50,   200] loss: 159524.920215\n",
            "[50,   400] loss: 118610.866338\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(50):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data['features'].to(device), data['price'].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # print(inputs)\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        # print(outputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # for name, param in net.named_parameters():\n",
        "        #   if param.requires_grad and name == 'fc2.weight':\n",
        "        #     print(name, param.data)\n",
        "        # print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:    # print every 200 mini-batches\n",
        "            print('[%d, %5d] loss: %.6f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "g4AI0k1qKiUI",
        "outputId": "742bb134-564e-4061-a8e2-d25b7677b7b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6287.0512013251055\n"
          ]
        }
      ],
      "source": [
        "# Testing after finte-tuning with pruning\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32,\n",
        "                        shuffle=True, num_workers=0)\n",
        "\n",
        "running_loss = 0.0\n",
        "for i, data in enumerate(test_loader, 0):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    inputs, labels = data['features'].to(device), data['price'].to(device)\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    # print(inputs)\n",
        "    # forward + backward + optimize\n",
        "    outputs = net(inputs)\n",
        "    # print(outputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    # for name, param in net.named_parameters():\n",
        "    #   if param.requires_grad and name == 'fc2.weight':\n",
        "    #     print(name, param.data)\n",
        "    # print(loss)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "\n",
        "running_loss = running_loss / 2613\n",
        "print(running_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83ySPYZLiO1j"
      },
      "source": [
        "## Plants classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj_J9BS9wpII"
      },
      "source": [
        "# Experiment 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZmgh3lIwy4t"
      },
      "source": [
        "**Dataset - One hundred plants texture**\n",
        "\n",
        "**Architecture - 3 layer Neural Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0GPY8im17JO"
      },
      "source": [
        "### Preliminary training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7p29zFzIw9DA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LlauX8Me5egY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OhS0mG2dwC2z",
        "outputId": "52c0af09-a982-4ceb-dc39-9605dd824d93"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-0xhaC_swEGs",
        "outputId": "928f4710-886d-443b-cd0b-a2e7d19bab21"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Y_zHdrIm3ulh"
      },
      "outputs": [],
      "source": [
        "class HouseSalesDataset(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, csv_file, root_dir, transform=None, train = True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.housing_data = pd.read_csv(csv_file)\n",
        "        self.x = self.housing_data.drop('price', axis = 1)\n",
        "        df = self.housing_data.drop('date', axis = 1)\n",
        "        self.x = (df-df.mean())/df.std()\n",
        "        self.y = self.housing_data[['price']]/1000\n",
        "\n",
        "        if train:\n",
        "          self.x = self.x.iloc[:18000, :]\n",
        "          self.y = self.y.iloc[:18000, :]\n",
        "\n",
        "        else:\n",
        "          self.x = self.x.iloc[18000:, :]\n",
        "          self.y = self.y.iloc[18000:, :]  \n",
        "\n",
        "\n",
        "        print('Length = ', len(self.y))\n",
        "        self.x = torch.Tensor(self.x.values).to(device)\n",
        "        self.y = torch.Tensor(self.y.values).to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # if self.transform:\n",
        "        #     sample = self.transform(sample)\n",
        "\n",
        "        features = self.x[idx]\n",
        "        price = self.y[idx]\n",
        "        return {'features': features, 'price': price}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wtU0HSzLDdgH",
        "outputId": "c3c5dbb7-958b-484a-915c-6006b2f2dae2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length =  18000\n",
            "Length =  3613\n"
          ]
        }
      ],
      "source": [
        "train_dataset = HouseSalesDataset(csv_file='kc_house_data.csv',\n",
        "                                    root_dir='/', train = True)\n",
        "\n",
        "test_dataset = HouseSalesDataset(csv_file='kc_house_data.csv',\n",
        "                                    root_dir='/', train = False)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32,\n",
        "                        shuffle=True, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yZFpI9pnxLZw"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(20, 16)\n",
        "        self.fc2 = nn.Linear(16, 4)\n",
        "        self.fc3 = nn.Linear(4, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = Net().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8bClq3wEdsnI"
      },
      "outputs": [],
      "source": [
        "# for i_batch, sample_batched in enumerate(dataloader):\n",
        "#     print(i_batch, sample_batched['features'].size(),\n",
        "#           sample_batched['price'].size())\n",
        "#     outputs = net(sample_batched['features'])\n",
        "#     print(outputs)\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yDB3zWTix0kk"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.MSELoss()\n",
        "# optimizer = optim.SGD(net.parameters(), lr=10)\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oK2BOIYezpg1",
        "outputId": "8491a183-db6f-48ca-e16d-7c0a634e9988"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19182.876130166474\n"
          ]
        }
      ],
      "source": [
        "# Testing before training\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32,\n",
        "                        shuffle=True, num_workers=0)\n",
        "\n",
        "running_loss = 0.0\n",
        "for i, data in enumerate(test_loader, 0):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    inputs, labels = data['features'].to(device), data['price'].to(device)\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    # print(inputs)\n",
        "    # forward + backward + optimize\n",
        "    outputs = net(inputs)\n",
        "    # print(outputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    # for name, param in net.named_parameters():\n",
        "    #   if param.requires_grad and name == 'fc2.weight':\n",
        "    #     print(name, param.data)\n",
        "    # print(loss)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "\n",
        "running_loss = running_loss / 2613\n",
        "print(running_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mG-qVOwBx1Of",
        "outputId": "7269e435-01cc-4457-d750-7dc06ab68cdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,   200] loss: 313454.279805\n",
            "[1,   400] loss: 270775.208789\n",
            "[2,   200] loss: 173404.642305\n",
            "[2,   400] loss: 154073.499307\n",
            "[3,   200] loss: 126168.431494\n",
            "[3,   400] loss: 108649.528228\n",
            "[4,   200] loss: 111971.183892\n",
            "[4,   400] loss: 84915.198352\n",
            "[5,   200] loss: 79682.803164\n",
            "[5,   400] loss: 72777.883687\n",
            "[6,   200] loss: 85777.619463\n",
            "[6,   400] loss: 71730.381182\n",
            "[7,   200] loss: 77663.372437\n",
            "[7,   400] loss: 71133.666733\n",
            "[8,   200] loss: 68441.446255\n",
            "[8,   400] loss: 73085.686968\n",
            "[9,   200] loss: 62232.009814\n",
            "[9,   400] loss: 64691.638643\n",
            "[10,   200] loss: 70460.636597\n",
            "[10,   400] loss: 59321.271416\n",
            "[11,   200] loss: 52715.929565\n",
            "[11,   400] loss: 61801.528491\n",
            "[12,   200] loss: 55852.758091\n",
            "[12,   400] loss: 57513.918513\n",
            "[13,   200] loss: 64912.645112\n",
            "[13,   400] loss: 53955.008301\n",
            "[14,   200] loss: 66313.721001\n",
            "[14,   400] loss: 49822.689976\n",
            "[15,   200] loss: 63912.291411\n",
            "[15,   400] loss: 44110.129214\n",
            "[16,   200] loss: 54497.053882\n",
            "[16,   400] loss: 50188.705439\n",
            "[17,   200] loss: 49379.482727\n",
            "[17,   400] loss: 42610.903491\n",
            "[18,   200] loss: 59957.267813\n",
            "[18,   400] loss: 50080.884316\n",
            "[19,   200] loss: 46133.739702\n",
            "[19,   400] loss: 65276.559966\n",
            "[20,   200] loss: 67578.728779\n",
            "[20,   400] loss: 43610.836382\n",
            "[21,   200] loss: 45696.908130\n",
            "[21,   400] loss: 50173.187114\n",
            "[22,   200] loss: 56071.002993\n",
            "[22,   400] loss: 50247.196440\n",
            "[23,   200] loss: 55016.052661\n",
            "[23,   400] loss: 50488.711948\n",
            "[24,   200] loss: 55530.580195\n",
            "[24,   400] loss: 46710.299243\n",
            "[25,   200] loss: 50145.584668\n",
            "[25,   400] loss: 64294.751284\n",
            "[26,   200] loss: 56513.984429\n",
            "[26,   400] loss: 47503.412251\n",
            "[27,   200] loss: 42251.289131\n",
            "[27,   400] loss: 49143.802524\n",
            "[28,   200] loss: 45314.980464\n",
            "[28,   400] loss: 48141.235679\n",
            "[29,   200] loss: 49080.925894\n",
            "[29,   400] loss: 47020.853491\n",
            "[30,   200] loss: 51800.148350\n",
            "[30,   400] loss: 47246.520830\n",
            "[31,   200] loss: 56228.788691\n",
            "[31,   400] loss: 39972.673987\n",
            "[32,   200] loss: 43493.816475\n",
            "[32,   400] loss: 46280.025967\n",
            "[33,   200] loss: 38603.290786\n",
            "[33,   400] loss: 49104.045127\n",
            "[34,   200] loss: 37322.213433\n",
            "[34,   400] loss: 55626.361709\n",
            "[35,   200] loss: 52964.671040\n",
            "[35,   400] loss: 37674.005264\n",
            "[36,   200] loss: 48702.160937\n",
            "[36,   400] loss: 45630.291348\n",
            "[37,   200] loss: 42946.363071\n",
            "[37,   400] loss: 49730.501089\n",
            "[38,   200] loss: 43421.830391\n",
            "[38,   400] loss: 51253.077485\n",
            "[39,   200] loss: 35154.172651\n",
            "[39,   400] loss: 42257.001606\n",
            "[40,   200] loss: 37330.988110\n",
            "[40,   400] loss: 45029.062158\n",
            "[41,   200] loss: 50086.358418\n",
            "[41,   400] loss: 45082.049189\n",
            "[42,   200] loss: 36698.096475\n",
            "[42,   400] loss: 43573.773672\n",
            "[43,   200] loss: 46353.044263\n",
            "[43,   400] loss: 40674.254453\n",
            "[44,   200] loss: 47176.282671\n",
            "[44,   400] loss: 52229.444585\n",
            "[45,   200] loss: 44482.265068\n",
            "[45,   400] loss: 54345.958643\n",
            "[46,   200] loss: 43584.472744\n",
            "[46,   400] loss: 50396.353594\n",
            "[47,   200] loss: 46192.103247\n",
            "[47,   400] loss: 54198.229604\n",
            "[48,   200] loss: 38657.483716\n",
            "[48,   400] loss: 43854.351509\n",
            "[49,   200] loss: 42077.222554\n",
            "[49,   400] loss: 42236.259824\n",
            "[50,   200] loss: 54836.742661\n",
            "[50,   400] loss: 45503.811787\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(50):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data['features'].to(device), data['price'].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # print(inputs)\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        # print(outputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # for name, param in net.named_parameters():\n",
        "        #   if param.requires_grad and name == 'fc2.weight':\n",
        "        #     print(name, param.data)\n",
        "        # print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:    # print every 200 mini-batches\n",
        "            print('[%d, %5d] loss: %.6f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6dVOovf1uGK"
      },
      "outputs": [],
      "source": [
        "# Testing after training\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32,\n",
        "                        shuffle=True, num_workers=0)\n",
        "\n",
        "running_loss = 0.0\n",
        "for i, data in enumerate(test_loader, 0):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    inputs, labels = data['features'].to(device), data['price'].to(device)\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    # print(inputs)\n",
        "    # forward + backward + optimize\n",
        "    outputs = net(inputs)\n",
        "    # print(outputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    # for name, param in net.named_parameters():\n",
        "    #   if param.requires_grad and name == 'fc2.weight':\n",
        "    #     print(name, param.data)\n",
        "    # print(loss)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "\n",
        "running_loss = running_loss / 2613\n",
        "print(running_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "INLwBAB2yAui"
      },
      "outputs": [],
      "source": [
        "# PATH = './cifar_net.pth'\n",
        "# torch.save(net.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qAu-V_4jyIjm"
      },
      "outputs": [],
      "source": [
        "# dataiter = iter(testloader)\n",
        "# images, labels = dataiter.next()\n",
        "\n",
        "# # print images\n",
        "# imshow(torchvision.utils.make_grid(images))\n",
        "# print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IhC2GF5ByMBq",
        "outputId": "8c131678-eb02-4af6-df77-c976e9c34489"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-8b61435874a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1483\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1484\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Net:\n\tUnexpected key(s) in state_dict: \"fc4.weight\", \"fc4.bias\", \"fc5.weight\", \"fc5.bias\", \"fc6.weight\", \"fc6.bias\", \"fc7.weight\", \"fc7.bias\", \"fc8.weight\", \"fc8.bias\", \"fc9.weight\", \"fc9.bias\", \"fc10.weight\", \"fc10.bias\". \n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([20, 20]) from checkpoint, the shape in current model is torch.Size([16, 20]).\n\tsize mismatch for fc1.bias: copying a param with shape torch.Size([20]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([16, 20]) from checkpoint, the shape in current model is torch.Size([4, 16]).\n\tsize mismatch for fc2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([4]).\n\tsize mismatch for fc3.weight: copying a param with shape torch.Size([16, 16]) from checkpoint, the shape in current model is torch.Size([1, 4]).\n\tsize mismatch for fc3.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1])."
          ]
        }
      ],
      "source": [
        "net = Net()\n",
        "net.load_state_dict(torch.load(PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ndKMGUlk10Lb"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MJ2qR7eiyOVA"
      },
      "outputs": [],
      "source": [
        "# outputs = net(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOScOg_Y1_6S"
      },
      "source": [
        "### Global pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vzCyqXbp2Itl"
      },
      "outputs": [],
      "source": [
        "net.parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XmsBmesT2Ch8"
      },
      "outputs": [],
      "source": [
        "parameters_to_prune = (\n",
        "    (net.fc1, 'weight'),\n",
        "    (net.fc2, 'weight'),\n",
        "    (net.fc3, 'weight'),\n",
        ")\n",
        "\n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune,\n",
        "    pruning_method=prune.L1Unstructured,\n",
        "    amount=0.2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UpWOfgvp2Ch_"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(net.fc1.weight == 0))\n",
        "        / float(net.fc1.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in fc2.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(net.fc2.weight == 0))\n",
        "        / float(net.fc2.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in fc3.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(net.fc3.weight == 0))\n",
        "        / float(net.fc3.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Global sparsity: {:.2f}%\".format(\n",
        "        100. * float(\n",
        "            + torch.sum(net.fc1.weight == 0)\n",
        "            + torch.sum(net.fc2.weight == 0)\n",
        "            + torch.sum(net.fc3.weight == 0)\n",
        "        )\n",
        "        / float(\n",
        "            + net.fc1.weight.nelement()\n",
        "            + net.fc2.weight.nelement()\n",
        "            + net.fc3.weight.nelement()\n",
        "        )\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "x37dQbGH2fTf"
      },
      "outputs": [],
      "source": [
        "# Testing after pruning\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32,\n",
        "                        shuffle=True, num_workers=0)\n",
        "\n",
        "running_loss = 0.0\n",
        "for i, data in enumerate(test_loader, 0):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    inputs, labels = data['features'].to(device), data['price'].to(device)\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    # print(inputs)\n",
        "    # forward + backward + optimize\n",
        "    outputs = net(inputs)\n",
        "    # print(outputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    # for name, param in net.named_parameters():\n",
        "    #   if param.requires_grad and name == 'fc2.weight':\n",
        "    #     print(name, param.data)\n",
        "    # print(loss)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "\n",
        "running_loss = running_loss / 2613\n",
        "print(running_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Sjw9tUxz3egg"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IeNmaL0l2s_f"
      },
      "outputs": [],
      "source": [
        "for epoch in range(50):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data['features'].to(device), data['price'].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # print(inputs)\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        # print(outputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # for name, param in net.named_parameters():\n",
        "        #   if param.requires_grad and name == 'fc2.weight':\n",
        "        #     print(name, param.data)\n",
        "        # print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:    # print every 200 mini-batches\n",
        "            print('[%d, %5d] loss: %.6f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M0e6AaML3Dr2"
      },
      "outputs": [],
      "source": [
        "# Testing after finte-tuning with pruning\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32,\n",
        "                        shuffle=True, num_workers=0)\n",
        "\n",
        "running_loss = 0.0\n",
        "for i, data in enumerate(test_loader, 0):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    inputs, labels = data['features'].to(device), data['price'].to(device)\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    # print(inputs)\n",
        "    # forward + backward + optimize\n",
        "    outputs = net(inputs)\n",
        "    # print(outputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    # for name, param in net.named_parameters():\n",
        "    #   if param.requires_grad and name == 'fc2.weight':\n",
        "    #     print(name, param.data)\n",
        "    # print(loss)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "\n",
        "running_loss = running_loss / 2613\n",
        "print(running_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRaMyEVzvoBQ"
      },
      "outputs": [],
      "source": [
        "module = net.fc1\n",
        "print(list(module.named_parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgvIrZguvsHB"
      },
      "outputs": [],
      "source": [
        "print(list(module.named_buffers()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oH2TsKSovvCL"
      },
      "outputs": [],
      "source": [
        "prune.random_unstructured(module, name=\"weight\", amount=0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ni68XwmBvyX4"
      },
      "outputs": [],
      "source": [
        "print(list(module.named_parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLjS5bxQv5O9"
      },
      "outputs": [],
      "source": [
        "print(list(module.named_buffers()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUWJiC8JwKk5"
      },
      "outputs": [],
      "source": [
        "print(module.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5FWQq_twKk6"
      },
      "outputs": [],
      "source": [
        "print(module._forward_pre_hooks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DW05XgsKwKk6"
      },
      "outputs": [],
      "source": [
        "# prune.l1_unstructured(module, name=\"bias\", amount=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQSrIwNewKk6"
      },
      "outputs": [],
      "source": [
        "# print(list(module.named_parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKE7_4lFwKk7"
      },
      "outputs": [],
      "source": [
        "# print(list(module.named_buffers()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wX9nN8DowKk7"
      },
      "outputs": [],
      "source": [
        "# print(module.bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bo2KYtfwKk7"
      },
      "outputs": [],
      "source": [
        "# print(module._forward_pre_hooks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cey-RehrwdjI"
      },
      "outputs": [],
      "source": [
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data['features'], data['price']\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # print(inputs)\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        # print(outputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:    # print every 200 mini-batches\n",
        "            print('[%d, %5d] loss: %.6f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pp6X2fNvCfB"
      },
      "source": [
        "## From the blog"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3sEyvXjvsks"
      },
      "source": [
        "### Local pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70cx-Wlcux3H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpHfP5rKvFLq"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 3x3 square conv kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5x5 image dimension\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, int(x.nelement() / x.shape[0]))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = LeNet().to(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFglxGfJvItu"
      },
      "outputs": [],
      "source": [
        "module = model.fc1\n",
        "print(list(module.named_parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4guflj9pvKlO"
      },
      "outputs": [],
      "source": [
        "print(list(module.named_buffers()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEqOQihkvOIA"
      },
      "outputs": [],
      "source": [
        "prune.random_unstructured(module, name=\"weight\", amount=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVV7QFzLvQ5Q"
      },
      "outputs": [],
      "source": [
        "print(list(module.named_parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvgHtZOzvSx7"
      },
      "outputs": [],
      "source": [
        "print(list(module.named_buffers()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tH4-6Su0vXzs"
      },
      "outputs": [],
      "source": [
        "print(module.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peFlVtovvarn"
      },
      "outputs": [],
      "source": [
        "print(module._forward_pre_hooks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArWdi4u_vcw3"
      },
      "outputs": [],
      "source": [
        "prune.l1_unstructured(module, name=\"bias\", amount=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MAvudMbveea"
      },
      "outputs": [],
      "source": [
        "print(list(module.named_parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7MNQUgAvgHk"
      },
      "outputs": [],
      "source": [
        "print(list(module.named_buffers()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfxcpRQ7viVc"
      },
      "outputs": [],
      "source": [
        "print(module.bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYUSGwucvk6d"
      },
      "outputs": [],
      "source": [
        "print(module._forward_pre_hooks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqsT1zllvvZQ"
      },
      "source": [
        "### Global pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5qY-3i5vzqx"
      },
      "outputs": [],
      "source": [
        "model = LeNet()\n",
        "\n",
        "parameters_to_prune = (\n",
        "    (model.conv1, 'weight'),\n",
        "    (model.conv2, 'weight'),\n",
        "    (model.fc1, 'weight'),\n",
        "    (model.fc2, 'weight'),\n",
        "    (model.fc3, 'weight'),\n",
        ")\n",
        "\n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune,\n",
        "    pruning_method=prune.L1Unstructured,\n",
        "    amount=0.2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFte1oQhv1FT"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    \"Sparsity in conv1.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.conv1.weight == 0))\n",
        "        / float(model.conv1.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in conv2.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.conv2.weight == 0))\n",
        "        / float(model.conv2.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.fc1.weight == 0))\n",
        "        / float(model.fc1.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in fc2.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.fc2.weight == 0))\n",
        "        / float(model.fc2.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in fc3.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.fc3.weight == 0))\n",
        "        / float(model.fc3.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Global sparsity: {:.2f}%\".format(\n",
        "        100. * float(\n",
        "            torch.sum(model.conv1.weight == 0)\n",
        "            + torch.sum(model.conv2.weight == 0)\n",
        "            + torch.sum(model.fc1.weight == 0)\n",
        "            + torch.sum(model.fc2.weight == 0)\n",
        "            + torch.sum(model.fc3.weight == 0)\n",
        "        )\n",
        "        / float(\n",
        "            model.conv1.weight.nelement()\n",
        "            + model.conv2.weight.nelement()\n",
        "            + model.fc1.weight.nelement()\n",
        "            + model.fc2.weight.nelement()\n",
        "            + model.fc3.weight.nelement()\n",
        "        )\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbG2AlTEv8Nm"
      },
      "source": [
        "### Custom pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B192Oibnv-qq"
      },
      "outputs": [],
      "source": [
        "class FooBarPruningMethod(prune.BasePruningMethod):\n",
        "    \"\"\"Prune every other entry in a tensor\n",
        "    \"\"\"\n",
        "    PRUNING_TYPE = 'unstructured'\n",
        "\n",
        "    def compute_mask(self, t, default_mask):\n",
        "        mask = default_mask.clone()\n",
        "        mask.view(-1)[::2] = 0\n",
        "        return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPZi4OHrwhGd"
      },
      "outputs": [],
      "source": [
        "def foobar_unstructured(module, name):\n",
        "    \"\"\"Prunes tensor corresponding to parameter called `name` in `module`\n",
        "    by removing every other entry in the tensors.\n",
        "    Modifies module in place (and also return the modified module)\n",
        "    by:\n",
        "    1) adding a named buffer called `name+'_mask'` corresponding to the\n",
        "    binary mask applied to the parameter `name` by the pruning method.\n",
        "    The parameter `name` is replaced by its pruned version, while the\n",
        "    original (unpruned) parameter is stored in a new parameter named\n",
        "    `name+'_orig'`.\n",
        "\n",
        "    Args:\n",
        "        module (nn.Module): module containing the tensor to prune\n",
        "        name (string): parameter name within `module` on which pruning\n",
        "                will act.\n",
        "\n",
        "    Returns:\n",
        "        module (nn.Module): modified (i.e. pruned) version of the input\n",
        "            module\n",
        "\n",
        "    Examples:\n",
        "        >>> m = nn.Linear(3, 4)\n",
        "        >>> foobar_unstructured(m, name='bias')\n",
        "    \"\"\"\n",
        "    FooBarPruningMethod.apply(module, name)\n",
        "    return module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1Nrk6qYwir6"
      },
      "outputs": [],
      "source": [
        "model = LeNet()\n",
        "foobar_unstructured(model.fc3, name='bias')\n",
        "\n",
        "print(model.fc3.bias_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sb3qU-zbwko_"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "kj_J9BS9wpII",
        "rbG2AlTEv8Nm"
      ],
      "name": "Model pruning - 1D data - Housing prices prediction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}